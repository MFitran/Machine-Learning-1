{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bab 2: Proyek Machine Learning End-to-End\n",
    "\n",
    "Dalam bab ini, kita akan bekerja melalui contoh proyek *Machine Learning* secara menyeluruh, berpura-pura menjadi *data scientist* yang baru direkrut di sebuah perusahaan real estat. Tujuan kita adalah membangun model untuk memprediksi harga perumahan di California berdasarkan data sensus. Kita akan mengikuti langkah-langkah khas proyek *Machine Learning*, mulai dari pemahaman masalah hingga penerapan model.\n",
    "\n",
    "**Daftar Isi:**\n",
    "\n",
    "1.  Melihat Gambaran Besar (Look at the Big Picture)\n",
    "    * Membingkai Masalah (Frame the Problem)\n",
    "    * Memilih Ukuran Kinerja (Select a Performance Measure)\n",
    "    * Memeriksa Asumsi (Check the Assumptions)\n",
    "2.  Mendapatkan Data (Get the Data)\n",
    "    * Membuat Ruang Kerja (Create the Workspace)\n",
    "    * Mengunduh Data (Download the Data)\n",
    "    * Melihat Sekilas Struktur Data (Take a Quick Look at the Data Structure)\n",
    "    * Membuat *Test Set* (Create a Test Set)\n",
    "3.  Menjelajahi dan Memvisualisasikan Data untuk Mendapatkan Wawasan (Discover and Visualize the Data to Gain Insights)\n",
    "    * Memvisualisasikan Data Geografis (Visualizing Geographical Data)\n",
    "    * Mencari Korelasi (Looking for Correlations)\n",
    "    * Bereksperimen dengan Kombinasi Atribut (Experimenting with Attribute Combinations)\n",
    "4.  Mempersiapkan Data untuk Algoritma *Machine Learning* (Prepare the Data for Machine Learning Algorithms)\n",
    "    * Pembersihan Data (Data Cleaning)\n",
    "    * Menangani Atribut Teks dan Kategorikal (Handling Text and Categorical Attributes)\n",
    "    * *Custom Transformers* (Transformer Kustom)\n",
    "    * *Feature Scaling* (Penskalaan Fitur)\n",
    "    * *Transformation Pipelines* (Pipeline Transformasi)\n",
    "5.  Memilih dan Melatih Model (Select and Train a Model)\n",
    "    * Melatih dan Mengevaluasi pada *Training Set* (Training and Evaluating on the Training Set)\n",
    "    * Evaluasi yang Lebih Baik Menggunakan *Cross-Validation* (Better Evaluation Using Cross-Validation)\n",
    "6.  Menyesuaikan Model Anda (*Fine-Tune Your Model*)\n",
    "    * *Grid Search* (Pencarian Grid)\n",
    "    * *Randomized Search* (Pencarian Acak)\n",
    "    * *Ensemble Methods* (Metode Ensemble)\n",
    "    * Menganalisis Model Terbaik dan Kesalahannya (Analyze the Best Models and Their Errors)\n",
    "7.  Mengevaluasi Sistem pada *Test Set* (Evaluate Your System on the Test Set)\n",
    "8.  Meluncurkan, Memantau, dan Memelihara Sistem Anda (Launch, Monitor, and Maintain Your System)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Melihat Gambaran Besar (Look at the Big Picture)**\n",
    "\n",
    "Pada tahap awal proyek *Machine Learning*, penting untuk memahami konteks bisnis dan tujuan yang ingin dicapai. Ini membantu kita dalam membingkai masalah, memilih metrik kinerja, dan mengidentifikasi asumsi-asumsi penting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Membingkai Masalah (Frame the Problem)**\n",
    "\n",
    "Pertama, kita perlu bertanya kepada atasan kita apa sebenarnya tujuan bisnisnya. Membangun model mungkin bukan tujuan akhir. Bagaimana perusahaan mengharapkan untuk menggunakan dan mendapatkan manfaat dari model ini? Mengetahui tujuan itu penting karena akan menentukan bagaimana Anda membingkai masalah, algoritma mana yang akan Anda pilih, ukuran kinerja mana yang akan Anda gunakan untuk mengevaluasi model Anda, dan berapa banyak upaya yang akan Anda habiskan untuk menyempurnakannya. \n",
    "\n",
    "Dalam kasus ini, output model kita (prediksi harga median distrik) akan diumpankan ke sistem *Machine Learning* lain untuk menentukan apakah suatu area layak diinvestasikan atau tidak. \n",
    "\n",
    "**Teori:**\n",
    "* **Pipeline Machine Learning:** Urutan komponen pemrosesan data disebut *data pipeline*. *Pipeline* ini umum dalam sistem *Machine Learning* karena melibatkan manipulasi data dan banyak transformasi data. \n",
    "* **Jenis Masalah ML:**\n",
    "    * **Supervised Learning:** Data pelatihan mencakup solusi yang diinginkan (label).  Contoh: klasifikasi (memprediksi kelas) dan regresi (memprediksi nilai numerik). \n",
    "    * **Unsupervised Learning:** Data pelatihan tidak berlabel.  Sistem mencoba belajar tanpa \"guru\".  Contoh: *clustering*, deteksi anomali, visualisasi, dan reduksi dimensi. \n",
    "    * **Semi-supervised Learning:** Menggunakan kombinasi data berlabel dan tidak berlabel. \n",
    "    * **Reinforcement Learning:** Sistem belajar dengan berinteraksi dengan lingkungan, memilih tindakan, dan menerima *reward* atau *penalty*. \n",
    "* **Jenis Pembelajaran:**\n",
    "    * **Batch Learning:** Sistem dilatih menggunakan semua data yang tersedia secara *offline*.  Tidak dapat belajar secara bertahap. \n",
    "    * **Online Learning:** Sistem dilatih secara bertahap dengan data yang masuk secara berurutan.  Cocok untuk data yang terus-menerus mengalir. \n",
    "\n",
    "Untuk proyek harga rumah ini:\n",
    "* Ini adalah tugas **Supervised Learning** karena kita memiliki data berlabel (harga median rumah). \n",
    "* Ini adalah tugas **Regression** karena kita memprediksi nilai numerik (harga). \n",
    "* Lebih spesifik, ini adalah masalah **Multiple Regression** (menggunakan banyak fitur) dan **Univariate Regression** (memprediksi satu nilai output). \n",
    "* Kita akan menggunakan **Batch Learning** karena data cukup kecil untuk muat di memori dan tidak ada kebutuhan untuk adaptasi cepat terhadap perubahan data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kode untuk membingkai masalah (tidak ada kode eksekusi, lebih ke konseptual)\n",
    "\n",
    "# Tujuan Bisnis: Memprediksi harga median perumahan di suatu distrik\n",
    "# Bagaimana model akan digunakan: Sebagai input untuk sistem ML lain untuk analisis investasi.\n",
    "# Solusi saat ini: Estimasi manual oleh para ahli (mahal, lambat, tidak akurat >20% kesalahan). \n",
    "# Jenis masalah: Supervised Learning (kita punya label harga) \n",
    "# Tipe tugas: Regresi (memprediksi nilai numerik) \n",
    "# Pembelajaran: Batch Learning (data statis, cukup kecil untuk memori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Memilih Ukuran Kinerja (Select a Performance Measure)**\n",
    "\n",
    "Langkah selanjutnya adalah memilih ukuran kinerja. Untuk masalah regresi, ukuran kinerja yang umum adalah *Root Mean Square Error* (RMSE). Ini memberikan gambaran tentang seberapa besar kesalahan yang biasanya dibuat oleh sistem dalam prediksinya, dengan bobot yang lebih tinggi untuk kesalahan besar. \n",
    "\n",
    "**Teori:**\n",
    "* **Root Mean Square Error (RMSE):** Mengukur deviasi standar dari residual (perbedaan antara nilai yang diprediksi dan nilai aktual). Ini peka terhadap *outlier*. \n",
    "    $$RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}$$\n",
    "    * $m$: jumlah instance dalam dataset. \n",
    "    * $x^{(i)}$: vektor fitur instance ke-i. \n",
    "    * $y^{(i)}$: label (nilai target) instance ke-i. \n",
    "    * $h$: fungsi prediksi sistem (hipotesis). \n",
    "* **Mean Absolute Error (MAE):** Mengukur rata-rata dari nilai absolut residual.  Kurang peka terhadap *outlier* dibandingkan RMSE. \n",
    "    $$MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^{m}|h(x^{(i)})-y^{(i)}|$$\n",
    "* **Norms ($l_k$):** RMSE berkaitan dengan $l_2$ norm (jarak Euclidean), sedangkan MAE berkaitan dengan $l_1$ norm (jarak Manhattan).  $l_k$ norm yang lebih tinggi lebih fokus pada nilai-nilai besar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teori: Kita akan menggunakan RMSE sebagai metrik utama. \n",
    "# Jika banyak outlier, mungkin pertimbangkan MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Memeriksa Asumsi (Check the Assumptions)**\n",
    "\n",
    "Penting untuk mencatat dan memverifikasi asumsi-asumsi yang telah dibuat sejauh ini. Ini dapat membantu mendeteksi masalah serius sejak awal. Misalnya, asumsi bahwa harga yang dihasilkan model akan digunakan sebagaimana adanya oleh sistem hilir. Jika sistem hilir mengubah harga menjadi kategori, maka masalah kita seharusnya dibingkai sebagai tugas klasifikasi, bukan regresi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumsi: Sistem hilir membutuhkan harga aktual, bukan kategori. \n",
    "# Verifikasi: Berbicara dengan tim hilir mengkonfirmasi kebutuhan harga aktual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Mendapatkan Data (Get the Data)**\n",
    "\n",
    "Setelah memahami masalahnya, saatnya untuk mendapatkan data dan menyiapkannya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Membuat Ruang Kerja (Create the Workspace)**\n",
    "\n",
    "Kita akan membuat direktori kerja dan menginstal pustaka Python yang diperlukan seperti Jupyter, NumPy, pandas, Matplotlib, dan Scikit-Learn. \n",
    "\n",
    "**Teori:**\n",
    "* **Lingkungan Terisolasi (Virtual Environment):** Sangat disarankan untuk bekerja dalam lingkungan terisolasi (misalnya menggunakan `virtualenv`) untuk menghindari konflik versi pustaka antar proyek. \n",
    "* **Jupyter Notebook:** Alat yang kuat untuk pengembangan interaktif, memungkinkan kombinasi kode, output, dan teks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import pustaka yang diperlukan\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib # Untuk menyimpan/memuat model\n",
    "\n",
    "# PATH untuk menyimpan data dan proyek\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\" # \n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\") # \n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\" # \n",
    "\n",
    "# Jalankan ini di terminal untuk membuat ruang kerja:\n",
    "# export ML_PATH=\"$HOME/ml\" # Atau path yang Anda inginkan\n",
    "# mkdir -p $ML_PATH\n",
    "# cd $ML_PATH\n",
    "# python3 -m pip install --user -U virtualenv\n",
    "# python3 -m virtualenv my_env\n",
    "# source my_env/bin/activate # Linux/macOS\n",
    "# .\\my_env\\Scripts\\activate # Windows\n",
    "# python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\n",
    "# python3 -m ipykernel install --user --name python3\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mengunduh Data (Download the Data)**\n",
    "\n",
    "Sebaiknya buat fungsi untuk mengunduh data. Ini berguna jika data sering berubah atau jika Anda perlu menginstal dataset di banyak mesin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\") # \n",
    "    urllib.request.urlretrieve(housing_url, ttgz_path) # \n",
    "    housing_tgz = tarfile.open(tgz_path) # \n",
    "    housing_tgz.extractall(path=housing_path) # \n",
    "    housing_tgz.close() # \n",
    "\n",
    "# Mengunduh data\n",
    "fetch_housing_data()\n",
    "\n",
    "# Fungsi untuk memuat data menggunakan pandas\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\") # \n",
    "    return pd.read_csv(csv_path) # \n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Melihat Sekilas Struktur Data (Take a Quick Look at the Data Structure)**\n",
    "\n",
    "Penting untuk melihat sekilas data untuk mendapatkan pemahaman umum tentang jenis data yang kita tangani. Ini termasuk melihat beberapa baris pertama, informasi umum tentang kolom, dan ringkasan statistik. \n",
    "\n",
    "**Teori:**\n",
    "* **DataFrame `head()`:** Menampilkan N baris pertama dari DataFrame. \n",
    "* **DataFrame `info()`:** Memberikan ringkasan cepat tentang data, termasuk total jumlah baris, tipe data setiap atribut, dan jumlah nilai non-null. \n",
    "* **`value_counts()`:** Berguna untuk atribut kategorikal untuk melihat kategori yang ada dan berapa banyak instance di setiap kategori. \n",
    "* **DataFrame `describe()`:** Menampilkan ringkasan statistik (count, mean, std, min, max, percentile) dari atribut numerik. \n",
    "* **`hist()`:** Memplot histogram untuk setiap atribut numerik, menunjukkan distribusi nilai. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat 5 baris pertama\n",
    "print(housing.head()) # \n",
    "\n",
    "# Mendapatkan info singkat tentang data\n",
    "print(housing.info()) # \n",
    "\n",
    "# Melihat kategori dan jumlahnya untuk ocean_proximity\n",
    "print(housing[\"ocean_proximity\"].value_counts()) # \n",
    "\n",
    "# Ringkasan statistik atribut numerik\n",
    "print(housing.describe()) # \n",
    "\n",
    "# Memplot histogram untuk setiap atribut numerik\n",
    "housing.hist(bins=50, figsize=(20,15)) # \n",
    "plt.show() # \n",
    "\n",
    "# Catatan penting dari histogram:\n",
    "# 1. median_income: Diskalakan dan dibatasi (capped). Perlu dipahami bagaimana dihitung. \n",
    "# 2. housing_median_age dan median_house_value: Juga dibatasi. Ini bisa menjadi masalah serius untuk median_house_value (target kita). \n",
    "#    Jika perlu prediksi di atas $500,000, ada dua opsi:\n",
    "#    a. Kumpulkan label yang tepat untuk distrik yang dibatasi. \n",
    "#    b. Hapus distrik tersebut dari training set (dan test set). \n",
    "# 3. Skala atribut sangat berbeda. Kita akan bahas feature scaling nanti. \n",
    "# 4. Banyak histogram 'tail-heavy' (ekornya panjang ke kanan). Ini bisa menyulitkan beberapa algoritma ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Membuat *Test Set* (Create a Test Set)**\n",
    "\n",
    "**Penting:** Selalu buat *test set* dan sisihkan sebelum melihat data lebih jauh. Ini untuk menghindari *data snooping bias*. \n",
    "\n",
    "**Teori:**\n",
    "* **Data Snooping Bias:** Terjadi ketika Anda secara tidak sengaja \"mengintip\" *test set*, yang dapat menyebabkan Anda memilih model atau *hyperparameter* yang terlalu optimis pada data tersebut, sehingga performa di dunia nyata tidak sesuai harapan. \n",
    "* **Pembagian Data:** Umumnya 80% untuk *training* dan 20% untuk *testing*. Ukuran *test set* dapat bervariasi tergantung ukuran dataset. \n",
    "* **Sampling Murni Acak:** Cara termudah, tetapi bisa menghasilkan *sampling bias* jika dataset tidak cukup besar. \n",
    "* **Sampling Bias:** Ketika metode *sampling* salah, bahkan sampel yang sangat besar pun bisa tidak representatif. \n",
    "* **Stratified Sampling:** Membagi populasi menjadi subkelompok homogen (strata) dan mengambil sampel yang tepat dari setiap stratum untuk memastikan *test set* representatif. Ini penting jika ada atribut penting yang distribusinya perlu dipertahankan. \n",
    "    * Untuk atribut numerik kontinu, perlu dikategorikan terlebih dahulu (binning). \n",
    "* **Scikit-Learn `train_test_split()`:** Fungsi sederhana untuk membagi dataset secara acak. Parameter `random_state` memastikan hasil yang reproduktif. \n",
    "* **Scikit-Learn `StratifiedShuffleSplit`:** Untuk *stratified sampling*, memastikan proporsi kategori dalam *test set* mirip dengan *full dataset*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Membagi data secara acak (tanpa stratified sampling)\n",
    "# train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "# print(f\"Ukuran training set (random): {len(train_set)}\")\n",
    "# print(f\"Ukuran test set (random): {len(test_set)}\")\n",
    "\n",
    "# Melakukan stratified sampling berdasarkan pendapatan median\n",
    "# Asumsi bahwa median_income adalah atribut penting untuk prediksi harga rumah. \n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5]) # \n",
    "\n",
    "# Memvisualisasikan distribusi kategori pendapatan\n",
    "# housing[\"income_cat\"].hist()\n",
    "# plt.title(\"Distribusi Kategori Pendapatan\")\n",
    "# plt.show()\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # \n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index] # \n",
    "    strat_test_set = housing.loc[test_index] # \n",
    "\n",
    "# Membandingkan proporsi kategori pendapatan di dataset penuh, stratified test set, dan random test set (opsional, untuk verifikasi)\n",
    "# Jika tertarik dengan perbandingan ini, lihat notebook asli. \n",
    "\n",
    "# Menghapus atribut 'income_cat' dari training dan test set agar data kembali ke keadaan semula\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True) # \n",
    "\n",
    "# Membuat salinan data training untuk eksplorasi lebih lanjut\n",
    "housing = strat_train_set.copy() # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Menjelajahi dan Memvisualisasikan Data untuk Mendapatkan Wawasan (Discover and Visualize the Data to Gain Insights)**\n",
    "\n",
    "Pada tahap ini, kita akan menggali lebih dalam data pelatihan untuk mendapatkan wawasan. Pastikan hanya menggunakan *training set*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Memvisualisasikan Data Geografis (Visualizing Geographical Data)**\n",
    "\n",
    "Karena dataset mengandung informasi geografis (lintang dan bujur), scatter plot sangat berguna untuk memvisualisasikan data. \n",
    "\n",
    "**Teori:**\n",
    "* **Scatter Plot:** Menampilkan hubungan antara dua variabel numerik. \n",
    "* **Alpha Option:** Mengatur transparansi titik pada plot. Nilai `alpha` yang rendah (misalnya 0.1) memungkinkan kita melihat area dengan kepadatan titik data tinggi. \n",
    "* **`s` (size):** Mengatur ukuran titik pada plot. Dapat digunakan untuk merepresentasikan atribut numerik (misalnya populasi). \n",
    "* **`c` (color):** Mengatur warna titik pada plot. Dapat digunakan untuk merepresentasikan atribut numerik (misalnya nilai harga rumah), dengan `cmap` (color map) untuk gradasi warna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot lintang dan bujur\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) # \n",
    "plt.title(\"Distribusi Geografis Distrik California\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot dengan harga rumah dan populasi\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True) # \n",
    "plt.legend() # \n",
    "plt.title(\"Harga Perumahan California berdasarkan Lokasi dan Populasi\")\n",
    "plt.show()\n",
    "\n",
    "# Wawasan: Harga rumah sangat terkait dengan lokasi (dekat laut) dan kepadatan populasi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mencari Korelasi (Looking for Correlations)**\n",
    "\n",
    "Kita dapat menghitung koefisien korelasi standar (Pearson's r) antar setiap pasang atribut. \n",
    "\n",
    "**Teori:**\n",
    "* **Koefisien Korelasi (Pearson's r):** Berkisar dari -1 hingga 1. \n",
    "    * Dekat 1: Korelasi positif kuat (satu atribut naik, yang lain cenderung naik). \n",
    "    * Dekat -1: Korelasi negatif kuat (satu atribut naik, yang lain cenderung turun). \n",
    "    * Dekat 0: Tidak ada korelasi linier. \n",
    "* **Keterbatasan Korelasi:** Hanya mengukur korelasi linier, dapat melewatkan hubungan nonlinier. \n",
    "* **`scatter_matrix()`:** Memplot setiap atribut numerik terhadap setiap atribut numerik lainnya, ditambah histogram setiap atribut pada diagonal utama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung matriks korelasi\n",
    "corr_matrix = housing.corr(numeric_only=True) # Tambahkan numeric_only=True \n",
    "\n",
    "# Melihat korelasi dengan median_house_value\n",
    "print(corr_matrix[\"median_house_value\"].sort_values(ascending=False)) # \n",
    "\n",
    "# Memvisualisasikan matriks korelasi untuk atribut-atribut penting\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"] # \n",
    "scatter_matrix(housing[attributes], figsize=(12, 8)) # \n",
    "plt.suptitle(\"Matriks Scatter Korelasi Atribut Terpilih\")\n",
    "plt.show()\n",
    "\n",
    "# Memperbesar scatter plot median_income vs median_house_value\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1) # \n",
    "plt.title(\"Pendapatan Median vs. Nilai Rumah Median\")\n",
    "plt.show()\n",
    "\n",
    "# Wawasan: Korelasi kuat antara median_income dan median_house_value. \n",
    "# Perhatikan 'price cap' dan garis horizontal lainnya yang mungkin merupakan 'data quirks'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bereksperimen dengan Kombinasi Atribut (Experimenting with Attribute Combinations)**\n",
    "\n",
    "Sebelum mempersiapkan data untuk algoritma *Machine Learning*, kita bisa mencoba kombinasi atribut baru yang mungkin lebih informatif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat atribut baru\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"] # \n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"] # \n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"] # \n",
    "\n",
    "# Menghitung ulang matriks korelasi dengan atribut baru\n",
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "print(corr_matrix[\"median_house_value\"].sort_values(ascending=False)) # \n",
    "\n",
    "# Wawasan: bedrooms_per_room dan rooms_per_household jauh lebih berkorelasi dengan harga rumah. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Mempersiapkan Data untuk Algoritma Machine Learning (Prepare the Data for Machine Learning Algorithms)**\n",
    "\n",
    "Langkah ini sangat penting. Kita harus menulis fungsi untuk transformasi data agar prosesnya dapat direproduksi, dapat digunakan kembali, dan memungkinkan kita untuk mencoba berbagai kombinasi transformasi secara otomatis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan prediktor dan label\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # membuat salinan \n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pembersihan Data (Data Cleaning)**\n",
    "\n",
    "Sebagian besar algoritma *Machine Learning* tidak dapat bekerja dengan fitur yang hilang. Ada beberapa opsi untuk menanganinya. \n",
    "\n",
    "**Teori:**\n",
    "* **Opsi Penanganan Missing Values:**\n",
    "    1.  Singkirkan distrik yang sesuai (baris dengan nilai hilang). \n",
    "    2.  Singkirkan seluruh atribut (kolom dengan nilai hilang). \n",
    "    3.  Isi nilai yang hilang dengan suatu nilai (nol, mean, median, dll.). \n",
    "* **Scikit-Learn `SimpleImputer`:** Kelas yang nyaman untuk menangani nilai yang hilang. Dapat mengisi nilai yang hilang dengan strategi tertentu (misalnya, median). Penting untuk melatih imputer hanya pada data pelatihan dan menyimpannya untuk digunakan pada data baru/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Memilih atribut numerik (mengabaikan ocean_proximity untuk imputer)\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # \n",
    "\n",
    "# Membuat instance SimpleImputer dengan strategi 'median'\n",
    "imputer = SimpleImputer(strategy=\"median\") # \n",
    "\n",
    "# Melatih imputer pada data pelatihan numerik\n",
    "imputer.fit(housing_num) # \n",
    "\n",
    "# Memeriksa statistik yang dihitung oleh imputer (median setiap atribut)\n",
    "# print(imputer.statistics_) # \n",
    "# print(housing_num.median().values) # Hasilnya harus sama \n",
    "\n",
    "# Mengubah data pelatihan numerik, mengisi nilai yang hilang\n",
    "X = imputer.transform(housing_num) # \n",
    "\n",
    "# Mengembalikannya ke DataFrame pandas\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Menangani Atribut Teks dan Kategorikal (Handling Text and Categorical Attributes)**\n",
    "\n",
    "Atribut teks dan kategorikal perlu diubah menjadi angka agar algoritma *Machine Learning* dapat bekerja dengannya. \n",
    "\n",
    "**Teori:**\n",
    "* **Atribut Kategorikal:** Atribut yang memiliki sejumlah nilai terbatas, masing-masing mewakili kategori. \n",
    "* **`OrdinalEncoder`:** Mengubah kategori teks menjadi angka integer. Masalahnya, algoritma ML akan berasumsi bahwa nilai yang berdekatan lebih mirip, yang tidak selalu benar untuk kategori yang tidak berurutan. \n",
    "* **One-Hot Encoding:** Membuat satu atribut biner per kategori.  Satu atribut akan bernilai 1 (hot) dan yang lainnya 0 (cold).  Ini mengatasi masalah urutan dalam `OrdinalEncoder`. Atribut baru ini disebut *dummy attributes*. \n",
    "* **SciPy Sparse Matrix:** Output `OneHotEncoder` adalah *sparse matrix*. Ini efisien untuk menyimpan matriks dengan banyak nilai nol, hanya menyimpan lokasi elemen non-nol.  Berguna ketika ada ribuan kategori. \n",
    "* **Keterbatasan One-Hot Encoding:** Jika jumlah kategori sangat besar, akan menghasilkan banyak fitur input yang dapat memperlambat pelatihan dan menurunkan kinerja. \n",
    "* **Embeddings:** Alternatif untuk kategori dengan banyak nilai. Ini adalah vektor berdimensi rendah yang dapat dipelajari selama pelatihan untuk merepresentasikan setiap kategori. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih atribut kategorikal\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # \n",
    "# print(housing_cat.head(10)) # \n",
    "\n",
    "# Menggunakan OneHotEncoder untuk mengubah atribut kategorikal menjadi one-hot vectors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder() # \n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat) # \n",
    "# print(housing_cat_1hot) # Ini adalah sparse matrix \n",
    "\n",
    "# Untuk melihat dalam array NumPy dense\n",
    "# print(housing_cat_1hot.toarray()) # \n",
    "\n",
    "# Melihat daftar kategori yang ditemukan encoder\n",
    "# print(cat_encoder.categories_) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer Kustom (Custom Transformers)**\n",
    "\n",
    "Kadang-kadang kita perlu menulis *transformer* sendiri untuk operasi pembersihan khusus atau menggabungkan atribut. Kita harus membuatnya kompatibel dengan fungsi Scikit-Learn (misalnya, *pipelines*). \n",
    "\n",
    "**Teori:**\n",
    "* **Duck Typing:** Scikit-Learn mengandalkan *duck typing*.  Untuk membuat *transformer* kustom yang kompatibel, kelas harus mengimplementasikan metode `fit()` (mengembalikan `self`), `transform()`, dan `fit_transform()`. \n",
    "* **`TransformerMixin`:** Memberikan metode `fit_transform()` secara gratis. \n",
    "* **`BaseEstimator`:** Memberikan metode `get_params()` dan `set_params()` yang berguna untuk penyetelan *hyperparameter* otomatis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Indeks kolom yang akan digunakan untuk atribut baru\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 # \n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs \n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do \n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix] # \n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix] # \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] # \n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room] # \n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household] # \n",
    "\n",
    "# Contoh penggunaan transformer kustom\n",
    "# attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "# housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "# print(housing_extra_attribs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Penskalaan Fitur (Feature Scaling)**\n",
    "\n",
    "Salah satu transformasi terpenting yang perlu diterapkan pada data adalah penskalaan fitur. Algoritma *Machine Learning* tidak berkinerja baik ketika atribut numerik input memiliki skala yang sangat berbeda. \n",
    "\n",
    "**Teori:**\n",
    "* **Kebutuhan Feature Scaling:** Kebanyakan algoritma ML tidak bekerja dengan baik pada fitur dengan skala berbeda. \n",
    "* **Min-Max Scaling (Normalisasi):** Nilai digeser dan diskalakan sehingga berkisar dari 0 hingga 1. Dilakukan dengan mengurangi nilai min dan membagi dengan (max - min). Scikit-Learn menyediakan `MinMaxScaler`. \n",
    "* **Standardization:** Mengurangi nilai *mean* (sehingga *mean* menjadi 0) dan membagi dengan standar deviasi (sehingga variansi menjadi 1). `StandardScaler` di Scikit-Learn. Kurang terpengaruh oleh *outlier*. \n",
    "* **Penting:** Selalu melatih penskala (scaler) hanya pada data pelatihan, bukan pada *full dataset* (termasuk *test set*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Contoh penggunaan StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline Transformasi (Transformation Pipelines)**\n",
    "\n",
    "Ada banyak langkah transformasi data yang perlu dieksekusi dalam urutan yang benar. Scikit-Learn menyediakan kelas `Pipeline` untuk membantu urutan transformasi tersebut. \n",
    "\n",
    "**Teori:**\n",
    "* **Scikit-Learn `Pipeline`:** Mengurutkan transformasi data. Semua estimator kecuali yang terakhir harus berupa *transformer* (memiliki metode `fit_transform()`). \n",
    "* **`ColumnTransformer` (Scikit-Learn 0.20+):** Mengaplikasikan transformasi yang berbeda ke kolom yang berbeda. Sangat berguna dengan pandas DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Pipeline untuk atribut numerik\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "]) # \n",
    "\n",
    "# Mengidentifikasi atribut numerik dan kategorikal\n",
    "housing_num_attribs = list(housing.drop(\"ocean_proximity\", axis=1).columns)\n",
    "housing_cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Pipeline penuh menggunakan ColumnTransformer\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, housing_num_attribs),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), housing_cat_attribs), # handle_unknown untuk kategori baru di masa depan\n",
    "]) # \n",
    "\n",
    "# Menerapkan pipeline ke data\n",
    "housing_prepared = full_pipeline.fit_transform(housing) # \n",
    "print(housing_prepared.shape) # Output harus (jumlah_instance, jumlah_fitur_baru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Memilih dan Melatih Model (Select and Train a Model)**\n",
    "\n",
    "Setelah data siap, kita dapat memilih dan melatih model *Machine Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Melatih dan Mengevaluasi pada Training Set (Training and Evaluating on the Training Set)**\n",
    "\n",
    "Mari kita mulai dengan melatih model *Linear Regression* dan *DecisionTreeRegressor*.\n",
    "\n",
    "**Teori:**\n",
    "* **`LinearRegression`:** Model regresi linier sederhana. \n",
    "* **`DecisionTreeRegressor`:** Model yang kuat, mampu menemukan hubungan nonlinier kompleks.  Namun, rentan terhadap *overfitting* pada data pelatihan. \n",
    "* **`mean_squared_error()`:** Fungsi Scikit-Learn untuk mengukur MSE. \n",
    "* **Underfitting:** Terjadi ketika model terlalu sederhana untuk belajar struktur dasar data.  Kesalahan tinggi baik pada *training* maupun *validation set*. Solusi: gunakan model yang lebih kompleks, fitur yang lebih baik, atau kurangi batasan model. \n",
    "* **Overfitting:** Terjadi ketika model berkinerja baik pada data pelatihan, tetapi tidak dapat digeneralisasi dengan baik ke data baru.  Kesalahan *training* rendah, tetapi kesalahan *generalization* tinggi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Model Linear Regression\n",
    "lin_reg = LinearRegression() # \n",
    "lin_reg.fit(housing_prepared, housing_labels) # \n",
    "\n",
    "# Evaluasi Linear Regression pada training set\n",
    "housing_predictions_lin = lin_reg.predict(housing_prepared) # \n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions_lin) # \n",
    "lin_rmse = np.sqrt(lin_mse) # \n",
    "print(f\"RMSE Linear Regression (Training Set): {lin_rmse}\") # Akan relatif tinggi \n",
    "\n",
    "# Model Decision Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42) # Set random_state untuk reproduksi \n",
    "tree_reg.fit(housing_prepared, housing_labels) # \n",
    "\n",
    "# Evaluasi Decision Tree pada training set\n",
    "housing_predictions_tree = tree_reg.predict(housing_prepared) # \n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions_tree) # \n",
    "tree_rmse = np.sqrt(tree_mse) # \n",
    "print(f\"RMSE Decision Tree (Training Set): {tree_rmse}\") # Akan sangat rendah (0.0), indikasi overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluasi yang Lebih Baik Menggunakan *Cross-Validation***\n",
    "\n",
    "Untuk mengatasi *overfitting* dan mendapatkan estimasi kinerja model yang lebih andal, kita menggunakan *cross-validation*. \n",
    "\n",
    "**Teori:**\n",
    "* **K-Fold Cross-Validation:** Membagi *training set* menjadi K *fold* (subset). Model dilatih K kali, setiap kali memilih *fold* yang berbeda untuk evaluasi dan melatih pada 9 *fold* lainnya. Hasilnya adalah array berisi K skor evaluasi. \n",
    "* **`cross_val_score()`:** Fungsi Scikit-Learn untuk melakukan *cross-validation*. `scoring=\"neg_mean_squared_error\"` digunakan karena Scikit-Learn mengharapkan fungsi utilitas (lebih tinggi lebih baik), bukan fungsi biaya (lebih rendah lebih baik). \n",
    "* **Manfaat Cross-Validation:** Memberikan estimasi kinerja model dan ukuran presisi estimasi tersebut (standar deviasi). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "# Evaluasi Decision Tree dengan cross-validation\n",
    "scores_tree = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                               scoring=\"neg_mean_squared_error\", cv=10) # \n",
    "tree_rmse_scores = np.sqrt(-scores_tree) # \n",
    "print(\"\\nDecision Tree Cross-Validation Scores:\")\n",
    "display_scores(tree_rmse_scores) # RMSE akan jauh lebih tinggi dari 0, menunjukkan overfitting \n",
    "\n",
    "# Evaluasi Linear Regression dengan cross-validation\n",
    "scores_lin = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                              scoring=\"neg_mean_squared_error\", cv=10) # \n",
    "lin_rmse_scores = np.sqrt(-scores_lin) # \n",
    "print(\"\\nLinear Regression Cross-Validation Scores:\")\n",
    "display_scores(lin_rmse_scores) # \n",
    "\n",
    "# Model RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42) # Set random_state untuk reproduksi \n",
    "scores_forest = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                 scoring=\"neg_mean_squared_error\", cv=10) # \n",
    "forest_rmse_scores = np.sqrt(-scores_forest) # \n",
    "print(\"\\nRandom Forest Cross-Validation Scores:\")\n",
    "display_scores(forest_rmse_scores) # \n",
    "\n",
    "# Wawasan: Random Forests terlihat menjanjikan, tetapi masih ada overfitting (skor training jauh lebih baik dari validation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Menyesuaikan Model Anda (*Fine-Tune Your Model*)**\n",
    "\n",
    "Setelah mendapatkan daftar model yang menjanjikan, kita perlu menyempurnakannya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Grid Search (Pencarian Grid)**\n",
    "\n",
    "Untuk menemukan kombinasi *hyperparameter* terbaik, kita dapat menggunakan `GridSearchCV`. Kita memberitahunya *hyperparameter* mana yang ingin kita coba dan nilai-nilai apa yang akan dicoba. Ini akan menggunakan *cross-validation* untuk mengevaluasi semua kombinasi yang mungkin. \n",
    "\n",
    "**Teori:**\n",
    "* **`GridSearchCV`:** Mengeksplorasi semua kombinasi *hyperparameter* yang ditentukan dalam `param_grid`.  Melakukan *cross-validation* untuk setiap kombinasi. \n",
    "* **`best_params_`:** Atribut yang menyimpan kombinasi *hyperparameter* terbaik yang ditemukan. \n",
    "* **`best_estimator_`:** Atribut yang menyimpan estimator terbaik yang dilatih pada seluruh *training set*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # Coba 12 kombinasi (3*4)\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, # \n",
    "    # Coba 6 kombinasi (2*3) dengan bootstrap=False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, # \n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True) # \n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels) # \n",
    "\n",
    "# Mendapatkan hyperparameter terbaik\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_) # \n",
    "\n",
    "# Mendapatkan estimator terbaik\n",
    "# print(\"Best estimator:\", grid_search.best_estimator_) # \n",
    "\n",
    "# Melihat skor evaluasi dari setiap kombinasi\n",
    "cvres = grid_search.cv_results_ # \n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params) # \n",
    "\n",
    "# Wawasan: Ditemukan kombinasi max_features dan n_estimators terbaik. \n",
    "# Penting: Jika nilai terbaik berada di batas rentang yang dicoba (misalnya 8 dan 30),\n",
    "# mungkin perlu dicoba rentang yang lebih luas lagi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Randomized Search (Pencarian Acak)**\n",
    "\n",
    "Ketika ruang pencarian *hyperparameter* besar, `RandomizedSearchCV` seringkali lebih disukai daripada `GridSearchCV`. Ini mengevaluasi sejumlah kombinasi acak. \n",
    "\n",
    "**Teori:**\n",
    "* **`RandomizedSearchCV`:** Mengevaluasi jumlah kombinasi acak yang diberikan (`n_iter`) dengan memilih nilai acak untuk setiap *hyperparameter* di setiap iterasi. \n",
    "* **Manfaat:**\n",
    "    * Mengeksplorasi lebih banyak nilai untuk setiap *hyperparameter*. \n",
    "    * Memberikan kontrol lebih besar atas anggaran komputasi (`n_iter`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Contoh param_distributions untuk RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(low=10, high=200),\n",
    "    'max_features': randint(low=1, high=8),\n",
    "}\n",
    "\n",
    "# randomized_search = RandomizedSearchCV(forest_reg, param_distributions, n_iter=10, cv=5,\n",
    "#                                       scoring='neg_mean_squared_error', random_state=42)\n",
    "# randomized_search.fit(housing_prepared, housing_labels)\n",
    "# print(\"\\nRandomized Search Best params:\", randomized_search.best_params_)\n",
    "# print(\"Randomized Search Best RMSE:\", np.sqrt(-randomized_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ensemble Methods (Metode Ensemble)**\n",
    "\n",
    "Cara lain untuk menyempurnakan sistem adalah dengan menggabungkan model-model yang berkinerja terbaik. Kelompok (atau \"ensemble\") seringkali berkinerja lebih baik daripada model individu terbaik, terutama jika model individu membuat jenis kesalahan yang sangat berbeda. \n",
    "\n",
    "**Teori:**\n",
    "* **Ensemble Learning:** Membangun model di atas banyak model lain.  Contohnya adalah Random Forests, yang menggabungkan banyak Decision Trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidak ada kode spesifik di sini, ini lebih ke konsep bahwa\n",
    "# metode ensemble seringkali memberikan hasil terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Menganalisis Model Terbaik dan Kesalahannya (Analyze the Best Models and Their Errors)**\n",
    "\n",
    "Kita bisa mendapatkan wawasan yang baik tentang masalah dengan memeriksa model terbaik. Misalnya, `RandomForestRegressor` dapat menunjukkan kepentingan relatif setiap atribut untuk membuat prediksi yang akurat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkan model terbaik dari grid search\n",
    "final_model = grid_search.best_estimator_ # \n",
    "\n",
    "# Mendapatkan kepentingan fitur\n",
    "feature_importances = final_model.feature_importances_ # \n",
    "\n",
    "# Mendapatkan nama atribut\n",
    "# Kita perlu daftar atribut numerik asli + atribut kustom + atribut one-hot encoded\n",
    "num_attribs_ = list(housing.drop(\"ocean_proximity\", axis=1).columns)\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder_ = full_pipeline.named_transformers_['cat'] # Mengakses OneHotEncoder dari pipeline\n",
    "cat_one_hot_attribs = list(cat_encoder_.categories_[0])\n",
    "attributes = num_attribs_ + extra_attribs + cat_one_hot_attribs # \n",
    "\n",
    "# Menampilkan kepentingan fitur yang diurutkan\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(sorted(zip(feature_importances, attributes), reverse=True)) # \n",
    "\n",
    "# Wawasan: median_income, INLAND, pop_per_hhold seringkali merupakan fitur paling penting.\n",
    "# Ini bisa menjadi dasar untuk feature selection atau re-engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Mengevaluasi Sistem pada *Test Set* (Evaluate Your System on the Test Set)**\n",
    "\n",
    "Setelah semua penyetelan selesai dan Anda yakin dengan model Anda, saatnya mengevaluasi model akhir pada *test set* yang belum pernah dilihat model sebelumnya. \n",
    "\n",
    "**Teori:**\n",
    "* **Tujuan Test Set:** Untuk mendapatkan estimasi *generalization error* yang tidak bias sebelum model diterapkan ke produksi. \n",
    "* **Penting:** Jangan menyesuaikan *hyperparameter* berdasarkan kinerja *test set*, karena ini akan menyebabkan *overfitting* pada *test set*. \n",
    "* **Confidence Interval:** Kita bisa menghitung interval kepercayaan untuk *generalization error* untuk mendapatkan gagasan tentang seberapa presisi estimasi tersebut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan fitur dan label dari test set\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1) # \n",
    "y_test = strat_test_set[\"median_house_value\"].copy() # \n",
    "\n",
    "# Mempersiapkan test set menggunakan pipeline yang sudah dilatih\n",
    "X_test_prepared = full_pipeline.transform(X_test) # \n",
    "\n",
    "# Membuat prediksi pada test set\n",
    "final_predictions = final_model.predict(X_test_prepared) # \n",
    "\n",
    "# Menghitung RMSE pada test set\n",
    "final_mse = mean_squared_error(y_test, final_predictions) # \n",
    "final_rmse = np.sqrt(final_mse) # \n",
    "print(f\"\\nFinal RMSE on the Test Set: {final_rmse}\") # \n",
    "\n",
    "# Menghitung interval kepercayaan 95% (opsional)\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "# print(\"95% Confidence Interval for Generalization Error:\",\n",
    "#       np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "#                                loc=squared_errors.mean(),\n",
    "#                                scale=stats.sem(squared_errors)))) # \n",
    "\n",
    "# Bandingkan RMSE final dengan estimasi ahli (misalnya, 20% kesalahan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Meluncurkan, Memantau, dan Memelihara Sistem Anda (Launch, Monitor, and Maintain Your System)**\n",
    "\n",
    "Langkah terakhir melibatkan persiapan model untuk produksi dan memastikan keberlanjutannya. \n",
    "\n",
    "**Teori:**\n",
    "* **Deployment:** Menyimpan model yang telah dilatih (misalnya menggunakan `joblib` atau format SavedModel TensorFlow) dan memuatnya di lingkungan produksi untuk membuat prediksi. \n",
    "* **Web Service/REST API:** Model dapat di-*wrap* dalam *web service* khusus yang dapat di-*query* oleh aplikasi web melalui REST API. Ini memisahkan model dari aplikasi utama dan mempermudah peningkatan versi serta penskalaan. \n",
    "* **Monitoring:** Menulis kode *monitoring* untuk memeriksa kinerja sistem secara teratur dan memicu *alert* jika kinerja menurun. \n",
    "    * **Model Rot:** Model cenderung \"membusuk\" seiring waktu karena dunia berubah (distribusi data input dapat berubah). \n",
    "    * **Human Raters:** Untuk beberapa tugas, mungkin perlu melibatkan penilai manusia untuk mengevaluasi kinerja model secara langsung (misalnya, dalam deteksi cacat produk). \n",
    "    * **Input Data Quality:** Memantau kualitas data input untuk mendeteksi masalah (misalnya, data hilang, nilai abnormal). \n",
    "* **Maintenance & Automation:** Mengotomatiskan proses pengumpulan data baru, pelabelan, pelatihan ulang model, evaluasi, dan penyebaran model baru. \n",
    "* **Backup & Rollback:** Pastikan untuk menyimpan cadangan setiap model dan versi dataset untuk kemungkinan *rollback* jika ada masalah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kode untuk menyimpan model (setelah tahap fine-tuning)\n",
    "import joblib\n",
    "\n",
    "# Ganti dengan path dan nama file yang sesuai\n",
    "# joblib.dump(final_model, \"my_housing_predictor.pkl\") # \n",
    "\n",
    "# Kode untuk memuat model (di lingkungan produksi)\n",
    "# my_model_loaded = joblib.load(\"my_housing_predictor.pkl\") # \n",
    "\n",
    "# Tidak ada kode eksekusi untuk monitoring dan maintenance, karena ini adalah aktivitas berkelanjutan.\n",
    "# Konsepnya adalah:\n",
    "# 1. Simpan model terbaik. \n",
    "# 2. Deploy model (misalnya sebagai web service). \n",
    "# 3. Tulis script untuk monitoring performa model secara berkala di produksi. \n",
    "# 4. Tulis script untuk otomatisasi pengumpulan data baru dan pelatihan ulang model. \n",
    "# 5. Tentukan kriteria untuk deployment otomatis model baru jika performanya bagus. \n",
    "# 6. Pastikan ada mekanisme rollback jika model baru bermasalah. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kesimpulan:**\n",
    "\n",
    "Kita telah melalui semua langkah utama dalam proyek *Machine Learning* end-to-end. Dari pemahaman masalah hingga persiapan data, pemilihan dan penyetelan model, serta evaluasi akhir dan pertimbangan *deployment*. Ingatlah bahwa sebagian besar pekerjaan dalam *Machine Learning* melibatkan persiapan data dan infrastruktur pendukung, bukan hanya algoritma itu sendiri. Ini adalah proses iteratif, dan pengalaman praktis adalah kunci untuk menjadi ahli *Machine Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Catatan untuk Pengguna Jupyter Notebook:**\n",
    "\n",
    "* Anda dapat menjalankan setiap blok kode secara terpisah.\n",
    "* Beberapa output mungkin bervariasi tergantung pada versi pustaka dan *random seed*.\n",
    "* Untuk bagian \"Create the Workspace\", perintah-perintahnya perlu dijalankan di terminal Anda, bukan di dalam notebook.\n",
    "* Jika Anda mengalami masalah dengan visualisasi Matplotlib di Jupyter, pastikan baris `%matplotlib inline` (atau `%matplotlib notebook` jika Anda ingin plot interaktif) ada di awal notebook Anda. \n",
    "* Untuk bagian *randomized search* dan *grid search*, prosesnya mungkin memakan waktu lama, tergantung pada spesifikasi komputer Anda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
