### **Ringkasan Bab 2: Proyek Machine Learning End-to-End**

Bab 2 dari buku "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" (Edisi ke-2) oleh Aurélien Géron menyajikan panduan praktis dan komprehensif tentang langkah-langkah dalam proyek Machine Learning (ML) yang realistis. Bab ini memposisikan pembaca sebagai seorang *data scientist* yang baru direkrut di perusahaan real estat, dengan tugas memprediksi harga perumahan di California berdasarkan data sensus.

Bab ini secara sistematis memandu pembaca melalui delapan langkah utama dalam alur kerja proyek ML:

1.  **Melihat Gambaran Besar (Look at the Big Picture)**: Tahap ini menekankan pentingnya memahami tujuan bisnis di balik proyek ML. Pembaca diajak untuk membingkai masalah (apakah ini masalah *supervised/unsupervised learning*, *classification/regression*, atau *batch/online learning*)  dan memilih metrik kinerja yang sesuai (misalnya, *Root Mean Square Error* untuk regresi). Selain itu, penting untuk memeriksa asumsi-asumsi awal agar tidak terjadi kesalahan fatal di kemudian hari.

2.  **Mendapatkan Data (Get the Data)**: Langkah ini mencakup proses pengumpulan data yang relevan. Penulis menyarankan untuk membuat ruang kerja yang terisolasi (misalnya dengan `virtualenv`)  dan mengotomatiskan proses pengunduhan data untuk mempermudah reproduksi dan pembaruan data di masa mendatang. Setelah data diunduh, dilakukan pemeriksaan cepat terhadap struktur data menggunakan metode pandas seperti `head()`, `info()`, `describe()`, dan `hist()` untuk memahami tipe dan distribusi atribut. **Poin krusial** pada tahap ini adalah membuat *test set* dan menyisihkannya segera untuk menghindari *data snooping bias* yang dapat menghasilkan evaluasi model yang terlalu optimis. Teknik *stratified sampling* diperkenalkan untuk memastikan *test set* merepresentasikan distribusi data secara keseluruhan.

3.  **Menjelajahi dan Memvisualisasikan Data untuk Mendapatkan Wawasan (Discover and Visualize the Data to Gain Insights)**: Pada tahap ini, pembaca didorong untuk menganalisis *training set* secara mendalam (tanpa menyentuh *test set*). Visualisasi data geografis menggunakan *scatter plot* sangat membantu untuk melihat pola spasial. Menghitung koefisien korelasi antar atribut  dan memvisualisasikannya dengan `scatter_matrix()` membantu mengidentifikasi hubungan penting dalam data. Eksperimen dengan kombinasi atribut baru (misalnya, `rooms_per_household`) juga dapat mengungkapkan fitur yang lebih informatif.

4.  **Mempersiapkan Data untuk Algoritma Machine Learning (Prepare the Data for Machine Learning Algorithms)**: Ini adalah salah satu tahap terpenting dan seringkali paling memakan waktu[cite: 92, 109]. Penulis menekankan untuk menulis fungsi-fungsi transformasi data agar prosesnya dapat direproduksi dan digunakan kembali[cite: 92, 109]. Tahap ini meliputi pembersihan data (menangani nilai yang hilang, misalnya dengan `SimpleImputer`) [cite: 93, 110], mengubah atribut teks dan kategorikal menjadi format numerik (misalnya dengan *one-hot encoding* menggunakan `OneHotEncoder`) [cite: 96, 97, 113, 114], membuat *custom transformer* untuk transformasi spesifik [cite: 98, 115], dan melakukan *feature scaling* (misalnya *standardization* dengan `StandardScaler`) untuk memastikan atribut memiliki skala yang serupa. Konsep *pipeline transformasi* menggunakan `Pipeline` dan `ColumnTransformer` dari Scikit-Learn diperkenalkan untuk mengurutkan dan mengotomatiskan langkah-langkah ini secara efisien.

5.  **Memilih dan Melatih Model (Select and Train a Model)**: Setelah data siap, berbagai model ML mulai dicoba. Bab ini mendemonstrasikan pelatihan dan evaluasi model *Linear Regression* dan *DecisionTreeRegressor* pada *training set*. Perbandingan performa awal menunjukkan potensi *underfitting* (pada *Linear Regression*) dan *overfitting* (pada *DecisionTreeRegressor*). Untuk evaluasi yang lebih handal dan mendeteksi *overfitting* secara akurat, teknik *K-fold cross-validation* diperkenalkan menggunakan `cross_val_score()`.

6.  **Menyesuaikan Model Anda (Fine-Tune Your Model)**: Setelah model-model awal diidentifikasi, fokus bergeser pada penyempurnaan *hyperparameter*. Metode *Grid Search* (`GridSearchCV`)  dan *Randomized Search* (`RandomizedSearchCV`)  dijelaskan sebagai cara otomatis untuk mengeksplorasi ruang *hyperparameter* dan menemukan kombinasi terbaik melalui *cross-validation*. Konsep *Ensemble Methods* (seperti *Random Forests*) juga ditekankan sebagai cara untuk meningkatkan kinerja model secara keseluruhan. Menganalisis model terbaik dan kesalahannya (misalnya, dengan melihat *feature importances* dari *Random Forests*) memberikan wawasan tambahan untuk perbaikan.

7.  **Mengevaluasi Sistem pada Test Set (Evaluate Your System on the Test Set)**: Setelah model disempurnakan dan diverifikasi pada *validation set*, langkah terakhir adalah mengevaluasinya pada *test set* yang benar-benar baru. Ini memberikan estimasi *generalization error* yang tidak bias sebelum model diluncurkan ke produksi. Penting untuk **tidak melakukan penyesuaian** pada model setelah evaluasi *test set* ini untuk menjaga integritas estimasi.

8.  **Meluncurkan, Memantau, dan Memelihara Sistem Anda (Launch, Monitor, and Maintain Your System)**: Tahap akhir ini membahas persiapan model untuk deployment (misalnya, menyimpan model dengan `joblib`  atau format SavedModel TensorFlow untuk *serving* ). Aspek penting lainnya adalah *monitoring* kinerja model di lingkungan produksi (misalnya, mendeteksi *model rot* karena perubahan data seiring waktu)  dan mengotomatiskan proses pelatihan ulang model secara berkala dengan data yang segar. Memiliki sistem *backup* dan *rollback* juga krusial untuk menghadapi potensi masalah.

Secara keseluruhan, Bab 2 berfungsi sebagai "laboratorium" mini yang membekali pembaca dengan pemahaman praktis dan keterampilan esensial untuk menjalani siklus hidup proyek Machine Learning secara lengkap, menekankan pentingnya setiap tahapan dan alat-alat praktis yang tersedia di Scikit-Learn.
