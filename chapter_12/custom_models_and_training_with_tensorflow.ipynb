{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bab 12 - Custom Models and Training with TensorFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pembelajaran Mesin: Bab 12 - Model Kustom dan Pelatihan dengan TensorFlow\n",
    "\n",
    "**Selamat datang di Bab 12!** Pada bab ini, kita akan menyelami lebih dalam TensorFlow, melampaui API tingkat tinggi `tf.keras` yang telah kita gunakan sejauh ini. Kita akan mempelajari bagaimana TensorFlow memungkinkan kita untuk memiliki kontrol lebih besar atas model dan proses pelatihan, yang sangat berguna ketika kita membutuhkan fungsionalitas yang lebih spesifik atau eksperimental.\n",
    "\n",
    "**Tujuan Pembelajaran Bab Ini:**\n",
    "* Memahami arsitektur dasar TensorFlow.\n",
    "* Mengenal dasar-dasar penggunaan TensorFlow seperti NumPy.\n",
    "* Membuat fungsi loss, metrik, lapisan, dan model kustom.\n",
    "* Mengimplementasikan *training loop* kustom.\n",
    "* Memanfaatkan fitur AutoGraph TensorFlow untuk mempercepat komputasi."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Tur Singkat TensorFlow\n",
    "\n",
    "TensorFlow adalah *library* komputasi numerik yang kuat, sangat cocok dan di-fine-tune untuk *Machine Learning* skala besar. Ini dikembangkan oleh tim Google Brain dan mendukung banyak layanan Google berskala besar.\n",
    "\n",
    "### Fitur Utama TensorFlow:\n",
    "* **Mirip NumPy dengan Dukungan GPU:** Inti TensorFlow sangat mirip dengan `NumPy ndarray`, tetapi dengan kemampuan untuk berjalan di GPU.\n",
    "* **Komputasi Terdistribusi:** Mendukung komputasi terdistribusi di berbagai perangkat dan server.\n",
    "* **Kompilator Just-In-Time (JIT):** Menyertakan sejenis kompilator *just-in-time* (JIT) yang memungkinkannya mengoptimalkan komputasi untuk kecepatan dan penggunaan memori. Ini bekerja dengan mengekstrak grafik komputasi dari fungsi Python, lalu mengoptimalkannya, dan akhirnya menjalankannya secara efisien (misalnya, dengan secara otomatis menjalankan operasi independen secara paralel).\n",
    "* **Portabilitas Model:** Grafik komputasi dapat diekspor ke format portabel, memungkinkan model TensorFlow dilatih di satu lingkungan (misalnya, menggunakan Python di Linux) dan dijalankan di lingkungan lain (misalnya, menggunakan Java di perangkat Android).\n",
    "* **Autodiff dan Optimizer:** Mengimplementasikan autodiff dan menyediakan beberapa *optimizer* yang sangat baik, seperti RMSProp dan Nadam, sehingga Anda dapat dengan mudah meminimalkan semua jenis fungsi *loss*.\n",
    "\n",
    "### Arsitektur TensorFlow\n",
    "Pada tingkat terendah, setiap operasi TensorFlow (disebut *op*) diimplementasikan menggunakan kode C++ yang sangat efisien. Banyak operasi memiliki banyak implementasi yang disebut *kernel*: setiap *kernel* didedikasikan untuk jenis perangkat tertentu, seperti CPU, GPU, atau bahkan TPU (*tensor processing units*).\n",
    "\n",
    "**[Tambahkan Diagram Arsitektur TensorFlow dari Gambar 12-2, hal. 378]**\n",
    "\n",
    "Sebagian besar waktu, kode Anda akan menggunakan API tingkat tinggi (terutama `tf.keras` dan `tf.data`). Namun, ketika Anda membutuhkan lebih banyak fleksibilitas, Anda akan menggunakan API Python tingkat rendah, menangani *tensor* secara langsung. TensorFlow berjalan tidak hanya di Windows, Linux, dan macOS, tetapi juga di perangkat seluler (menggunakan TensorFlow Lite), termasuk iOS dan Android. Bahkan ada implementasi JavaScript yang disebut TensorFlow.js yang memungkinkan model Anda berjalan langsung di *browser* Anda."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Menggunakan TensorFlow seperti NumPy\n",
    "\n",
    "API TensorFlow berpusat pada *tensor*, yang mengalir dari satu operasi ke operasi lainnyaâ€”itulah asal nama TensorFlow. Sebuah *tensor* sangat mirip dengan `NumPy ndarray`: biasanya merupakan *array* multidimensi, tetapi juga dapat menampung *skalar* (nilai tunggal, seperti 42). *Tensor* ini akan penting ketika kita membuat fungsi *loss* kustom, metrik kustom, lapisan kustom, dan lainnya, jadi mari kita lihat cara membuat dan memanipulasinya."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor dan Operasi\n",
    "\n",
    "Kita bisa membuat *tensor* dengan `tf.constant()`. Misalnya, berikut adalah *tensor* yang merepresentasikan matriks dengan dua baris dan tiga kolom *float*:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Membuat tensor matriks\n",
    "t_matrix = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(\"Tensor Matriks:\\n\", t_matrix)\n",
    "\n",
    "# Membuat tensor skalar\n",
    "t_scalar = tf.constant(42)\n",
    "print(\"Tensor Skalar:\\n\", t_scalar)\n",
    "\n",
    "# Mengakses shape dan dtype\n",
    "print(\"Shape t_matrix:\", t_matrix.shape)\n",
    "print(\"Dtype t_matrix:\", t_matrix.dtype)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sama seperti `ndarray`, `tf.Tensor` memiliki `shape` dan tipe data (`dtype`). Pengindeksan (indexing) pada *tensor* bekerja mirip dengan NumPy:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Pengindeksan mirip NumPy\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(\"Slice t[:, 1:]:\\n\", t[:, 1:])\n",
    "print(\"Slice t[..., 1, tf.newaxis]:\\n\", t[..., 1, tf.newaxis])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yang paling penting, semua jenis operasi *tensor* tersedia:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Operasi tensor\n",
    "print(\"t + 10:\\n\", t + 10)\n",
    "print(\"tf.square(t):\\n\", tf.square(t))\n",
    "\n",
    "# Perkalian matriks\n",
    "print(\"t @ tf.transpose(t):\\n\", t @ tf.transpose(t))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perhatikan bahwa menulis `t + 10` setara dengan memanggil `tf.add(t, 10)`. Operator lain seperti `-` dan `*` juga didukung. Operator `@` ditambahkan di Python 3.5, untuk perkalian matriks: ini setara dengan memanggil fungsi `tf.matmul()`.\n",
    "\n",
    "**Catatan:** Banyak fungsi dan kelas memiliki alias (misalnya, `tf.add()` dan `tf.math.add()` adalah fungsi yang sama). Ini memungkinkan TensorFlow memiliki nama yang ringkas untuk operasi yang paling umum sambil mempertahankan paket yang terorganisir dengan baik.\n",
    "\n",
    "### API Tingkat Rendah Keras\n",
    "API Keras memiliki API tingkat rendah sendiri, yang terletak di `keras.backend`. Ini mencakup fungsi-fungsi seperti `square()`, `exp()`, dan `sqrt()`. Di `tf.keras`, fungsi-fungsi ini umumnya hanya memanggil operasi TensorFlow yang sesuai. Jika Anda ingin menulis kode yang akan portabel ke implementasi Keras lainnya, Anda harus menggunakan fungsi-fungsi Keras ini."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "\n",
    "t_keras = tf.constant([[1., 2.], [3., 4.]])\n",
    "print(\"K.square(K.transpose(t_keras)) + 10:\\n\", K.square(K.transpose(t_keras)) + 10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor dan NumPy\n",
    "\n",
    "*Tensor* dan NumPy berinteraksi dengan baik: Anda dapat membuat *tensor* dari *array* NumPy, dan sebaliknya. Anda bahkan dapat menerapkan operasi TensorFlow ke *array* NumPy dan operasi NumPy ke *tensor*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "a_np = np.array([2., 4., 5.])\n",
    "print(\"tf.constant(a_np):\\n\", tf.constant(a_np))\n",
    "\n",
    "t_tf = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(\"t_tf.numpy():\\n\", t_tf.numpy()) # atau np.array(t_tf)\n",
    "\n",
    "print(\"tf.square(a_np):\\n\", tf.square(a_np))\n",
    "print(\"np.square(t_tf):\\n\", np.square(t_tf))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Penting:** NumPy menggunakan presisi 64-bit secara *default*, sedangkan TensorFlow menggunakan 32-bit. Ini karena presisi 32-bit umumnya lebih dari cukup untuk jaringan saraf, ditambah lagi berjalan lebih cepat dan menggunakan lebih sedikit RAM. Jadi, saat Anda membuat *tensor* dari *array* NumPy, pastikan untuk mengatur `dtype=tf.float32`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Konversi Tipe\n",
    "Konversi tipe dapat sangat merusak kinerja, dan dapat dengan mudah tidak disadari ketika dilakukan secara otomatis. Untuk menghindarinya, TensorFlow tidak melakukan konversi tipe secara otomatis: ia hanya akan memberikan pengecualian jika Anda mencoba menjalankan operasi pada *tensor* dengan tipe yang tidak kompatibel. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40) # Akan menyebabkan error\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(e)\n",
    "\n",
    "# Gunakan tf.cast() untuk konversi tipe eksplisit\n",
    "t2_tf64 = tf.constant(40., dtype=tf.float64)\n",
    "print(\"tf.constant(2.0) + tf.cast(t2_tf64, tf.float32):\\n\", tf.constant(2.0) + tf.cast(t2_tf64, tf.float32))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variabel\n",
    "Nilai `tf.Tensor` yang telah kita lihat sejauh ini bersifat *immutable* (tidak dapat diubah). Ini berarti kita tidak dapat menggunakan *tensor* biasa untuk mengimplementasikan bobot dalam jaringan saraf, karena perlu diubah dengan *backpropagation*. Ditambah lagi, parameter lain juga mungkin perlu berubah seiring waktu (misalnya, *momentum optimizer* melacak *gradient* sebelumnya). Yang kita butuhkan adalah `tf.Variable`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(\"Variabel v:\\n\", v)\n",
    "\n",
    "# Variabel dapat dimodifikasi di tempat\n",
    "v.assign(2 * v)\n",
    "print(\"v setelah v.assign(2 * v):\\n\", v)\n",
    "\n",
    "v[0, 1].assign(42)\n",
    "print(\"v setelah v[0, 1].assign(42):\\n\", v)\n",
    "\n",
    "v[:, 2].assign([0., 1.])\n",
    "print(\"v setelah v[:, 2].assign([0., 1.]):\\n\", v)\n",
    "\n",
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
    "print(\"v setelah v.scatter_nd_update():\\n\", v)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dalam praktiknya, Anda jarang perlu membuat variabel secara manual, karena Keras menyediakan metode `add_weight()` yang akan menanganinya untuk Anda. Ditambah lagi, parameter model umumnya akan diperbarui langsung oleh *optimizer*, jadi Anda jarang perlu memperbarui variabel secara manual."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Struktur Data Lain\n",
    "TensorFlow mendukung beberapa struktur data lain, antara lain:\n",
    "* **Sparse Tensors (`tf.SparseTensor`):** Merepresentasikan *tensor* yang sebagian besar berisi nol secara efisien.\n",
    "* **Tensor Arrays (`tf.TensorArray`):** Adalah daftar *tensor*. Mereka memiliki ukuran tetap secara *default* tetapi secara opsional dapat dibuat dinamis. Semua *tensor* yang mereka muat harus memiliki *shape* dan tipe data yang sama.\n",
    "* **Ragged Tensors (`tf.RaggedTensor`):** Merepresentasikan daftar *list tensor* statis, di mana setiap *tensor* memiliki *shape* dan tipe data yang sama.\n",
    "* **String Tensors:** *Tensor* biasa dengan tipe `tf.string`. Ini merepresentasikan *byte string*, bukan *Unicode string*.\n",
    "* **Sets:** Direpresentasikan sebagai *tensor* biasa (atau *sparse tensor*). Misalnya, `tf.constant([[1, 2], [3, 4]])` merepresentasikan dua *set* `{1, 2}` dan `{3, 4}`.\n",
    "* **Queues:** Menyimpan *tensor* di beberapa langkah. TensorFlow menawarkan berbagai jenis *queue*: *queue* *First In, First Out* (FIFO) sederhana (`FIFOQueue`), *queue* yang dapat memprioritaskan beberapa *item* (`PriorityQueue`), mengacak *item* mereka (`RandomShuffleQueue`), dan *batch item* dengan *shape* yang berbeda dengan *padding* (`PaddingFIFOQueue`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Menyesuaikan Model dan Algoritma Pelatihan\n",
    "\n",
    "Dengan *tensor*, operasi, variabel, dan berbagai struktur data yang tersedia, kita siap untuk menyesuaikan model dan algoritma pelatihan kita."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fungsi Loss Kustom (Custom Loss Functions)\n",
    "\n",
    "Misalnya, kita ingin menggunakan *Huber loss* (diperkenalkan di Bab 10) sebagai fungsi *loss*. Meskipun sudah tersedia di `tf.keras`, kita akan mengimplementasikannya sendiri untuk demonstrasi. Cukup buat fungsi yang menerima *label* dan prediksi sebagai argumen, dan gunakan operasi TensorFlow untuk menghitung *loss* setiap *instance*:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Contoh penggunaan\n",
    "model = keras.models.Sequential([keras.layers.Dense(1, input_shape=[1])])\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "\n",
    "# Data dummy untuk demonstrasi\n",
    "X_train_dummy = tf.constant(np.random.rand(100, 1), dtype=tf.float32)\n",
    "y_train_dummy = tf.constant(np.random.rand(100, 1), dtype=tf.float32)\n",
    "\n",
    "model.fit(X_train_dummy, y_train_dummy, epochs=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Penting untuk mengembalikan *tensor* yang berisi satu *loss* per *instance* daripada *mean loss*, agar Keras dapat menerapkan *class weights* atau *sample weights* jika diperlukan."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Menyimpan dan Memuat Model yang Mengandung Komponen Kustom\n",
    "\n",
    "Menyimpan model yang berisi fungsi *loss* kustom berfungsi dengan baik, karena Keras menyimpan nama fungsi tersebut. Setiap kali Anda memuatnya, Anda perlu menyediakan kamus yang memetakan nama fungsi ke fungsi aktual."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Simpan model\n",
    "model.save(\"my_model_with_a_custom_loss.h5\")\n",
    "\n",
    "# Muat model (dengan menyediakan fungsi kustom)\n",
    "loaded_model = keras.models.load_model(\n",
    "    \"my_model_with_a_custom_loss.h5\",\n",
    "    custom_objects={\"huber_fn\": huber_fn}\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jika fungsi *loss* kustom Anda memiliki *hyperparameter* (misalnya, *threshold* untuk *Huber loss*), Anda harus membuat *subclass* dari `keras.losses.Loss` dan mengimplementasikan metode `get_config()` untuk menyimpan *hyperparameter* tersebut."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# Contoh penggunaan\n",
    "model_huber_class = keras.models.Sequential([keras.layers.Dense(1, input_shape=[1])])\n",
    "model_huber_class.compile(loss=HuberLoss(2.0), optimizer=\"nadam\")\n",
    "model_huber_class.fit(X_train_dummy, y_train_dummy, epochs=2)\n",
    "\n",
    "# Simpan dan muat kembali\n",
    "model_huber_class.save(\"my_model_huber_class.h5\")\n",
    "loaded_model_huber_class = keras.models.load_model(\n",
    "    \"my_model_huber_class.h5\",\n",
    "    custom_objects={\"HuberLoss\": HuberLoss}\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fungsi Aktivasi Kustom, Initializer, Regularizer, dan Constraints\n",
    "\n",
    "Sebagian besar fungsionalitas Keras, seperti *loss*, *regularizer*, *constraint*, *initializer*, metrik, fungsi aktivasi, lapisan, dan bahkan model lengkap, dapat disesuaikan dengan cara yang sama. Seringkali, Anda hanya perlu menulis fungsi sederhana dengan input dan output yang sesuai."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Fungsi aktivasi kustom\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "# Initializer kustom (Glorot normal)\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# Regularizer kustom (L1)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# Constraint kustom (hanya bobot positif)\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "# Contoh penggunaan dalam layer Dense\n",
    "layer_custom = keras.layers.Dense(\n",
    "    30,\n",
    "    activation=my_softplus,\n",
    "    kernel_initializer=my_glorot_initializer,\n",
    "    kernel_regularizer=my_l1_regularizer,\n",
    "    kernel_constraint=my_positive_weights\n",
    ")\n",
    "\n",
    "# Jika fungsi kustom memiliki hyperparameter yang perlu disimpan,\n",
    "# subclass class yang sesuai (misalnya, keras.regularizers.Regularizer)\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "\n",
    "# Contoh penggunaan MyL1Regularizer\n",
    "model_reg_custom = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation=\"relu\", input_shape=[10]),\n",
    "    keras.layers.Dense(1, activity_regularizer=MyL1Regularizer(0.01))\n",
    "])\n",
    "model_reg_custom.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_reg_custom.fit(tf.random.normal([100, 10]), tf.random.normal([100, 1]), epochs=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metrik Kustom\n",
    "\n",
    "*Loss* dan metrik secara konseptual tidak sama: *loss* (misalnya, *cross entropy*) digunakan oleh *Gradient Descent* untuk melatih model, jadi harus *differentiable* (setidaknya di mana mereka dievaluasi), dan *gradient* mereka tidak boleh 0 di mana-mana. Sebaliknya, metrik (misalnya, akurasi) digunakan untuk mengevaluasi model: mereka harus lebih mudah diinterpretasikan, dan mereka bisa tidak *differentiable* atau memiliki *gradient* 0 di mana-mana.\n",
    "\n",
    "Meskipun demikian, dalam kebanyakan kasus, mendefinisikan fungsi metrik kustom sama persis dengan mendefinisikan fungsi *loss* kustom. Namun, untuk metrik *streaming* (yang diperbarui secara bertahap di setiap *batch* seperti *precision* atau *recall*), Anda perlu membuat *subclass* dari `keras.metrics.Metric`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = huber_fn # Menggunakan huber_fn yang sudah didefinisikan\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# Contoh penggunaan\n",
    "model_metric = keras.models.Sequential([keras.layers.Dense(1, input_shape=[1])])\n",
    "model_metric.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[HuberMetric(2.0)])\n",
    "model_metric.fit(X_train_dummy, y_train_dummy, epochs=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lapisan Kustom (Custom Layers)\n",
    "\n",
    "Untuk membuat lapisan kustom tanpa bobot (misalnya, lapisan aktivasi), Anda dapat menggunakan `keras.layers.Lambda`. Contoh berikut adalah lapisan yang akan menerapkan fungsi eksponensial ke inputnya:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Layer kustom tanpa bobot\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "\n",
    "# Contoh penggunaan\n",
    "model_lambda = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=[10]),\n",
    "    exponential_layer\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Untuk membuat lapisan kustom dengan bobot, Anda perlu membuat *subclass* dari `keras.layers.Layer` dan mengimplementasikan metode `__init__()`, `build()`, `call()`, dan opsional `compute_output_shape()` serta `get_config()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # Harus di akhir build()\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}\n",
    "\n",
    "# Contoh penggunaan MyDense\n",
    "model_my_dense = keras.models.Sequential([\n",
    "    MyDense(10, activation=\"relu\", input_shape=[10])\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jika lapisan Anda memiliki perilaku berbeda selama pelatihan dan pengujian (misalnya, menggunakan *Dropout* atau *BatchNormalization*), Anda harus menambahkan argumen `training` ke metode `call()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "\n",
    "# Contoh penggunaan\n",
    "model_noise = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=[10]),\n",
    "    MyGaussianNoise(0.1)\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Kustom\n",
    "\n",
    "Kita sudah membahas pembuatan kelas model kustom di Bab 10 dengan Subclassing API. Ini melibatkan *subclassing* kelas `keras.Model`, membuat lapisan dan variabel di konstruktor, dan mengimplementasikan metode `call()` untuk menentukan perilaku model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Contoh Residual Block (dari buku)\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                           kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z # Skip connection\n",
    "\n",
    "# Contoh Model Kustom (dari buku)\n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                           kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3): # 1+3 blocks (from the book's example)\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z) # This is where the second block is applied\n",
    "        return self.out(Z)\n",
    "\n",
    "# Menggunakan model kustom\n",
    "model_custom = ResidualRegressor(1, input_shape=[8]) # input_shape here is for example\n",
    "model_custom.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "# Latih model_custom dengan data yang sesuai\n",
    "# model_custom.fit(X_train_data, y_train_data, epochs=...)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Losses dan Metrik Berbasis Internal Model\n",
    "\n",
    "Kadang-kadang, Anda mungkin ingin mendefinisikan *loss* atau metrik berdasarkan bagian lain dari model Anda, seperti bobot atau aktivasi lapisan tersembunyi. Ini dapat berguna untuk tujuan regularisasi atau untuk memantau aspek internal model Anda. Anda dapat mencapai ini dengan menghitung *loss* atau metrik tersebut di dalam metode `call()` model kustom Anda dan kemudian meneruskannya ke metode `add_loss()` atau `add_metric()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                           kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss) # Menambahkan reconstruction loss\n",
    "        return self.out(Z)\n",
    "\n",
    "# Contoh penggunaan\n",
    "model_reconstruct = ReconstructingRegressor(1, input_shape=[8]) # input_shape here is for example\n",
    "model_reconstruct.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "# model_reconstruct.fit(X_train_data, X_train_data, epochs=...) # Latih dengan input sebagai target"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Menghitung Gradient Menggunakan Autodiff\n",
    "\n",
    "Untuk memahami cara menggunakan *autodiff* (dijelaskan di Bab 10 dan Lampiran D) untuk menghitung *gradient* secara otomatis, mari kita pertimbangkan fungsi mainan sederhana: `f(w1, w2) = 3 * w1 ** 2 + 2 * w1 * w2`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "# Menggunakan tf.GradientTape untuk menghitung gradient\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(\"Gradients:\", gradients)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Secara *default*, *tape* akan secara otomatis dihapus segera setelah Anda memanggil metode `gradient()`-nya. Jika Anda perlu memanggil `gradient()` lebih dari sekali, Anda harus membuat *tape* *persistent* dan menghapusnya setiap kali Anda selesai dengannya untuk membebaskan sumber daya."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Tape persistent\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "print(\"dz_dw1 (persistent):\", dz_dw1)\n",
    "print(\"dz_dw2 (persistent):\", dz_dw2)\n",
    "del tape # Penting untuk menghapus tape persistent setelah digunakan"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Secara *default*, *tape* hanya akan melacak operasi yang melibatkan variabel. Namun, Anda dapat memaksa *tape* untuk mengawasi *tensor* apa pun yang Anda inginkan, untuk merekam setiap operasi yang melibatkannya."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients_watched = tape.gradient(z, [c1, c2])\n",
    "print(\"Gradients (watched constants):\", gradients_watched)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Untuk menghentikan *gradient* dari *backpropagating* melalui bagian tertentu dari jaringan, gunakan `tf.stop_gradient()`. Fungsi ini mengembalikan inputnya selama *forward pass*, tetapi tidak membiarkan *gradient* melewati selama *backpropagation*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def f_stop_gradient(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "w1_sg, w2_sg = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z_sg = f_stop_gradient(w1_sg, w2_sg)\n",
    "\n",
    "gradients_sg = tape.gradient(z_sg, [w1_sg, w2_sg])\n",
    "print(\"Gradients (with stop_gradient):\", gradients_sg)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Perulangan Pelatihan Kustom (Custom Training Loops)\n",
    "\n",
    "Dalam beberapa kasus yang jarang terjadi, metode `fit()` mungkin tidak cukup fleksibel untuk kebutuhan Anda (misalnya, menggunakan *optimizer* yang berbeda untuk bagian yang berbeda dari jaringan). Menulis *training loop* kustom akan membuat kode Anda lebih panjang, lebih rentan terhadap kesalahan, dan lebih sulit untuk dipelihara. Namun, ia memberi Anda kontrol penuh."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Dummy data untuk pelatihan\n",
    "X_train_scaled = tf.constant(np.random.rand(100, 8), dtype=tf.float32)\n",
    "y_train_scaled = tf.constant(np.random.rand(100, 1), dtype=tf.float32)\n",
    "\n",
    "# Model sederhana\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model_custom_loop = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg, input_shape=[8]),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "# Fungsi untuk mengambil batch acak\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# Fungsi untuk menampilkan status bar (versi sederhana)\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\" {}: {:.4f}\".format(m.name, m.result())\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} -\".format(iteration, total) + metrics, end=end)\n",
    "\n",
    "# Hyperparameter pelatihan\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train_scaled) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# Perulangan pelatihan kustom\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train_scaled)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model_custom_loop(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model_custom_loop.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model_custom_loop.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model_custom_loop.trainable_variables))\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        \n",
    "        print_status_bar(step * batch_size, len(X_train_scaled), mean_loss, metrics)\n",
    "    \n",
    "    print_status_bar(len(X_train_scaled), len(X_train_scaled), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Fungsi dan Grafik TensorFlow\n",
    "\n",
    "Di TensorFlow 1, grafik tidak dapat dihindari (begitu juga dengan kompleksitas yang menyertainya) karena mereka adalah bagian sentral dari API TensorFlow. Di TensorFlow 2, mereka masih ada, tetapi tidak terlalu sentral, dan jauh lebih sederhana untuk digunakan. Untuk menunjukkan betapa sederhananya, mari kita mulai dengan fungsi trivial yang menghitung pangkat tiga dari inputnya:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    print(\"x =\", x) # Ini hanya akan dieksekusi saat tracing\n",
    "    return x ** 3\n",
    "\n",
    "print(\"Hasil tf_cube(2.0):\", tf_cube(tf.constant(2.0)))\n",
    "print(\"Hasil tf_cube(3.0):\", tf_cube(tf.constant(3.0))) # Reuses existing graph\n",
    "print(\"Hasil tf_cube(2):\", tf_cube(2)) # New Python value, new graph is traced"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Di bawah kap, `tf.function()` menganalisis komputasi yang dilakukan oleh fungsi `cube()` dan menghasilkan *computation graph* yang setara. TensorFlow mengoptimalkan *computation graph*, memangkas *node* yang tidak digunakan, menyederhanakan ekspresi (misalnya, `1 + 2` akan diganti dengan `3`), dan lainnya. Setelah *graph* yang dioptimalkan siap, *TF Function* secara efisien mengeksekusi operasi dalam *graph*, dalam urutan yang sesuai (dan secara paralel jika bisa). Sebagai hasilnya, *TF Function* biasanya akan berjalan jauh lebih cepat daripada fungsi Python aslinya, terutama jika melakukan komputasi yang kompleks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AutoGraph dan Tracing\n",
    "\n",
    "Bagaimana TensorFlow menghasilkan grafik? Ini dimulai dengan menganalisis kode sumber fungsi Python untuk menangkap semua pernyataan kontrol aliran, seperti loop `for`, loop `while`, dan pernyataan `if`, serta pernyataan `break`, `continue`, dan `return`. Langkah pertama ini disebut *AutoGraph*. Setelah menganalisis kode fungsi, *AutoGraph* menghasilkan versi fungsi yang ditingkatkan di mana semua pernyataan kontrol aliran diganti dengan operasi TensorFlow yang sesuai.\n",
    "\n",
    "**[Tambahkan Diagram AutoGraph dan Tracing dari Gambar 12-4, hal. 408]**\n",
    "\n",
    "`for` loop hanya akan ditangkap sebagai *loop* dinamis dalam *graph* jika berulang pada `tf.range()`, bukan `range()`. Ini memberi Anda pilihan:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def add_10_dynamic(x):\n",
    "    for i in tf.range(10): # Menggunakan tf.range\n",
    "        x += 1\n",
    "    return x\n",
    "\n",
    "print(\"Hasil add_10_dynamic(0):\", add_10_dynamic(tf.constant(0)))\n",
    "# Periksa operasi graph untuk melihat While op\n",
    "# print(add_10_dynamic.get_concrete_function(tf.constant(0)).graph.get_operations())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aturan Fungsi TF\n",
    "Sebagian besar waktu, mengubah fungsi Python yang melakukan operasi TensorFlow menjadi *TF Function* sangatlah mudah: cukup *decorate* dengan `@tf.function` atau biarkan Keras menanganinya untuk Anda. Namun, ada beberapa aturan yang harus dipatuhi:\n",
    "* **Operasi TensorFlow:** Jika Anda memanggil *library* eksternal apa pun, termasuk NumPy atau bahkan *standard library*, panggilan ini hanya akan berjalan selama *tracing*; itu tidak akan menjadi bagian dari *graph*. Jadi, pastikan Anda menggunakan `tf.reduce_sum()` daripada `np.sum()`, `tf.sort()` daripada fungsi bawaan `sorted()`, dan seterusnya (kecuali Anda benar-benar ingin kode tersebut hanya berjalan selama *tracing*).\n",
    "* **Efek Samping:** Jika kode non-TensorFlow Anda memiliki efek samping (misalnya, mencatat sesuatu atau memperbarui *counter* Python), maka Anda tidak boleh berharap efek samping tersebut terjadi setiap kali Anda memanggil *TF Function*, karena itu hanya akan terjadi saat fungsi tersebut di-*trace*.\n",
    "* **Pembuatan Variabel:** Jika fungsi membuat variabel TensorFlow (atau objek TensorFlow *stateful* lainnya, seperti *dataset* atau *queue*), ia harus melakukannya pada panggilan pertama, dan hanya pada saat itu, jika tidak, Anda akan mendapatkan pengecualian. Umumnya lebih baik membuat variabel di luar *TF Function* (misalnya, di metode `build()` dari lapisan kustom). Jika Anda ingin menetapkan nilai baru ke variabel, pastikan Anda memanggil metode `assign()`-nya, daripada menggunakan operator `=`.\n",
    "* **Kode Sumber:** Kode sumber fungsi Python Anda harus tersedia untuk TensorFlow. Jika kode sumber tidak tersedia, maka proses pembuatan *graph* akan gagal atau memiliki fungsionalitas terbatas.\n",
    "* **Loop Dinamis:** TensorFlow hanya akan menangkap loop `for` yang berulang pada *tensor* atau *dataset*. Jadi pastikan Anda menggunakan `for i in tf.range(x)` daripada `for i in range(x)`, jika tidak, loop tersebut tidak akan ditangkap dalam *graph*.\n",
    "* **Vektorisasi:** Seperti biasa, untuk alasan kinerja, Anda harus lebih memilih implementasi *vectorized* kapan pun Anda bisa, daripada menggunakan *loop*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Demikianlah kerangka Jupyter Notebook untuk Bab 12.** Anda dapat mengisi bagian \"dummy data\" dengan data yang lebih relevan (misalnya, dari dataset California Housing atau Fashion MNIST) dan menambahkan visualisasi serta analisis tambahan untuk memperdalam pemahaman Anda. Pastikan untuk menjalankan kode secara bertahap dan mengamati keluarannya."
   ],
   "metadata": {}
  }
 ]
}