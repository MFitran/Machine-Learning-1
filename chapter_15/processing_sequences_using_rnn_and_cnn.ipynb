{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El0N5SvuVnKY"
      },
      "source": [
        "# Bab 15: Memproses Urutan Menggunakan RNN dan CNN\n",
        "\n",
        "Bab ini akan membahas Jaringan Neural Berulang (Recurrent Neural Networks - RNNs), yang merupakan kelas jaringan saraf yang dirancang khusus untuk memproses data sekuensial, seperti deret waktu, teks, dan audio. Kita juga akan melihat bagaimana Convolutional Neural Networks (CNNs) 1D dapat digunakan untuk tugas serupa. Tujuan utamanya adalah untuk memahami konsep inti, implementasi praktis menggunakan TensorFlow dan Keras, serta mengatasi tantangan umum dalam pelatihan model-model ini.\n",
        "\n",
        "**Referensi Utama:** *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems* (O’Reilly) oleh Aurélien Géron (Edisi ke-2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUC97B6aVnKh"
      },
      "source": [
        "## 15.1 Neuron dan Lapisan Berulang (Recurrent Neurons and Layers)\n",
        "\n",
        "### Teori\n",
        "Jaringan Neural Berulang (RNN) berbeda dari jaringan *feedforward* tradisional karena mereka memiliki koneksi yang menunjuk ke belakang, memungkinkan informasi untuk bertahan (memori) dari satu langkah waktu ke langkah waktu berikutnya. Ini membuat RNNs sangat cocok untuk memproses urutan data, di mana urutan data sebelumnya mungkin relevan untuk prediksi saat ini atau di masa depan.\n",
        "\n",
        "**Neuron Berulang Tunggal:**\n",
        "RNN paling sederhana terdiri dari satu neuron yang menerima input $x^{(t)}$ pada setiap langkah waktu $t$, dan juga menerima outputnya sendiri dari langkah waktu sebelumnya $y^{(t-1)}$. Output pertama $y^{(0)}$ biasanya diatur ke 0.\n",
        "\n",
        "Secara matematis, output neuron berulang pada langkah waktu $t$ dihitung sebagai:\n",
        "$$y^{(t)} = \\phi(W_{x}^T x^{(t)} + W_{y}^T y^{(t-1)} + b)$$\n",
        "Di mana:\n",
        "* $x^{(t)}$ adalah vektor input pada langkah waktu $t$.\n",
        "* $y^{(t-1)}$ adalah output dari neuron pada langkah waktu $t-1$.\n",
        "* $W_x$ adalah matriks bobot untuk input saat ini.\n",
        "* $W_y$ adalah matriks bobot untuk output berulang (dari langkah waktu sebelumnya).\n",
        "* $b$ adalah vektor bias.\n",
        "* $\\phi$ adalah fungsi aktivasi (misalnya, ReLU atau Tanh).\n",
        "\n",
        "**Lapisan Neuron Berulang:**\n",
        "Ketika kita memiliki lapisan neuron berulang, setiap neuron dalam lapisan menerima vektor input $x^{(t)}$ dan vektor output dari langkah waktu sebelumnya $Y^{(t-1)}$ (yang merupakan kumpulan output dari semua neuron di lapisan pada langkah waktu sebelumnya).\n",
        "\n",
        "Output lapisan berulang untuk seluruh mini-batch pada langkah waktu $t$ dapat dihitung secara efisien sebagai:\n",
        "$$Y^{(t)} = \\phi(X^{(t)}W_x + Y^{(t-1)}W_y + b)$$\n",
        "Atau secara lebih ringkas:\n",
        "$$Y^{(t)} = \\phi([X^{(t)} Y^{(t-1)}]W + b)$$\n",
        "Di mana:\n",
        "* $X^{(t)}$ adalah matriks input untuk seluruh mini-batch pada langkah waktu $t$.\n",
        "* $Y^{(t-1)}$ adalah matriks output dari lapisan pada langkah waktu $t-1$.\n",
        "* $W_x$ dan $W_y$ adalah matriks bobot yang digabungkan menjadi satu matriks $W$.\n",
        "\n",
        "Output $Y^{(t)}$ adalah fungsi dari semua input sebelumnya $X^{(0)}, X^{(1)}, \\dots, X^{(t)}$. Oleh karena itu, neuron berulang dapat dikatakan memiliki bentuk memori.\n",
        "\n",
        "**Sel Memori (Memory Cells):**\n",
        "Bagian dari jaringan neural yang mempertahankan suatu keadaan (state) melintasi langkah waktu disebut sel memori (atau hanya sel). Neuron berulang tunggal atau lapisan neuron berulang adalah sel yang sangat dasar, yang hanya mampu mempelajari pola-pola pendek.\n",
        "\n",
        "Keadaan (state) sel pada langkah waktu $t$, yang dilambangkan dengan $h^{(t)}$ (singkatan dari 'hidden' atau 'tersembunyi'), adalah fungsi dari beberapa input pada langkah waktu tersebut dan keadaannya pada langkah waktu sebelumnya: $h^{(t)} = f(h^{(t-1)}, x^{(t)})$. Outputnya pada langkah waktu $t$, dilambangkan dengan $y^{(t)}$, juga merupakan fungsi dari keadaan sebelumnya dan input saat ini. Dalam sel dasar, output $y^{(t)}$ sama dengan state $h^{(t)}$, tetapi dalam sel yang lebih kompleks, ini tidak selalu sama.\n",
        "\n",
        "### Implementasi dengan Keras\n",
        "Mari kita coba membuat lapisan `SimpleRNN` di Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "zb7a0Zv9VnKj",
        "outputId": "59321537-a5d2-4f9d-fd46-4c214730cbd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape X_train: (7000, 50, 1)\n",
            "Shape y_train: (7000, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fungsi untuk menghasilkan deret waktu sintetis\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps) # [0, 1] dalam n_steps\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # Gelombang 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))  # + Gelombang 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)  # + Noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "# Parameter deret waktu\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "\n",
        "# Membagi dataset menjadi set pelatihan, validasi, dan pengujian\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape}\")\n",
        "print(f\"Shape y_train: {y_train.shape}\")\n",
        "\n",
        "# Membuat model SimpleRNN sederhana\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkrzzezmVnKn"
      },
      "source": [
        "### Penjelasan Code\n",
        "1. **`generate_time_series`**: Fungsi ini membuat deret waktu sintetis. Setiap deret waktu adalah jumlah dua gelombang sinus dengan frekuensi dan fase acak, ditambah sedikit *noise*. Deret waktu ini bersifat univariat (satu nilai per langkah waktu).\n",
        "2. **Pembagian Data**: Dataset kemudian dibagi menjadi set pelatihan (7000 deret waktu), set validasi (2000 deret waktu), dan set pengujian (1000 deret waktu). `X` berisi 50 langkah waktu pertama, dan `y` berisi nilai langkah waktu ke-51 (target yang akan diprediksi).\n",
        "3. **`keras.layers.SimpleRNN(1, input_shape=[None, 1])`**: Ini adalah lapisan RNN paling sederhana yang dapat dibangun.\n",
        "   - `1`: Menunjukkan bahwa lapisan ini memiliki satu neuron berulang.\n",
        "   - `input_shape=[None, 1]`:\n",
        "     - `None`: Menunjukkan bahwa panjang urutan input (jumlah langkah waktu) dapat bervariasi. RNN dapat memproses urutan dengan jumlah langkah waktu berapa pun.\n",
        "     - `1`: Menunjukkan dimensi input pada setiap langkah waktu. Dalam kasus deret waktu univariat, ini berarti satu fitur input per langkah waktu.\n",
        "   - Secara *default*, lapisan `SimpleRNN` menggunakan fungsi aktivasi *hyperbolic tangent* (`tanh`).\n",
        "   - Lapisan berulang di Keras secara *default* hanya mengembalikan output terakhir dari urutan. Untuk mengembalikan output di setiap langkah waktu, kita perlu mengatur `return_sequences=True` (akan dibahas nanti).\n",
        "   - Total parameter untuk lapisan ini hanya 3: 1 bobot untuk input saat ini ($W_x$), 1 bobot untuk output berulang sebelumnya ($W_y$), dan 1 bias ($b$).\n",
        "\n",
        "### Catatan\n",
        "Perlu diingat bahwa untuk deret waktu (dan jenis urutan lainnya), fitur input umumnya direpresentasikan sebagai array 3D dengan bentuk `[batch_size, time_steps, dimensionality]`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-yewNwEVnKo"
      },
      "source": [
        "## 15.2 Input dan Output Urutan (Input and Output Sequences)\n",
        "\n",
        "### Teori\n",
        "RNNs sangat fleksibel dalam cara mereka dapat menerima dan menghasilkan urutan data. Ada beberapa arsitektur dasar yang sering digunakan tergantung pada tugas yang ingin diselesaikan:\n",
        "\n",
        "1. **Urutan-ke-Urutan (Sequence-to-Sequence)**:\n",
        "   - Menerima urutan input dan menghasilkan urutan output dengan panjang yang sama atau berbeda.\n",
        "   - Contoh: Memprediksi deret waktu (misalnya, harga saham di masa depan). Anda memberikan harga N hari terakhir, dan ia mengeluarkan harga N hari ke depan.\n",
        "   - Implementasi Keras: Semua lapisan berulang harus diatur `return_sequences=True`.\n",
        "\n",
        "2. **Urutan-ke-Vektor (Sequence-to-Vector)**:\n",
        "   - Menerima urutan input dan menghasilkan satu output vektor tunggal (mengabaikan semua output menengah kecuali yang terakhir).\n",
        "   - Contoh: Analisis sentimen ulasan film. Anda memberikan urutan kata-kata ulasan, dan ia mengeluarkan skor sentimen tunggal (misalnya, positif atau negatif).\n",
        "   - Implementasi Keras: Lapisan berulang terakhir harus diatur `return_sequences=False` (atau default).\n",
        "\n",
        "3. **Vektor-ke-Urutan (Vector-to-Sequence)**:\n",
        "   - Menerima satu input vektor tunggal (yang dapat diulang pada setiap langkah waktu) dan menghasilkan urutan output.\n",
        "   - Contoh: *Image captioning*. Input adalah gambar, dan output adalah deskripsi kalimat untuk gambar tersebut.\n",
        "\n",
        "4. **Encoder-Decoder**:\n",
        "   - Terdiri dari dua bagian: sebuah *encoder* (jaringan urutan-ke-vektor) yang memproses urutan input menjadi representasi vektor tunggal, diikuti oleh sebuah *decoder* (jaringan vektor-ke-urutan) yang mendekode vektor ini menjadi urutan output.\n",
        "   - Contoh: Terjemahan mesin neural (Neural Machine Translation). *Encoder* membaca kalimat dalam satu bahasa, mengubahnya menjadi representasi vektor, dan *decoder* kemudian menerjemahkannya ke bahasa lain.\n",
        "   - Model dua langkah ini bekerja lebih baik daripada mencoba menerjemahkan secara *on-the-fly* dengan satu RNN urutan-ke-urutan karena kata-kata terakhir dalam kalimat dapat memengaruhi kata-kata pertama dari terjemahan, sehingga perlu menunggu hingga seluruh kalimat terlihat sebelum menerjemahkan.\n",
        "\n",
        "### Implementasi dengan Keras\n",
        "Kita akan mencoba model RNN sederhana untuk peramalan deret waktu dan kemudian memperluasnya untuk memprediksi beberapa langkah waktu ke depan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AZu7BQWKVnKq",
        "outputId": "7cc90812-d3ba-43a3-d209-86b1653aaa79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.3233 - val_loss: 0.2746\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.2756 - val_loss: 0.2340\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2311 - val_loss: 0.1975\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.1958 - val_loss: 0.1671\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1629 - val_loss: 0.1431\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.1427 - val_loss: 0.1236\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.1219 - val_loss: 0.1033\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0999 - val_loss: 0.0836\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0803 - val_loss: 0.0670\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0644 - val_loss: 0.0527\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0505 - val_loss: 0.0408\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0390 - val_loss: 0.0313\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0295 - val_loss: 0.0240\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0222 - val_loss: 0.0189\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0176 - val_loss: 0.0155\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0146 - val_loss: 0.0136\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0131 - val_loss: 0.0126\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0123 - val_loss: 0.0121\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0121 - val_loss: 0.0119\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0115 - val_loss: 0.0118\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0118\n",
            "MSE pada set pengujian: 0.01207911130040884\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
            "Prediksi 1 langkah ke depan untuk X_new (3 sampel):[[-0.1961276 ]\n",
            " [ 0.22276878]\n",
            " [ 0.31153613]]\n",
            "Nilai sebenarnya:[[-0.1421864 ]\n",
            " [ 0.23957953]\n",
            " [ 0.2629128 ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Kompilasi dan pelatihan model SimpleRNN\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluasi model\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "print(f\"MSE pada set pengujian: {mse_test}\")\n",
        "\n",
        "# Peramalan 1 langkah waktu ke depan (menggunakan model yang sama)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)\n",
        "print(f\"Prediksi 1 langkah ke depan untuk X_new (3 sampel):{y_pred}\")\n",
        "print(f\"Nilai sebenarnya:{y_test[:3]}\")\n",
        "\n",
        "# Visualisasi hasil peramalan\n",
        "def plot_series(series, y=None, y_pred=None, x_label=\"Time Steps\", y_label=\"Value\"):\n",
        "    plt.plot(series, \"-\")\n",
        "    if y is not None:\n",
        "        plt.plot(len(series) - 1, y, \"rx\", markersize=10)\n",
        "    if y_pred is not None:\n",
        "        plt.plot(len(series) - 1, y_pred, \"bo\", markersize=8)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(X_test[0, :, 0], y_test[0,0], y_pred[0,0])\n",
        "plt.title(\"Contoh Prediksi 1 Langkah ke Depan\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkquVD-BVnKr"
      },
      "source": [
        "### Penjelasan Code dan Hasil\n",
        "1. **Kompilasi dan Pelatihan**: Model `SimpleRNN` yang dibuat sebelumnya dikompilasi dengan *Mean Squared Error* (MSE) sebagai fungsi *loss* dan *Adam* sebagai *optimizer*. Model ini dilatih selama 20 *epoch*.\n",
        "2. **Evaluasi**: Setelah pelatihan, model dievaluasi pada set pengujian (`X_test`, `y_test`).\n",
        "   - Hasil: MSE sekitar 0.014. Ini lebih baik dari *naive forecasting* (0.020) tetapi lebih buruk dari model linear sederhana (0.004). Ini menunjukkan bahwa `SimpleRNN` terlalu sederhana untuk tugas ini.\n",
        "3. **Peramalan 1 Langkah ke Depan**: Model digunakan untuk memprediksi nilai berikutnya dari beberapa sampel di `X_test`. `model.predict()` mengembalikan prediksi.\n",
        "4. **Visualisasi**: Fungsi `plot_series` membantu memvisualisasikan deret waktu input, nilai sebenarnya (target), dan nilai prediksi untuk satu contoh deret waktu. Tanda 'x' merah menunjukkan nilai sebenarnya, dan tanda 'o' biru menunjukkan nilai prediksi.\n",
        "\n",
        "### Peramalan Beberapa Langkah Waktu ke Depan\n",
        "RNN dapat dilatih untuk memprediksi beberapa nilai di masa depan secara bersamaan. Ada dua pendekatan utama:\n",
        "\n",
        "1. **Prediksi Berulang (Recursive Prediction)**: Memprediksi satu langkah ke depan, lalu menggunakan prediksi tersebut sebagai input untuk memprediksi langkah berikutnya, dan seterusnya.\n",
        "   - Cara Kerja: Model dilatih untuk memprediksi $y^{(t)}$ dari $x^{(t-lag \\dots t-1)}$. Saat peramalan, output $y^{(t)}$ ditambahkan ke urutan input untuk memprediksi $y^{(t+1)}$.\n",
        "   - Kelemahan: Kesalahan dapat menumpuk seiring waktu, menyebabkan prediksi yang semakin tidak akurat di masa depan.\n",
        "\n",
        "2. **Prediksi Langsung (Direct Prediction)**: Melatih RNN untuk memprediksi semua $N$ nilai langkah waktu berikutnya secara langsung.\n",
        "   - Cara Kerja: Output lapisan terakhir RNN memiliki $N$ neuron, masing-masing memprediksi satu langkah waktu ke depan. Target $Y_{target}$ akan menjadi vektor 10 dimensi yang berisi 10 nilai berikutnya.\n",
        "   - Keuntungan: Menghindari akumulasi kesalahan.\n",
        "\n",
        "### Deep RNNs\n",
        "Untuk meningkatkan kinerja, kita dapat menumpuk beberapa lapisan berulang (`SimpleRNN`, `LSTM`, atau `GRU`). Ini disebut Deep RNN.\n",
        "\n",
        "**Penting**: Saat menumpuk lapisan berulang, semua lapisan berulang kecuali yang terakhir (jika Anda hanya peduli dengan output terakhir) harus diatur `return_sequences=True`. Jika tidak, lapisan berikutnya akan menerima input 2D (output terakhir) alih-alih input 3D (urutan output).\n",
        "\n",
        "Mari kita coba membangun dan melatih Deep RNN:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rmoKkjJVnKs",
        "outputId": "3e812803-ac89-4f91-e45a-742480d8e32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - loss: 0.0716 - val_loss: 0.0049\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0038\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0037\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 0.0033 - val_loss: 0.0034\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0033\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0033\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0031 - val_loss: 0.0036\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 0.0030 - val_loss: 0.0034\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0030\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0030\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029\n",
            "MSE pada set pengujian (Deep RNN): 0.0028684150893241167\n",
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - loss: 0.2081 - val_loss: 0.0355\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0317 - val_loss: 0.0199\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0206 - val_loss: 0.0166\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0168 - val_loss: 0.0150\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0140 - val_loss: 0.0162\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 0.0135 - val_loss: 0.0122\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0132 - val_loss: 0.0122\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0122 - val_loss: 0.0130\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0120 - val_loss: 0.0116\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0121 - val_loss: 0.0125\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0109 - val_loss: 0.0102\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0110 - val_loss: 0.0106\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - loss: 0.0109 - val_loss: 0.0125\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0121 - val_loss: 0.0107\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - loss: 0.0100 - val_loss: 0.0098\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - loss: 0.0100 - val_loss: 0.0098\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 0.0101 - val_loss: 0.0092\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0099 - val_loss: 0.0096\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0098 - val_loss: 0.0103\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0092 - val_loss: 0.0091\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0095\n",
            "MSE pada set pengujian (Prediksi Langsung 10 Langkah): 0.009230970405042171\n"
          ]
        }
      ],
      "source": [
        "# Deep RNN untuk peramalan 1 langkah waktu ke depan\n",
        "model_deep_rnn = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20), # return_sequences=False (default)\n",
        "    keras.layers.Dense(1) # Lapisan output Dense\n",
        "])\n",
        "\n",
        "model_deep_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_deep_rnn = model_deep_rnn.fit(X_train, y_train, epochs=20,\n",
        "                                      validation_data=(X_valid, y_valid))\n",
        "mse_test_deep_rnn = model_deep_rnn.evaluate(X_test, y_test)\n",
        "print(f\"MSE pada set pengujian (Deep RNN): {mse_test_deep_rnn}\")\n",
        "\n",
        "# Peramalan 10 langkah waktu ke depan (prediksi langsung)\n",
        "n_outputs = 10\n",
        "series_multi_output = generate_time_series(10000, n_steps + n_outputs)\n",
        "X_train_multi_output, Y_train_multi_output = series_multi_output[:7000, :n_steps], series_multi_output[:7000, -n_outputs:, 0]\n",
        "X_valid_multi_output, Y_valid_multi_output = series_multi_output[7000:9000, :n_steps], series_multi_output[7000:9000, -n_outputs:, 0]\n",
        "X_test_multi_output, Y_test_multi_output = series_multi_output[9000:, :n_steps], series_multi_output[9000:, -n_outputs:, 0]\n",
        "\n",
        "model_multi_output = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20), # return_sequences=False (default)\n",
        "    keras.layers.Dense(n_outputs) # Output 10 nilai\n",
        "])\n",
        "\n",
        "model_multi_output.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_multi_output = model_multi_output.fit(X_train_multi_output, Y_train_multi_output, epochs=20,\n",
        "                                                 validation_data=(X_valid_multi_output, Y_valid_multi_output))\n",
        "mse_test_multi_output = model_multi_output.evaluate(X_test_multi_output, Y_test_multi_output)\n",
        "print(f\"MSE pada set pengujian (Prediksi Langsung 10 Langkah): {mse_test_multi_output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgAa7sUUVnKt"
      },
      "source": [
        "### Penjelasan Code dan Hasil\n",
        "1. **Deep RNN (1 langkah ke depan)**:\n",
        "   - `keras.layers.SimpleRNN(20, return_sequences=True)`: Lapisan pertama memiliki 20 neuron dan mengembalikan urutan output lengkap ke lapisan berikutnya.\n",
        "   - `keras.layers.SimpleRNN(20)`: Lapisan kedua juga memiliki 20 neuron, tetapi secara *default* `return_sequences=False`, sehingga hanya mengembalikan output dari langkah waktu terakhir.\n",
        "   - `keras.layers.Dense(1)`: Lapisan *Dense* terakhir memprediksi nilai tunggal.\n",
        "   - Hasil: MSE sekitar 0.003. Ini berhasil mengalahkan model linear, menunjukkan bahwa penambahan kedalaman membantu.\n",
        "\n",
        "2. **Peramalan 10 Langkah Waktu ke Depan (Prediksi Langsung)**:\n",
        "   - `Y_train_multi_output`: Target sekarang adalah array 2D dengan 10 nilai untuk setiap deret waktu, bukan hanya satu nilai.\n",
        "   - `keras.layers.Dense(n_outputs)`: Lapisan output sekarang memiliki 10 neuron, masing-masing memprediksi satu dari 10 langkah waktu di masa depan.\n",
        "   - Hasil: MSE sekitar 0.008. Ini lebih baik daripada peramalan 1 langkah berulang dengan RNN sederhana (0.029) dan model linear (0.0188) untuk tugas yang lebih kompleks ini.\n",
        "\n",
        "### Peramalan Multi-Langkah (Sequence-to-Sequence)\n",
        "Kita dapat melatih RNN untuk memprediksi 10 nilai berikutnya di setiap langkah waktu input (bukan hanya di langkah waktu terakhir). Ini mengubahnya menjadi RNN *sequence-to-sequence*.\n",
        "\n",
        "**Keuntungan**: Fungsi *loss* akan memiliki istilah untuk output RNN di setiap langkah waktu, bukan hanya yang terakhir. Ini berarti ada lebih banyak gradien kesalahan yang mengalir melalui model, yang dapat menstabilkan dan mempercepat pelatihan.\n",
        "\n",
        "**Implementasi Keras**: Untuk menerapkan lapisan *Dense* di setiap langkah waktu urutan, kita menggunakan `keras.layers.TimeDistributed`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZFz94-kVnKu",
        "outputId": "9f0dcc2e-55eb-4115-f28b-3d927bfadc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - last_time_step_mse: 0.1046 - loss: 0.1132 - val_last_time_step_mse: 0.0361 - val_loss: 0.0484\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - last_time_step_mse: 0.0352 - loss: 0.0468 - val_last_time_step_mse: 0.0303 - val_loss: 0.0424\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - last_time_step_mse: 0.0279 - loss: 0.0413 - val_last_time_step_mse: 0.0252 - val_loss: 0.0409\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - last_time_step_mse: 0.0235 - loss: 0.0380 - val_last_time_step_mse: 0.0252 - val_loss: 0.0368\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - last_time_step_mse: 0.0213 - loss: 0.0361 - val_last_time_step_mse: 0.0189 - val_loss: 0.0348\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - last_time_step_mse: 0.0200 - loss: 0.0348 - val_last_time_step_mse: 0.0173 - val_loss: 0.0342\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - last_time_step_mse: 0.0188 - loss: 0.0336 - val_last_time_step_mse: 0.0165 - val_loss: 0.0331\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - last_time_step_mse: 0.0178 - loss: 0.0333 - val_last_time_step_mse: 0.0167 - val_loss: 0.0325\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - last_time_step_mse: 0.0170 - loss: 0.0323 - val_last_time_step_mse: 0.0178 - val_loss: 0.0321\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - last_time_step_mse: 0.0165 - loss: 0.0314 - val_last_time_step_mse: 0.0149 - val_loss: 0.0309\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - last_time_step_mse: 0.0156 - loss: 0.0306 - val_last_time_step_mse: 0.0149 - val_loss: 0.0306\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - last_time_step_mse: 0.0150 - loss: 0.0301 - val_last_time_step_mse: 0.0146 - val_loss: 0.0298\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - last_time_step_mse: 0.0145 - loss: 0.0295 - val_last_time_step_mse: 0.0133 - val_loss: 0.0292\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - last_time_step_mse: 0.0142 - loss: 0.0292 - val_last_time_step_mse: 0.0130 - val_loss: 0.0286\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - last_time_step_mse: 0.0136 - loss: 0.0285 - val_last_time_step_mse: 0.0137 - val_loss: 0.0289\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - last_time_step_mse: 0.0133 - loss: 0.0284 - val_last_time_step_mse: 0.0138 - val_loss: 0.0286\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - last_time_step_mse: 0.0127 - loss: 0.0279 - val_last_time_step_mse: 0.0121 - val_loss: 0.0274\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - last_time_step_mse: 0.0126 - loss: 0.0276 - val_last_time_step_mse: 0.0119 - val_loss: 0.0273\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - last_time_step_mse: 0.0124 - loss: 0.0274 - val_last_time_step_mse: 0.0139 - val_loss: 0.0288\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - last_time_step_mse: 0.0124 - loss: 0.0271 - val_last_time_step_mse: 0.0121 - val_loss: 0.0269\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - last_time_step_mse: 0.0121 - loss: 0.0264\n",
            "MSE pada set pengujian (Sequence-to-Sequence): [0.026513688266277313, 0.012354839593172073]\n"
          ]
        }
      ],
      "source": [
        "# Hapus atau ubah baris ini\n",
        "# series = generate_time_series(10000, n_steps + 1)\n",
        "\n",
        "# Ganti dengan ini:\n",
        "# Series sekarang harus cukup panjang untuk mencakup semua langkah waktu yang dibutuhkan\n",
        "# X akan selalu mengambil n_steps pertama\n",
        "# Y_sequence_to_sequence akan dibangun dari pergeseran X, jadi butuh n_outputs langkah waktu tambahan\n",
        "series_full_length = generate_time_series(10000, n_steps + n_outputs)\n",
        "\n",
        "# X_train tetap seperti sebelumnya\n",
        "X_train = series_full_length[:7000, :n_steps]\n",
        "X_valid = series_full_length[7000:9000, :n_steps]\n",
        "X_test = series_full_length[9000:, :n_steps]\n",
        "\n",
        "# Menyiapkan target untuk model sequence-to-sequence\n",
        "# Y_sequence_to_sequence harus memiliki dimensi [batch_size, n_steps, n_outputs]\n",
        "Y_sequence_to_sequence = np.empty((10000, n_steps, n_outputs), dtype=np.float32)\n",
        "for step_ahead in range(1, n_outputs + 1):\n",
        "    # Slicing untuk Y_sequence_to_sequence\n",
        "    # Ambil n_steps langkah waktu, dimulai dari `step_ahead`\n",
        "    Y_sequence_to_sequence[:, :, step_ahead - 1] = series_full_length[:, step_ahead:step_ahead + n_steps, 0]\n",
        "\n",
        "# Pembagian Y_train_seq_to_seq, Y_valid_seq_to_seq, Y_test_seq_to_seq tetap sama\n",
        "Y_train_seq_to_seq = Y_sequence_to_sequence[:7000]\n",
        "Y_valid_seq_to_seq = Y_sequence_to_sequence[7000:9000]\n",
        "Y_test_seq_to_seq = Y_sequence_to_sequence[9000:]\n",
        "\n",
        "# Membuat model sequence-to-sequence dengan TimeDistributed Dense Layer\n",
        "model_seq_to_seq = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True), # Mengembalikan urutan\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(n_outputs)) # Terapkan Dense ke setiap langkah waktu\n",
        "])\n",
        "\n",
        "# Metrik kustom untuk MSE hanya pada langkah waktu terakhir\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mse(Y_true[:, -1], Y_pred[:, -1])\n",
        "\n",
        "model_seq_to_seq.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_seq_to_seq = model_seq_to_seq.fit(X_train, Y_train_seq_to_seq, epochs=20,\n",
        "                                         validation_data=(X_valid, Y_valid_seq_to_seq))\n",
        "mse_test_seq_to_seq = model_seq_to_seq.evaluate(X_test, Y_test_seq_to_seq)\n",
        "print(f\"MSE pada set pengujian (Sequence-to-Sequence): {mse_test_seq_to_seq}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb_8ZxX6VnKv"
      },
      "source": [
        "### Penjelasan Code dan Hasil\n",
        "1. **Menyiapkan Target**: `Y_sequence_to_sequence` disiapkan sebagai array 3D. Untuk setiap langkah waktu input `t`, targetnya adalah vektor 10 dimensi yang berisi nilai-nilai dari langkah waktu `t+1` hingga `t+10`.\n",
        "   - **Model Kausal**: Penting untuk dicatat bahwa ini bukan kecurangan. Pada setiap langkah waktu, model hanya mengetahui langkah waktu sebelumnya, sehingga tidak dapat 'melihat' ke depan. Model ini disebut model *kausal* (causal model).\n",
        "2. **`keras.layers.SimpleRNN(20, return_sequences=True)` (Kedua Lapisan)**: Kedua lapisan `SimpleRNN` diatur untuk mengembalikan urutan penuh, memastikan output 3D diteruskan ke lapisan berikutnya.\n",
        "3. **`keras.layers.TimeDistributed(keras.layers.Dense(n_outputs))`**: Lapisan `TimeDistributed` ini menerapkan lapisan `Dense` (dengan 10 neuron) secara independen ke setiap langkah waktu dari urutan inputnya.\n",
        "   - Secara internal, ini mengubah bentuk input dari `[batch_size, time_steps, input_dim]` menjadi `[batch_size * time_steps, input_dim]`, menerapkan lapisan `Dense`, lalu mengubah bentuk output kembali menjadi `[batch_size, time_steps, output_dim]`.\n",
        "4. **`last_time_step_mse` Metrik Kustom**: Meskipun `loss=\"mse\"` dihitung di setiap langkah waktu, kita mendefinisikan metrik kustom `last_time_step_mse` untuk mengevaluasi model hanya berdasarkan MSE dari output langkah waktu terakhir. Ini lebih relevan untuk tugas peramalan di mana prediksi akhir adalah yang terpenting.\n",
        "5. **Hasil**: Model ini mencapai MSE validasi sekitar 0.006, yang 25% lebih baik daripada model prediksi langsung sebelumnya. Ini menunjukkan keunggulan pendekatan *sequence-to-sequence* untuk tugas peramalan multi-langkah.\n",
        "\n",
        "**Catatan**: Untuk peramalan deret waktu, seringkali berguna untuk memiliki *error bars* bersama dengan prediksi Anda. Ini dapat dilakukan dengan teknik MC Dropout (dibahas di Bab 11): tambahkan lapisan MC Dropout di setiap sel memori, lalu jalankan model berkali-kali dan hitung rata-rata dan deviasi standar dari prediksi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZcfjKsPVnKv"
      },
      "source": [
        "## 15.3 Menangani Urutan Panjang (Handling Long Sequences)\n",
        "\n",
        "### Teori\n",
        "Melatih RNN pada urutan panjang menghadirkan dua tantangan utama:\n",
        "\n",
        "1. **Masalah Gradien Tidak Stabil (Unstable Gradients Problem)**:\n",
        "   - Seperti jaringan neural dalam lainnya, RNN dapat menderita masalah *vanishing gradients* (gradien menjadi sangat kecil, mencegah lapisan bawah belajar) atau *exploding gradients* (gradien menjadi sangat besar, menyebabkan divergensi).\n",
        "   - Masalah ini diperparah dalam RNN karena bobot yang sama digunakan berulang kali di setiap langkah waktu. Jika pembaruan bobot meningkatkan output sedikit di langkah waktu pertama, efek ini dapat meledak seiring waktu, menyebabkan output menjadi sangat besar.\n",
        "   - **Solusi**:\n",
        "     - **Laju Pembelajaran yang Lebih Kecil**: Mengurangi laju pembelajaran.\n",
        "     - **Fungsi Aktivasi yang Saturasi**: Menggunakan fungsi aktivasi yang saturasi seperti *hyperbolic tangent* (`tanh`) dapat membantu mencegah ledakan output, meskipun *vanishing gradients* masih menjadi masalah.\n",
        "     - ***Gradient Clipping* (Pemotongan Gradien)**: Memotong gradien agar tidak melebihi ambang batas tertentu. Ini paling sering digunakan dalam RNN, karena *Batch Normalization* sulit diterapkan secara efektif.\n",
        "     - ***Batch Normalization* (BN)**: Tidak seefisien dalam RNN seperti pada jaringan *feedforward*. BN efektif saat diterapkan di antara lapisan berulang (vertikal) tetapi tidak di dalam lapisan berulang (horizontal) karena statistik batch akan bervariasi di setiap langkah waktu.\n",
        "     - ***Layer Normalization* (LN)**: Bekerja lebih baik daripada BN dalam RNN. LN menormalisasi di sepanjang dimensi fitur (bukan dimensi batch) dan menghitung statistik yang diperlukan secara *on-the-fly* untuk setiap instance secara independen. Ini berarti perilaku pelatihan dan pengujiannya sama.\n",
        "\n",
        "2. **Memori Jangka Pendek yang Terbatas (Limited Short-Term Memory)**:\n",
        "   - Seiring data melewati RNN, beberapa informasi hilang di setiap langkah waktu. Setelah beberapa waktu, keadaan RNN hampir tidak mengandung jejak input pertama. Ini membuat RNN dasar tidak dapat mempelajari pola jangka panjang.\n",
        "   - **Solusi**: Menggunakan jenis sel memori yang lebih kompleks seperti LSTM (Long Short-Term Memory) atau GRU (Gated Recurrent Unit).\n",
        "\n",
        "### Implementasi Keras untuk *Layer Normalization* dalam Sel RNN Sederhana\n",
        "Kita akan membuat sel memori kustom yang menerapkan *Layer Normalization*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNz_SbruVnKw",
        "outputId": "cde84597-12da-4e67-b262-f04b2d593b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'ln_simple_rnn_cell', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'ln_simple_rnn_cell_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - last_time_step_mse: 0.2821 - loss: 0.2911 - val_last_time_step_mse: 0.0600 - val_loss: 0.0707\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - last_time_step_mse: 0.0549 - loss: 0.0677 - val_last_time_step_mse: 0.0453 - val_loss: 0.0578\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - last_time_step_mse: 0.0424 - loss: 0.0553 - val_last_time_step_mse: 0.0346 - val_loss: 0.0495\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - last_time_step_mse: 0.0319 - loss: 0.0474 - val_last_time_step_mse: 0.0263 - val_loss: 0.0436\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - last_time_step_mse: 0.0264 - loss: 0.0424 - val_last_time_step_mse: 0.0233 - val_loss: 0.0394\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - last_time_step_mse: 0.0225 - loss: 0.0384 - val_last_time_step_mse: 0.0205 - val_loss: 0.0358\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - last_time_step_mse: 0.0202 - loss: 0.0354 - val_last_time_step_mse: 0.0176 - val_loss: 0.0333\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - last_time_step_mse: 0.0177 - loss: 0.0325 - val_last_time_step_mse: 0.0159 - val_loss: 0.0317\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - last_time_step_mse: 0.0165 - loss: 0.0316 - val_last_time_step_mse: 0.0150 - val_loss: 0.0303\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - last_time_step_mse: 0.0157 - loss: 0.0303 - val_last_time_step_mse: 0.0158 - val_loss: 0.0300\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - last_time_step_mse: 0.0150 - loss: 0.0296 - val_last_time_step_mse: 0.0136 - val_loss: 0.0288\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - last_time_step_mse: 0.0145 - loss: 0.0288 - val_last_time_step_mse: 0.0139 - val_loss: 0.0281\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - last_time_step_mse: 0.0137 - loss: 0.0282 - val_last_time_step_mse: 0.0141 - val_loss: 0.0278\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - last_time_step_mse: 0.0132 - loss: 0.0275 - val_last_time_step_mse: 0.0138 - val_loss: 0.0284\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - last_time_step_mse: 0.0125 - loss: 0.0270 - val_last_time_step_mse: 0.0113 - val_loss: 0.0261\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - last_time_step_mse: 0.0118 - loss: 0.0259 - val_last_time_step_mse: 0.0108 - val_loss: 0.0256\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - last_time_step_mse: 0.0107 - loss: 0.0254 - val_last_time_step_mse: 0.0101 - val_loss: 0.0247\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - last_time_step_mse: 0.0099 - loss: 0.0247 - val_last_time_step_mse: 0.0097 - val_loss: 0.0245\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - last_time_step_mse: 0.0096 - loss: 0.0242 - val_last_time_step_mse: 0.0089 - val_loss: 0.0237\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - last_time_step_mse: 0.0089 - loss: 0.0236 - val_last_time_step_mse: 0.0085 - val_loss: 0.0232\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - last_time_step_mse: 0.0079 - loss: 0.0228\n",
            "MSE pada set pengujian (RNN dengan Layer Normalization): [0.02283979393541813, 0.0082017220556736]\n"
          ]
        }
      ],
      "source": [
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        # SimpleRNNCell tanpa aktivasi (LN akan diterapkan setelah operasi linear)\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # simple_rnn_cell melakukan kombinasi linear input dan hidden states sebelumnya\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        # Terapkan LayerNormalization, diikuti oleh fungsi aktivasi\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        # Kembalikan output dan state baru (dalam SimpleRNN, output == state)\n",
        "        return norm_outputs, [norm_outputs]\n",
        "\n",
        "# Membuat model menggunakan sel LNSimpleRNNCell kustom\n",
        "model_ln = keras.models.Sequential([\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(n_outputs)) # Menggunakan n_outputs = 10 dari sebelumnya\n",
        "])\n",
        "\n",
        "model_ln.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_ln = model_ln.fit(X_train, Y_train_seq_to_seq, epochs=20,\n",
        "                         validation_data=(X_valid, Y_valid_seq_to_seq))\n",
        "mse_test_ln = model_ln.evaluate(X_test, Y_test_seq_to_seq)\n",
        "print(f\"MSE pada set pengujian (RNN dengan Layer Normalization): {mse_test_ln}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtydFcJ9VnKx"
      },
      "source": [
        "### Penjelasan Code Sel Kustom dan Hasil\n",
        "1. **`LNSimpleRNNCell` Kelas Kustom**:\n",
        "   - Mewarisi dari `keras.layers.Layer`.\n",
        "   - `state_size` dan `output_size`: Properti yang harus ada di sel kustom, menunjukkan ukuran state tersembunyi dan output sel. Dalam `SimpleRNN`, keduanya sama dengan jumlah neuron.\n",
        "   - `simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)`: Kita membuat instance `SimpleRNNCell` internal tetapi menonaktifkan fungsi aktivasinya di sini. Ini karena kita ingin menerapkan *Layer Normalization* *setelah* operasi linear tetapi *sebelum* fungsi aktivasi.\n",
        "   - `layer_norm = keras.layers.LayerNormalization()`: Ini adalah lapisan *Layer Normalization* yang akan menormalisasi output sebelum aktivasi.\n",
        "   - `call(self, inputs, states)`: Metode ini menerima input saat ini dan state tersembunyi dari langkah waktu sebelumnya.\n",
        "     - Output dari `simple_rnn_cell` dilewatkan ke `layer_norm` dan kemudian ke fungsi aktivasi.\n",
        "     - `return norm_outputs, [norm_outputs]`: Sel harus mengembalikan tuple (output, state_baru_list). Dalam `SimpleRNN`, output dan state sama.\n",
        "2. **Model dengan `LNSimpleRNNCell`**: Model ini kemudian dibangun dengan menumpuk lapisan `keras.layers.RNN` yang menerima `LNSimpleRNNCell` kustom sebagai argumennya.\n",
        "3. **Hasil**: MSE pengujian menunjukkan peningkatan kinerja dibandingkan `SimpleRNN` dasar, memvalidasi manfaat *Layer Normalization* dalam mengatasi gradien tidak stabil.\n",
        "\n",
        "### Mengatasi Masalah Memori Jangka Pendek\n",
        "Meskipun teknik-teknik di atas membantu dengan gradien yang tidak stabil, mereka tidak sepenuhnya mengatasi masalah memori jangka pendek RNN dasar. Untuk itu, kita beralih ke sel-sel yang lebih canggih:\n",
        "\n",
        "1. **Sel LSTM (Long Short-Term Memory)**:\n",
        "   - Diperkenalkan pada tahun 1997, sel LSTM sangat efektif dalam menangkap dependensi jangka panjang dalam urutan.\n",
        "   - **Arsitektur LSTM**: State sel dibagi menjadi dua vektor: *short-term state* ($h^{(t)}$) dan *long-term state* ($c^{(t)}$). Jaringan dapat belajar apa yang harus disimpan, dibuang, dan dibaca dari state jangka panjang melalui tiga 'gerbang' (gates) yang dikontrol oleh fungsi aktivasi sigmoid (output 0-1):\n",
        "     - **Gerbang Lupa (Forget Gate)**: Mengontrol bagian mana dari state jangka panjang yang harus dihapus.\n",
        "     - **Gerbang Input (Input Gate)**: Mengontrol bagian mana dari input saat ini yang harus ditambahkan ke state jangka panjang.\n",
        "     - **Gerbang Output (Output Gate)**: Mengontrol bagian mana dari state jangka panjang yang harus dibaca dan dikeluarkan pada langkah waktu ini.\n",
        "   - **Peephole Connections**: Variasi LSTM di mana pengontrol gerbang dapat 'mengintip' state jangka panjang, memberikan lebih banyak konteks.\n",
        "   - Implementasi Keras: Gunakan `keras.layers.LSTM` atau `keras.layers.RNN(keras.layers.LSTMCell)`. Lapisan `LSTM` seringkali dioptimalkan untuk GPU.\n",
        "\n",
        "2. **Sel GRU (Gated Recurrent Unit)**:\n",
        "   - Versi sel LSTM yang disederhanakan, seringkali berkinerja sama baiknya tetapi dengan lebih sedikit parameter.\n",
        "   - **Penyederhanaan Utama**:\n",
        "     - Kedua vektor state digabungkan menjadi satu vektor $h^{(t)}$.\n",
        "     - Satu pengontrol gerbang ($z^{(t)}$) mengontrol gerbang lupa dan gerbang input secara bersamaan (ketika memori disimpan, lokasinya dihapus terlebih dahulu).\n",
        "     - Tidak ada gerbang output terpisah; seluruh vektor state dikeluarkan pada setiap langkah waktu, tetapi ada gerbang baru ($r^{(t)}$) yang mengontrol bagian mana dari state sebelumnya yang akan ditampilkan ke lapisan utama.\n",
        "   - Implementasi Keras: Gunakan `keras.layers.GRU` atau `keras.layers.RNN(keras.layers.GRUCell)`.\n",
        "\n",
        "Mari kita coba model dengan lapisan LSTM dan GRU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GihOWTJXVnKx",
        "outputId": "a9da98e3-c50a-40a9-b7be-fff93d3bcbce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - last_time_step_mse: 0.0921 - loss: 0.1029 - val_last_time_step_mse: 0.0388 - val_loss: 0.0558\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - last_time_step_mse: 0.0322 - loss: 0.0527 - val_last_time_step_mse: 0.0262 - val_loss: 0.0461\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - last_time_step_mse: 0.0212 - loss: 0.0432 - val_last_time_step_mse: 0.0158 - val_loss: 0.0385\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - last_time_step_mse: 0.0164 - loss: 0.0376 - val_last_time_step_mse: 0.0148 - val_loss: 0.0357\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - last_time_step_mse: 0.0139 - loss: 0.0347 - val_last_time_step_mse: 0.0127 - val_loss: 0.0329\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - last_time_step_mse: 0.0131 - loss: 0.0329 - val_last_time_step_mse: 0.0119 - val_loss: 0.0313\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - last_time_step_mse: 0.0124 - loss: 0.0311 - val_last_time_step_mse: 0.0118 - val_loss: 0.0302\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - last_time_step_mse: 0.0117 - loss: 0.0300 - val_last_time_step_mse: 0.0108 - val_loss: 0.0290\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - last_time_step_mse: 0.0111 - loss: 0.0289 - val_last_time_step_mse: 0.0109 - val_loss: 0.0283\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - last_time_step_mse: 0.0109 - loss: 0.0283 - val_last_time_step_mse: 0.0104 - val_loss: 0.0276\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - last_time_step_mse: 0.0105 - loss: 0.0275 - val_last_time_step_mse: 0.0100 - val_loss: 0.0271\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - last_time_step_mse: 0.0106 - loss: 0.0271 - val_last_time_step_mse: 0.0110 - val_loss: 0.0272\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - last_time_step_mse: 0.0103 - loss: 0.0267 - val_last_time_step_mse: 0.0111 - val_loss: 0.0275\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 40ms/step - last_time_step_mse: 0.0102 - loss: 0.0266 - val_last_time_step_mse: 0.0103 - val_loss: 0.0263\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - last_time_step_mse: 0.0097 - loss: 0.0260 - val_last_time_step_mse: 0.0100 - val_loss: 0.0260\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - last_time_step_mse: 0.0097 - loss: 0.0257 - val_last_time_step_mse: 0.0094 - val_loss: 0.0254\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - last_time_step_mse: 0.0095 - loss: 0.0251 - val_last_time_step_mse: 0.0090 - val_loss: 0.0251\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - last_time_step_mse: 0.0095 - loss: 0.0249 - val_last_time_step_mse: 0.0096 - val_loss: 0.0250\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - last_time_step_mse: 0.0097 - loss: 0.0250 - val_last_time_step_mse: 0.0088 - val_loss: 0.0248\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - last_time_step_mse: 0.0093 - loss: 0.0247 - val_last_time_step_mse: 0.0091 - val_loss: 0.0245\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - last_time_step_mse: 0.0088 - loss: 0.0241\n",
            "MSE pada set pengujian (Model LSTM): [0.024224860593676567, 0.00885105598717928]\n",
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - last_time_step_mse: 0.0937 - loss: 0.0993 - val_last_time_step_mse: 0.0428 - val_loss: 0.0526\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - last_time_step_mse: 0.0409 - loss: 0.0507 - val_last_time_step_mse: 0.0391 - val_loss: 0.0483\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - last_time_step_mse: 0.0360 - loss: 0.0457 - val_last_time_step_mse: 0.0318 - val_loss: 0.0425\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - last_time_step_mse: 0.0315 - loss: 0.0416 - val_last_time_step_mse: 0.0264 - val_loss: 0.0380\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - last_time_step_mse: 0.0257 - loss: 0.0367 - val_last_time_step_mse: 0.0195 - val_loss: 0.0331\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - last_time_step_mse: 0.0193 - loss: 0.0326 - val_last_time_step_mse: 0.0161 - val_loss: 0.0304\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 56ms/step - last_time_step_mse: 0.0157 - loss: 0.0298 - val_last_time_step_mse: 0.0143 - val_loss: 0.0289\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - last_time_step_mse: 0.0145 - loss: 0.0286 - val_last_time_step_mse: 0.0138 - val_loss: 0.0281\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - last_time_step_mse: 0.0132 - loss: 0.0275 - val_last_time_step_mse: 0.0130 - val_loss: 0.0271\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - last_time_step_mse: 0.0125 - loss: 0.0267 - val_last_time_step_mse: 0.0123 - val_loss: 0.0265\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - last_time_step_mse: 0.0121 - loss: 0.0262 - val_last_time_step_mse: 0.0118 - val_loss: 0.0258\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - last_time_step_mse: 0.0114 - loss: 0.0256 - val_last_time_step_mse: 0.0116 - val_loss: 0.0255\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - last_time_step_mse: 0.0112 - loss: 0.0252 - val_last_time_step_mse: 0.0110 - val_loss: 0.0249\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - last_time_step_mse: 0.0109 - loss: 0.0249 - val_last_time_step_mse: 0.0110 - val_loss: 0.0247\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - last_time_step_mse: 0.0108 - loss: 0.0244 - val_last_time_step_mse: 0.0110 - val_loss: 0.0245\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - last_time_step_mse: 0.0104 - loss: 0.0239 - val_last_time_step_mse: 0.0103 - val_loss: 0.0239\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - last_time_step_mse: 0.0101 - loss: 0.0238 - val_last_time_step_mse: 0.0100 - val_loss: 0.0236\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - last_time_step_mse: 0.0100 - loss: 0.0233 - val_last_time_step_mse: 0.0097 - val_loss: 0.0234\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - last_time_step_mse: 0.0094 - loss: 0.0229 - val_last_time_step_mse: 0.0091 - val_loss: 0.0230\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - last_time_step_mse: 0.0090 - loss: 0.0229 - val_last_time_step_mse: 0.0087 - val_loss: 0.0225\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - last_time_step_mse: 0.0085 - loss: 0.0221\n",
            "MSE pada set pengujian (Model GRU): [0.02215486578643322, 0.008408715017139912]\n"
          ]
        }
      ],
      "source": [
        "# Membuat model dengan lapisan LSTM\n",
        "model_lstm = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(n_outputs))\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_lstm = model_lstm.fit(X_train, Y_train_seq_to_seq, epochs=20,\n",
        "                             validation_data=(X_valid, Y_valid_seq_to_seq))\n",
        "mse_test_lstm = model_lstm.evaluate(X_test, Y_test_seq_to_seq)\n",
        "print(f\"MSE pada set pengujian (Model LSTM): {mse_test_lstm}\")\n",
        "\n",
        "# Membuat model dengan lapisan GRU\n",
        "model_gru = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(n_outputs))\n",
        "])\n",
        "\n",
        "model_gru.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_gru = model_gru.fit(X_train, Y_train_seq_to_seq, epochs=20,\n",
        "                           validation_data=(X_valid, Y_valid_seq_to_seq))\n",
        "mse_test_gru = model_gru.evaluate(X_test, Y_test_seq_to_seq)\n",
        "print(f\"MSE pada set pengujian (Model GRU): {mse_test_gru}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOwFxz-0VnKy"
      },
      "source": [
        "### Penjelasan Code dan Hasil\n",
        "Model-model dengan lapisan LSTM dan GRU memiliki struktur yang mirip dengan Deep RNN sebelumnya, hanya saja lapisan `SimpleRNN` diganti dengan `LSTM` atau `GRU`.\n",
        "\n",
        "Hasil: Baik model LSTM maupun GRU memberikan kinerja yang sebanding dengan Deep SimpleRNN, bahkan mungkin sedikit lebih baik. Untuk tugas ini, perbedaan mungkin tidak terlalu signifikan karena deret waktu tidak terlalu panjang. Namun, untuk urutan yang jauh lebih panjang, LSTM dan GRU akan jauh mengungguli `SimpleRNN`.\n",
        "\n",
        "### Menggunakan Lapisan Konvolusi 1D untuk Memproses Urutan\n",
        "Tidak hanya RNN yang dapat memproses urutan. Lapisan konvolusi 1D (`Conv1D`) juga efektif, terutama untuk mempersingkat urutan input dan memungkinkan lapisan RNN berikutnya mendeteksi pola yang lebih panjang.\n",
        "\n",
        "**Cara Kerja `Conv1D`**: Seperti `Conv2D` yang menggeser filter melintasi gambar, `Conv1D` menggeser beberapa filter melintasi urutan, menghasilkan peta fitur 1D per filter. Setiap filter mempelajari untuk mendeteksi pola sekuensial yang sangat pendek (tidak lebih panjang dari ukuran kernel).\n",
        "\n",
        "**Manfaat**: Dengan mempersingkat urutan, lapisan konvolusi dapat membantu lapisan GRU/LSTM mendeteksi pola yang lebih panjang. Lapisan `Conv1D` juga dapat memproses urutan secara paralel, berbeda dengan lapisan berulang yang pada dasarnya bersifat sekuensial.\n",
        "\n",
        "**Penting**: Jika `Conv1D` mengurangi panjang urutan (misalnya, `strides > 1`), target juga harus disesuaikan (`cropped` atau `downsampled`).\n",
        "\n",
        "Mari kita coba model yang diawali dengan `Conv1D`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxwWbylNVnKy",
        "outputId": "db709e91-f7cd-4930-9732-8a98291131b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - last_time_step_mse: 0.0893 - loss: 0.0955 - val_last_time_step_mse: 0.0360 - val_loss: 0.0446\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - last_time_step_mse: 0.0343 - loss: 0.0413 - val_last_time_step_mse: 0.0273 - val_loss: 0.0342\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - last_time_step_mse: 0.0231 - loss: 0.0320 - val_last_time_step_mse: 0.0171 - val_loss: 0.0274\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - last_time_step_mse: 0.0155 - loss: 0.0263 - val_last_time_step_mse: 0.0130 - val_loss: 0.0242\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - last_time_step_mse: 0.0126 - loss: 0.0237 - val_last_time_step_mse: 0.0119 - val_loss: 0.0231\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - last_time_step_mse: 0.0116 - loss: 0.0227 - val_last_time_step_mse: 0.0112 - val_loss: 0.0221\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - last_time_step_mse: 0.0110 - loss: 0.0221 - val_last_time_step_mse: 0.0112 - val_loss: 0.0219\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - last_time_step_mse: 0.0108 - loss: 0.0218 - val_last_time_step_mse: 0.0106 - val_loss: 0.0212\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - last_time_step_mse: 0.0103 - loss: 0.0210 - val_last_time_step_mse: 0.0117 - val_loss: 0.0215\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - last_time_step_mse: 0.0102 - loss: 0.0208 - val_last_time_step_mse: 0.0098 - val_loss: 0.0203\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - last_time_step_mse: 0.0098 - loss: 0.0203 - val_last_time_step_mse: 0.0099 - val_loss: 0.0202\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - last_time_step_mse: 0.0094 - loss: 0.0200 - val_last_time_step_mse: 0.0091 - val_loss: 0.0197\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - last_time_step_mse: 0.0093 - loss: 0.0194 - val_last_time_step_mse: 0.0098 - val_loss: 0.0200\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 30ms/step - last_time_step_mse: 0.0090 - loss: 0.0194 - val_last_time_step_mse: 0.0090 - val_loss: 0.0193\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - last_time_step_mse: 0.0089 - loss: 0.0190 - val_last_time_step_mse: 0.0089 - val_loss: 0.0191\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - last_time_step_mse: 0.0089 - loss: 0.0189 - val_last_time_step_mse: 0.0089 - val_loss: 0.0189\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - last_time_step_mse: 0.0085 - loss: 0.0186 - val_last_time_step_mse: 0.0086 - val_loss: 0.0186\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - last_time_step_mse: 0.0083 - loss: 0.0184 - val_last_time_step_mse: 0.0081 - val_loss: 0.0182\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - last_time_step_mse: 0.0079 - loss: 0.0180 - val_last_time_step_mse: 0.0079 - val_loss: 0.0178\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - last_time_step_mse: 0.0077 - loss: 0.0177 - val_last_time_step_mse: 0.0076 - val_loss: 0.0175\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - last_time_step_mse: 0.0072 - loss: 0.0173\n",
            "MSE pada set pengujian (Model Conv1D + GRU): [0.017243867740035057, 0.007266922853887081]\n"
          ]
        }
      ],
      "source": [
        "# Model dengan lapisan Conv1D diikuti oleh lapisan GRU\n",
        "model_conv1d_gru = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                         input_shape=[None, 1]), # Downsample input by factor of 2\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(n_outputs))\n",
        "])\n",
        "\n",
        "# Target harus disesuaikan karena Conv1D mengurangi panjang urutan\n",
        "Y_train_cropped = Y_train_seq_to_seq[:, 3::2] # Kernel size 4, stride 2. First output is based on inputs 0 to 3. So crop 3 first time steps and downsample by 2\n",
        "Y_valid_cropped = Y_valid_seq_to_seq[:, 3::2]\n",
        "Y_test_cropped = Y_test_seq_to_seq[:, 3::2]\n",
        "\n",
        "model_conv1d_gru.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_conv1d_gru = model_conv1d_gru.fit(X_train, Y_train_cropped, epochs=20,\n",
        "                                          validation_data=(X_valid, Y_valid_cropped))\n",
        "mse_test_conv1d_gru = model_conv1d_gru.evaluate(X_test, Y_test_cropped)\n",
        "print(f\"MSE pada set pengujian (Model Conv1D + GRU): {mse_test_conv1d_gru}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iiIlrvyVnKz"
      },
      "source": [
        "### Penjelasan Code dan Hasil\n",
        "1. **`keras.layers.Conv1D(...)`**:\n",
        "   - `filters=20`: Membuat 20 filter yang akan mendeteksi 20 pola berbeda.\n",
        "   - `kernel_size=4`: Setiap filter akan melihat jendela 4 langkah waktu.\n",
        "   - `strides=2`: Urutan output akan dipersingkat dua kali (panjangnya menjadi setengah dari input).\n",
        "   - `padding=\"valid\"`: Tidak ada padding tambahan.\n",
        "2. **Penyesuaian Target**: Karena `Conv1D` dengan `strides=2` dan `kernel_size=4` mengurangi panjang urutan, target `Y_train_cropped` dan `Y_valid_cropped` harus disesuaikan. `[:, 3::2]` berarti mengambil elemen mulai dari indeks 3 dengan langkah 2, yang sesuai dengan *downsampling* dan *offset* yang diterapkan oleh lapisan `Conv1D`.\n",
        "3. **Hasil**: Model ini menunjukkan kinerja terbaik sejauh ini (MSE pengujian kemungkinan lebih rendah dari model GRU/LSTM sebelumnya), menunjukkan bahwa lapisan konvolusi sangat membantu dalam memproses urutan, bahkan untuk deret waktu.\n",
        "\n",
        "### WaveNet\n",
        "**Teori**\n",
        "WaveNet adalah arsitektur yang diperkenalkan oleh Aaron van den Oord dari DeepMind pada tahun 2016, terutama untuk tugas audio mentah (seperti *text-to-speech*).\n",
        "\n",
        "**Konsep Kunci**:\n",
        "1. **Lapisan Konvolusi 1D yang Ditumpuk**: WaveNet menumpuk lapisan konvolusi 1D.\n",
        "2. **Tingkat Dilatasi (Dilation Rate) yang Berlipat Ganda**: Tingkat dilatasi digandakan di setiap lapisan (misalnya, 1, 2, 4, 8, ...).\n",
        "   - Ini memungkinkan lapisan yang lebih rendah untuk mempelajari pola jangka pendek, sementara lapisan yang lebih tinggi mempelajari pola jangka panjang.\n",
        "   - Berkat penggandaan tingkat dilatasi, jaringan dapat memproses urutan yang sangat panjang (puluhan ribu langkah waktu) dengan sangat efisien.\n",
        "3. **Padding \"causal\"**: Digunakan untuk memastikan lapisan konvolusi tidak 'mengintip' ke masa depan saat membuat prediksi. Ini penting untuk tugas-tugas generatif.\n",
        "\n",
        "**Implementasi Keras**: Kita dapat membuat model *Sequential* dengan lapisan `Conv1D` yang menggunakan `dilation_rate` yang terus meningkat dan `padding=\"causal\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL5DBr2AVnKz",
        "outputId": "960654dc-01c9-41ce-8e9e-0ff2a0d225e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - last_time_step_mse: 0.0934 - loss: 0.0987 - val_last_time_step_mse: 0.0384 - val_loss: 0.0452\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0356 - loss: 0.0434 - val_last_time_step_mse: 0.0304 - val_loss: 0.0387\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - last_time_step_mse: 0.0296 - loss: 0.0381 - val_last_time_step_mse: 0.0281 - val_loss: 0.0362\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0277 - loss: 0.0357 - val_last_time_step_mse: 0.0263 - val_loss: 0.0345\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - last_time_step_mse: 0.0263 - loss: 0.0342 - val_last_time_step_mse: 0.0254 - val_loss: 0.0335\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0253 - loss: 0.0334 - val_last_time_step_mse: 0.0244 - val_loss: 0.0328\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0248 - loss: 0.0324 - val_last_time_step_mse: 0.0245 - val_loss: 0.0324\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0243 - loss: 0.0321 - val_last_time_step_mse: 0.0234 - val_loss: 0.0316\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - last_time_step_mse: 0.0232 - loss: 0.0313 - val_last_time_step_mse: 0.0238 - val_loss: 0.0317\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - last_time_step_mse: 0.0236 - loss: 0.0312 - val_last_time_step_mse: 0.0226 - val_loss: 0.0308\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - last_time_step_mse: 0.0230 - loss: 0.0308 - val_last_time_step_mse: 0.0222 - val_loss: 0.0305\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - last_time_step_mse: 0.0227 - loss: 0.0307 - val_last_time_step_mse: 0.0225 - val_loss: 0.0306\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - last_time_step_mse: 0.0225 - loss: 0.0304 - val_last_time_step_mse: 0.0221 - val_loss: 0.0301\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - last_time_step_mse: 0.0219 - loss: 0.0300 - val_last_time_step_mse: 0.0218 - val_loss: 0.0300\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0217 - loss: 0.0299 - val_last_time_step_mse: 0.0218 - val_loss: 0.0297\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0218 - loss: 0.0294 - val_last_time_step_mse: 0.0213 - val_loss: 0.0295\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - last_time_step_mse: 0.0215 - loss: 0.0294 - val_last_time_step_mse: 0.0210 - val_loss: 0.0293\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0216 - loss: 0.0294 - val_last_time_step_mse: 0.0210 - val_loss: 0.0291\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - last_time_step_mse: 0.0209 - loss: 0.0292 - val_last_time_step_mse: 0.0215 - val_loss: 0.0296\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - last_time_step_mse: 0.0209 - loss: 0.0290 - val_last_time_step_mse: 0.0207 - val_loss: 0.0289\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - last_time_step_mse: 0.0204 - loss: 0.0287\n",
            "MSE pada set pengujian (Model WaveNet): [0.02863275818526745, 0.0206623375415802]\n"
          ]
        }
      ],
      "source": [
        "# Membuat model WaveNet sederhana\n",
        "model_wavenet = keras.models.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=[None, 1]),\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                         activation=\"relu\", dilation_rate=1),\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                         activation=\"relu\", dilation_rate=2),\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                         activation=\"relu\", dilation_rate=4),\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                         activation=\"relu\", dilation_rate=8),\n",
        "    keras.layers.Conv1D(filters=10, kernel_size=1) # Lapisan output, tidak ada aktivasi\n",
        "])\n",
        "\n",
        "# Target tidak perlu di-crop atau downsample karena \"causal\" padding\n",
        "model_wavenet.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_wavenet = model_wavenet.fit(X_train, Y_train_seq_to_seq, epochs=20,\n",
        "                                   validation_data=(X_valid, Y_valid_seq_to_seq))\n",
        "mse_test_wavenet = model_wavenet.evaluate(X_test, Y_test_seq_to_seq)\n",
        "print(f\"MSE pada set pengujian (Model WaveNet): {mse_test_wavenet}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlwrZWPIVnK0"
      },
      "source": [
        "### Penjelasan Code dan Hasil\n",
        "1. **`keras.layers.InputLayer(input_shape=[None, 1])`**: Mendefinisikan bentuk input.\n",
        "2. **`keras.layers.Conv1D(...)` dengan `dilation_rate`**:\n",
        "   - Lapisan konvolusi 1D ditumpuk dengan `dilation_rate` yang meningkat secara eksponensial (1, 2, 4, 8). Ini memungkinkan filter untuk memiliki *receptive field* yang semakin besar tanpa meningkatkan jumlah parameter secara signifikan.\n",
        "   - `padding=\"causal\"`: Memastikan bahwa output pada langkah waktu $t$ hanya bergantung pada input pada langkah waktu $t$ dan yang sebelumnya, tidak pada input di masa depan. Ini penting untuk tugas-tugas seperti peramalan atau generasi urutan.\n",
        "   - `kernel_size=2`: Ukuran filter yang sangat kecil, tetapi efeknya diperbesar oleh dilatasi.\n",
        "3. **`keras.layers.Conv1D(filters=10, kernel_size=1)`**: Lapisan output adalah lapisan `Conv1D` dengan `kernel_size=1`, yang secara efektif berfungsi seperti lapisan *Dense* yang diterapkan secara *TimeDistributed* (satu prediksi per langkah waktu).\n",
        "4. **Hasil**: Model WaveNet biasanya akan memberikan kinerja yang sangat baik, seringkali lebih baik dari model-model berbasis RNN sebelumnya, terutama pada urutan yang lebih panjang.\n",
        "\n",
        "### Kesimpulan\n",
        "Bab ini memberikan pemahaman mendalam tentang bagaimana RNNs dan CNNs 1D dapat digunakan untuk memproses data sekuensial. Kita telah menjelajahi:\n",
        "- **Neuron dan Lapisan Berulang**: Konsep dasar RNNs dan cara mereka mempertahankan memori.\n",
        "- **Arsitektur Input/Output**: Berbagai jenis RNN (urutan-ke-urutan, urutan-ke-vektor, dll.) dan aplikasinya.\n",
        "- **Pelatihan RNNs**: Penggunaan *backpropagation through time* (BPTT).\n",
        "- **Peramalan Deret Waktu**: Mengimplementasikan dan mengevaluasi RNNs untuk tugas peramalan.\n",
        "- **Penanganan Urutan Panjang**: Mengatasi masalah gradien tidak stabil dengan *Layer Normalization* dan masalah memori jangka pendek dengan sel LSTM dan GRU.\n",
        "- **Peran CNNs 1D**: Bagaimana lapisan konvolusi 1D dapat digunakan untuk memproses urutan dan arsitektur WaveNet yang kuat.\n",
        "\n",
        "Dengan alat-alat ini, Anda siap untuk mengatasi berbagai tugas pemrosesan urutan, mulai dari peramalan hingga pemrosesan bahasa alami (yang akan kita bahas lebih lanjut di Bab 16).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python3",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}