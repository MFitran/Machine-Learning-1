{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0hYTUZABrlF"
      },
      "source": [
        "# Bab 18: Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLSnaT12BrlK"
      },
      "source": [
        "Bab ini bertujuan untuk memperdalam pemahaman tentang Reinforcement Learning (RL), sebuah paradigma pembelajaran mesin di mana agen belajar membuat keputusan berdasarkan interaksi dengan lingkungannya untuk memaksimalkan *reward*. Kita akan mereproduksi konsep inti melalui kode Python dan penjelasan teoritis terstruktur, menggunakan buku \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" sebagai referensi utama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_em6d6YqBrlL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71PtmnecBrlL"
      },
      "source": [
        "### **1. Pendahuluan**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7FezfckBrlM"
      },
      "source": [
        "Reinforcement Learning (RL) adalah salah satu bidang Machine Learning yang paling menarik dan juga salah satu yang tertua. Ini adalah paradigma pembelajaran di mana agen perangkat lunak membuat observasi dan mengambil tindakan dalam suatu lingkungan, dan sebagai balasannya menerima *reward* (hadiah). Tujuan agen adalah belajar bertindak sedemikian rupa untuk memaksimalkan *reward* yang diharapkan dari waktu ke waktu. Ini berbeda dari *supervised learning* karena agen tidak secara eksplisit diberikan jawaban yang 'benar', dan berbeda dari *unsupervised learning* karena ada bentuk pengawasan melalui *reward*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0boA5Q2BrlM"
      },
      "source": [
        "Beberapa contoh aplikasi RL yang relevan termasuk:\n",
        "* **Robotika**: Agen mengontrol robot untuk navigasi atau tugas tertentu.\n",
        "* **Bermain Game**: Agen belajar bermain game Atari dari awal, bahkan mengungguli manusia.\n",
        "* **Sistem Rekomendasi**: Agen merekomendasikan produk atau konten kepada pengguna.\n",
        "* **Smart Thermostat**: Agen mengontrol suhu untuk menghemat energi dan mengantisipasi kebutuhan manusia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW4lyYbyBrlN"
      },
      "source": [
        "Bab ini akan membahas teknik-teknik penting dalam Deep Reinforcement Learning, seperti *policy gradients* dan *deep Q-networks* (DQNs). Kita akan menggunakan **OpenAI Gym** untuk lingkungan simulasi dan **TF-Agents** untuk implementasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnb-cD9JBrlN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuh_1Lb8BrlO"
      },
      "source": [
        "### **2. Belajar Mengoptimalkan *Rewards***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3PTaliGBrlO"
      },
      "source": [
        "Dalam Reinforcement Learning, interaksi agen dengan lingkungan dapat digambarkan sebagai siklus. Agen membuat **observasi** dari lingkungan, mengambil **aksi** berdasarkan *policy*-nya, dan sebagai balasannya menerima **reward** dari lingkungan. Tujuan utama agen adalah untuk belajar **policy** yang akan memaksimalkan total *reward* yang diharapkan dari waktu ke waktu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9W415dHBrlO"
      },
      "source": [
        "**Elemen-elemen Kunci RL:**\n",
        "* **Agen (Agent)**: Entitas perangkat lunak yang belajar dan mengambil tindakan.\n",
        "* **Lingkungan (Environment)**: Dunia tempat agen berinteraksi; ini bisa berupa simulasi atau dunia nyata.\n",
        "* **Aksi (Actions)**: Tindakan yang dapat diambil agen dalam lingkungan.\n",
        "* **Observasi (Observations)**: Informasi yang diterima agen dari lingkungan setelah mengambil aksi.\n",
        "* **Reward (Hadiah)**: Umpan balik numerik (positif atau negatif) yang diterima agen dari lingkungan setelah mengambil aksi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6u4xpwaBrlO"
      },
      "source": [
        "**Siklus Interaksi Agen-Lingkungan:**\n",
        "1.  Agen mengamati lingkungan (`Observation`).\n",
        "2.  Berdasarkan observasi dan *policy* saat ini, agen memilih sebuah `Action`.\n",
        "3.  Agen mengirimkan `Action` ke lingkungan.\n",
        "4.  Lingkungan merespons dengan `New Observation` dan sebuah `Reward`.\n",
        "5.  Lingkungan juga memberikan informasi apakah episode telah selesai (`done`) atau tidak.\n",
        "6.  Siklus berlanjut hingga episode berakhir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6z_wN3xBrlP"
      },
      "source": [
        "**Contoh Aplikasi RL (seperti yang disajikan di buku):**\n",
        "* **Robotika**: Agen mengontrol robot untuk mencapai tujuan.\n",
        "* **Ms. Pac-Man**: Agen bermain game Atari untuk mendapatkan skor tinggi.\n",
        "* **Permainan Go**: Agen mengontrol pemain Go untuk mengalahkan juara dunia.\n",
        "* **Termostat Cerdas**: Agen mengontrol suhu untuk kenyamanan dan efisiensi energi.\n",
        "* **Trader Otomatis**: Agen membuat keputusan jual/beli di pasar saham untuk memaksimalkan keuntungan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA4eZQwEBrlP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3W7Evp1BrlP"
      },
      "source": [
        "### **3. *Policy Search***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9RVFidrBrlP"
      },
      "source": [
        "**Policy** adalah algoritma yang digunakan agen untuk menentukan tindakannya. Policy bisa berupa algoritma apa pun, dan tidak harus deterministik; dalam beberapa kasus, ia bahkan tidak perlu mengamati lingkungan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CntA6XRoBrlP"
      },
      "source": [
        "**Jenis Policy:**\n",
        "* **Deterministic Policy**: Untuk setiap observasi, *policy* selalu menghasilkan aksi yang sama.\n",
        "* **Stochastic Policy**: Untuk setiap observasi, *policy* menghasilkan distribusi probabilitas di atas aksi, dan aksi kemudian dipilih secara acak dari distribusi ini. *Stochastic policy* seringkali lebih baik karena memungkinkan agen untuk menjelajahi lingkungan (eksplorasi) daripada hanya mengeksploitasi pengetahuan yang ada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRMvPVAFBrlP"
      },
      "source": [
        "**Metode Pencarian Policy:**\n",
        "1.  **Brute Force**: Mencoba berbagai nilai parameter untuk *policy* dan memilih kombinasi yang paling berkinerja baik. Ini tidak skalabel untuk ruang *policy* yang besar.\n",
        "2.  **Algoritma Genetika**: Membuat populasi *policy* secara acak, mengevaluasi kinerja mereka, dan membiarkan *policy* terbaik 'bereproduksi' dengan variasi acak untuk menghasilkan generasi berikutnya.\n",
        "3.  **Policy Gradients (PG)**: Menggunakan teknik optimasi berbasis *gradient* untuk mengoptimalkan parameter *policy*. Ini melibatkan evaluasi *gradient* *reward* terhadap parameter *policy*, lalu menyesuaikan parameter untuk mendapatkan *reward* yang lebih tinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zDpJ5P_BrlQ"
      },
      "source": [
        "### Kode Praktik: Demonstrasi Sederhana Policy Search (Non-Gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQLGminTBrlQ",
        "outputId": "7ae7c299-d863-4a39-eca9-1a2a61aedcca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mulai pencarian policy sederhana...\n",
            "Reward terbaik: 100.00\n",
            "Parameter policy terbaik: {'action_prob': np.float64(1.0), 'angle_range': np.float64(0.1)}\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall numpy\n",
        "# !pip install numpy==2.0.0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Simulasi lingkungan sederhana: menghasilkan reward berdasarkan dua parameter policy\n",
        "def simple_environment(action_prob, angle_range, num_steps=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_steps):\n",
        "        if np.random.rand() < action_prob: # Policy parameter 1: prob of taking action\n",
        "            # Simulasi aksi yang \"baik\"\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            # Simulasi aksi yang \"kurang baik\" (tergantung angle_range)\n",
        "            reward = -0.5 * (np.random.rand() * angle_range) # Policy parameter 2: angle_range\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "# Mencoba berbagai kombinasi parameter (Brute Force Policy Search)\n",
        "best_reward = -np.inf\n",
        "best_params = None\n",
        "\n",
        "action_probs = np.linspace(0.1, 1.0, 10)\n",
        "angle_ranges = np.linspace(0.1, 5.0, 10)\n",
        "\n",
        "print(\"Mulai pencarian policy sederhana...\")\n",
        "for prob in action_probs:\n",
        "    for angle in angle_ranges:\n",
        "        current_reward = simple_environment(prob, angle)\n",
        "        if current_reward > best_reward:\n",
        "            best_reward = current_reward\n",
        "            best_params = {'action_prob': prob, 'angle_range': angle}\n",
        "\n",
        "print(f\"Reward terbaik: {best_reward:.2f}\")\n",
        "print(f\"Parameter policy terbaik: {best_params}\")\n",
        "\n",
        "# Catatan: Ini adalah contoh yang sangat disederhanakan dan tidak merepresentasikan RL sepenuhnya.\n",
        "# Tujuan utamanya adalah untuk menunjukkan ide di balik \"policy search\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W0HQrb_BrlR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3uLm0sJBrlR"
      },
      "source": [
        "### **4. Pengenalan OpenAI Gym**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPvyfmwEBrlR"
      },
      "source": [
        "**OpenAI Gym** adalah *toolkit* yang menyediakan berbagai lingkungan simulasi (misalnya, game Atari, game papan, simulasi fisik 2D dan 3D) untuk mengembangkan dan membandingkan agen RL. Menggunakan lingkungan simulasi memiliki banyak keuntungan:\n",
        "* **Reproduktifitas**: Eksperimen dapat diulang dengan kondisi yang sama.\n",
        "* **Kecepatan**: Simulasi dapat berjalan lebih cepat dari waktu nyata atau dapat dipercepat.\n",
        "* **Keamanan**: Agen dapat melakukan kesalahan fatal (misalnya, robot jatuh dari tebing) tanpa konsekuensi nyata.\n",
        "* **Paralelisasi**: Banyak agen dapat dilatih secara paralel dalam lingkungan simulasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCDRs1LCBrlR"
      },
      "source": [
        "### Kode Praktik: OpenAI Gym - Lingkungan CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "jPaMtBAaBrlS",
        "outputId": "61a8b5e0-2c19-40d0-c80f-2999033913f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Membuat lingkungan CartPole-v1...\n",
            "Observasi Awal: [-0.01937494 -0.03814903  0.00175956 -0.00074087]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruang Aksi: Discrete(2)\n",
            "Melakukan satu langkah di lingkungan...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'bool8'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-781791740.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Melakukan satu langkah di lingkungan...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Contoh aksi: mempercepat ke kanan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Observasi Baru: {obs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {reward}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Reward 1.0 per langkah yang berhasil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             logger.warn(\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34mf\"Expects `terminated` signal to be a boolean, actual type: {type(terminated)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchararray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
          ]
        }
      ],
      "source": [
        "# Instalasi OpenAI Gym (jika belum)\n",
        "# !pip install -U gym\n",
        "# !pip install -U 'gym[classic_control]' # Untuk CartPole\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Membuat Lingkungan CartPole\n",
        "print(\"Membuat lingkungan CartPole-v1...\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "print(f\"Observasi Awal: {obs}\")\n",
        "\n",
        "# Observasi untuk CartPole-v1 adalah NumPy array 1D dengan 4 float:\n",
        "# [posisi_kereta, kecepatan_kereta, sudut_tiang, kecepatan_sudut_tiang]\n",
        "\n",
        "# 2. Visualisasi Lingkungan (Opsional - memerlukan X Server, misalnya VcXsrv di Windows)\n",
        "# Jika Anda berada di lingkungan tanpa kepala (headless server), ini akan gagal.\n",
        "# Untuk visualisasi di headless server, Anda mungkin perlu Xvfb atau pyvirtualdisplay.\n",
        "try:\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"CartPole-v1 Environment\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Tidak dapat me-render lingkungan: {e}\")\n",
        "    print(\"Pastikan Anda memiliki X Server yang berjalan jika di Windows/Linux Desktop, atau gunakan pyvirtualdisplay di server headless.\")\n",
        "\n",
        "# 3. Aksi dan Observasi\n",
        "print(f\"Ruang Aksi: {env.action_space}\")\n",
        "# Discrete(2) berarti ada 2 aksi diskrit: 0 (percepat kiri), 1 (percepat kanan).\n",
        "\n",
        "print(\"Melakukan satu langkah di lingkungan...\")\n",
        "action = 1 # Contoh aksi: mempercepat ke kanan\n",
        "obs, reward, done, info = env.step(action)\n",
        "print(f\"Observasi Baru: {obs}\")\n",
        "print(f\"Reward: {reward}\") # Reward 1.0 per langkah yang berhasil\n",
        "print(f\"Done (Episode Selesai): {done}\") # True jika tiang jatuh atau keluar layar\n",
        "print(f\"Info Tambahan: {info}\")\n",
        "\n",
        "# 4. Contoh Basic Policy (Hardcoded)\n",
        "print(\"\\nMenguji basic policy (hardcoded) selama 500 episode...\")\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2] # Ambil sudut tiang\n",
        "    return 0 if angle < 0 else 1 # Pergi kiri jika condong kiri, kanan jika condong kanan\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200): # Maksimal 200 langkah per episode di CartPole-v1\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)\n",
        "\n",
        "print(f\"Reward Rata-rata: {np.mean(totals):.2f}\")\n",
        "print(f\"Standar Deviasi Reward: {np.std(totals):.2f}\")\n",
        "print(f\"Reward Minimal: {np.min(totals):.2f}\")\n",
        "print(f\"Reward Maksimal: {np.max(totals):.2f}\")\n",
        "\n",
        "# Penting: Selalu tutup lingkungan setelah selesai\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oclm5uiDBrlS"
      },
      "source": [
        "**Diskusi Hasil Basic Policy:**\n",
        "Seperti yang terlihat dari hasil di atas, *basic policy* (hardcoded) tidak mampu menjaga tiang seimbang untuk waktu yang lama. Ini menunjukkan bahwa meskipun mudah untuk diimplementasikan, *policy* sederhana mungkin tidak optimal dan berkinerja buruk. Kita akan melihat apakah jaringan saraf tiruan dapat menghasilkan *policy* yang lebih baik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK5QLsERBrlS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGLKLQdhBrlS"
      },
      "source": [
        "### **5. *Neural Network Policies***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pygeaQwuBrlS"
      },
      "source": [
        "Salah satu pendekatan umum dalam Reinforcement Learning adalah menggunakan Jaringan Saraf Tiruan (Neural Network) untuk merepresentasikan **policy** agen. Jaringan saraf ini akan mengambil **observasi** lingkungan sebagai input dan menghasilkan **aksi** yang harus diambil.\n",
        "\n",
        "Untuk lingkungan seperti CartPole, di mana observasi (`obs`) berisi semua informasi yang diperlukan tentang *state* lingkungan (posisi kereta, kecepatan, sudut tiang, kecepatan sudut), jaringan saraf sederhana dengan *feedforward* sudah cukup. Namun, jika ada *state* tersembunyi atau observasi yang bising, maka Recurrent Neural Network (RNN) mungkin diperlukan untuk mempertimbangkan observasi dan aksi sebelumnya.\n",
        "\n",
        "**Mengapa Memilih Aksi Secara Stokastik?**\n",
        "Daripada hanya memilih aksi dengan probabilitas tertinggi yang diprediksi oleh jaringan saraf, kita memilih aksi secara acak berdasarkan distribusi probabilitas yang diberikan oleh jaringan. Pendekatan ini memungkinkan agen untuk menyeimbangkan antara:\n",
        "* **Eksplorasi (Exploration)**: Mencoba aksi-aksi baru atau jalur yang belum dikenal untuk menemukan *reward* potensial yang lebih tinggi.\n",
        "* **Eksploitasi (Exploitation)**: Memanfaatkan aksi-aksi yang sudah diketahui menghasilkan *reward* yang baik.\n",
        "\n",
        "Tanpa eksplorasi, agen bisa terjebak dalam *local optima*, yaitu solusi yang baik tetapi bukan yang terbaik secara keseluruhan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsY9uZJSBrlS"
      },
      "source": [
        "### Kode Praktik: Membangun Jaringan Saraf Tiruan untuk Policy (CartPole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "CJtryqt2BrlT",
        "outputId": "ec7bf134-c313-4042-c9b8-a0e32240571d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah Input Jaringan Saraf (Observasi): 4\n",
            "Jumlah Output Jaringan Saraf (Aksi): 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m25\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31\u001b[0m (124.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> (124.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31\u001b[0m (124.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> (124.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Pastikan env sudah diinisialisasi atau inisialisasi ulang jika belum\n",
        "try:\n",
        "    env.reset() # Coba reset env yang sudah ada\n",
        "except NameError: # Jika env belum ada\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.reset()\n",
        "\n",
        "n_inputs = env.observation_space.shape[0] # Ukuran observasi CartPole adalah 4\n",
        "n_outputs = env.action_space.n # Jumlah aksi CartPole adalah 2 (kiri/kanan)\n",
        "\n",
        "print(f\"Jumlah Input Jaringan Saraf (Observasi): {n_inputs}\")\n",
        "print(f\"Jumlah Output Jaringan Saraf (Aksi): {n_outputs}\")\n",
        "\n",
        "# Model Sequential sederhana dengan satu neuron output dan aktivasi sigmoid\n",
        "# Neuron output tunggal untuk probabilitas aksi 0 (kiri), probabilitas aksi 1 (kanan) adalah 1 - p\n",
        "model_policy = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_policy.summary()\n",
        "\n",
        "# Tutup lingkungan setelah digunakan (jika tidak akan digunakan lagi di bagian ini)\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w84r-BgSBrlT"
      },
      "source": [
        "**Diskusi Model Policy:**\n",
        "Model yang dibuat adalah jaringan saraf *feedforward* sederhana. Inputnya adalah 4 *float* dari observasi CartPole, dan outputnya adalah 1 *float* (probabilitas). Aktivasi *sigmoid* di layer output memastikan output berada di antara 0 dan 1, yang dapat diinterpretasikan sebagai probabilitas. Jika ada lebih dari dua aksi, kita akan menggunakan lebih banyak neuron output dan aktivasi *softmax*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoN-ZTYWBrlT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQboSTOIBrlT"
      },
      "source": [
        "### **6. Mengevaluasi Aksi: Masalah *Credit Assignment***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBL-oP7DBrlT"
      },
      "source": [
        "Salah satu tantangan fundamental dalam Reinforcement Learning adalah **masalah *credit assignment***. Ketika agen menerima *reward*, sulit untuk menentukan aksi mana dari serangkaian aksi sebelumnya yang harus 'diberi kredit' (atau 'disalahkan') atas *reward* tersebut, terutama karena *reward* seringkali jarang dan tertunda. Misalnya, jika agen berhasil menyeimbangkan tiang selama 100 langkah, bagaimana ia tahu aksi mana yang baik dan mana yang buruk dari 100 aksi tersebut?\n",
        "\n",
        "Untuk mengatasi masalah ini, strategi umum adalah mengevaluasi aksi berdasarkan **jumlah semua *reward* yang datang setelahnya**, yang biasanya menerapkan **faktor diskon ($\\gamma$)** pada setiap langkah. Jumlah *reward* yang didiskon ini disebut **Return** dari aksi tersebut.\n",
        "\n",
        "**Return ($R_t$)**: Jumlah *reward* yang didiskon dari waktu $t$ hingga akhir episode. Rumusnya adalah:\n",
        "\n",
        "$$ R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^{T-t} r_T $$\n",
        "\n",
        "Di mana:\n",
        "* $r_t$ adalah *reward* pada waktu $t$.\n",
        "* $\\gamma$ (gamma) adalah **faktor diskon** (biasanya antara 0,9 dan 0,99).\n",
        "    * Jika $\\gamma$ mendekati 0, *reward* di masa depan kurang penting dibandingkan *reward* langsung.\n",
        "    * Jika $\\gamma$ mendekati 1, *reward* di masa depan hampir sama pentingnya dengan *reward* langsung.\n",
        "* $T$ adalah langkah waktu terakhir dalam episode.\n",
        "\n",
        "**Action Advantage**: Untuk mengetahui seberapa baik atau buruk suatu aksi dibandingkan dengan aksi lain yang mungkin, kita perlu menormalisasi *return* aksi. Ini biasanya dilakukan dengan mengurangi rata-rata dan membagi dengan standar deviasi semua *return* aksi dari beberapa episode. Aksi dengan *advantage* positif dianggap baik, sementara yang negatif dianggap buruk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuHaoRCHBrlT"
      },
      "source": [
        "### Kode Praktik: Menghitung Return dan Normalisasi Advantage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vVDPMsdBrlT",
        "outputId": "6e5d810c-526b-4443-9a8a-eaae7d64639d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards asli: [10, 0, -50]\n",
            "Discounted rewards (gamma=0.8): [-22 -40 -50]\n",
            "\n",
            "Semua Rewards Asli: [[10, 0, -50], [10, 20]]\n",
            "Normalized Advantages: [array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk menghitung discounted rewards\n",
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    # Iterasi mundur untuk menghitung discounted sum\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "# Contoh penggunaan discount_rewards\n",
        "rewards_example = [10, 0, -50]\n",
        "discount_factor_example = 0.8\n",
        "discounted_rewards_output = discount_rewards(rewards_example, discount_factor_example)\n",
        "print(f\"Rewards asli: {rewards_example}\")\n",
        "print(f\"Discounted rewards (gamma={discount_factor_example}): {discounted_rewards_output}\")\n",
        "# Hasil harusnya [-22, -40, -50] seperti di buku.\n",
        "\n",
        "# Fungsi untuk menormalisasi discounted rewards (untuk mendapatkan action advantage)\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "\n",
        "    # Menggabungkan semua discounted rewards menjadi satu array datar\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "\n",
        "    # Menghitung mean dan standard deviation dari semua rewards\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "\n",
        "    # Menormalisasi setiap discounted reward\n",
        "    # Tambahkan epsilon kecil untuk menghindari pembagian dengan nol jika reward_std sangat kecil\n",
        "    return [(discounted_rewards - reward_mean) / (reward_std + 1e-8)\n",
        "            for discounted_rewards in all_discounted_rewards]\n",
        "\n",
        "# Contoh penggunaan discount_and_normalize_rewards\n",
        "all_rewards_example = [[10, 0, -50], [10, 20]]\n",
        "normalized_advantages = discount_and_normalize_rewards(all_rewards_example, discount_factor_example)\n",
        "print(f\"\\nSemua Rewards Asli: {all_rewards_example}\")\n",
        "print(f\"Normalized Advantages: {normalized_advantages}\")\n",
        "# Hasil harusnya array([[-0.28435071, -0.86597718, -1.18910299]), ([1.26665318, 1.0727777 ])]).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w87rn65BrlU"
      },
      "source": [
        "**Diskusi Hasil:**\n",
        "Fungsi `discount_rewards` secara akurat menghitung total *reward* yang didiskon untuk setiap langkah. Fungsi `discount_and_normalize_rewards` kemudian mengambil *return* dari beberapa episode dan menormalisasinya untuk menghasilkan **action advantage**. Normalisasi ini penting karena memastikan bahwa *gradient* diterapkan secara proporsional, di mana aksi yang menghasilkan *return* lebih tinggi dari rata-rata akan diperkuat, dan aksi yang menghasilkan *return* lebih rendah dari rata-rata akan dilemahkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4AwRYsRBrlU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wxz45ciBrlZ"
      },
      "source": [
        "### **7. *Policy Gradients (PG)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Nby0orBrla"
      },
      "source": [
        "Algoritma **Policy Gradients (PG)** bertujuan untuk mengoptimalkan parameter dari sebuah *policy* dengan mengikuti *gradient* ke arah *reward* yang lebih tinggi. Salah satu kelas algoritma PG yang populer adalah **REINFORCE**.\n",
        "\n",
        "**Langkah-langkah Algoritma REINFORCE:**\n",
        "1.  **Pengumpulan Episode**: Biarkan *policy* jaringan saraf bermain dalam lingkungan beberapa kali (misalnya, beberapa episode). Pada setiap langkah dalam episode, hitung *gradient* yang akan membuat aksi yang dipilih lebih mungkin terjadi. Namun, *gradient* ini **belum diterapkan** saat ini; mereka hanya disimpan.\n",
        "2.  **Perhitungan Advantage**: Setelah beberapa episode selesai, hitung *advantage* untuk setiap aksi yang diambil menggunakan metode *discounted and normalized rewards* yang dibahas sebelumnya.\n",
        "3.  **Penyesuaian Gradient**: Jika *advantage* suatu aksi positif (artinya aksi itu kemungkinan baik), terapkan *gradient* yang dihitung sebelumnya untuk membuat aksi tersebut lebih mungkin dipilih di masa depan. Sebaliknya, jika *advantage* negatif (aksi itu kemungkinan buruk), terapkan *gradient* yang berlawanan untuk membuat aksi tersebut sedikit kurang mungkin terjadi di masa depan. Ini dicapai dengan mengalikan setiap vektor *gradient* dengan *advantage* aksi yang sesuai.\n",
        "4.  **Langkah *Gradient Descent***: Hitung rata-rata semua vektor *gradient* yang dihasilkan dari langkah sebelumnya, dan gunakan rata-rata ini untuk melakukan satu langkah *Gradient Descent* pada parameter *policy* jaringan saraf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo_Oktt0Brla"
      },
      "source": [
        "### Kode Praktik: Implementasi Policy Gradients (CartPole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3zsbqtzBrla",
        "outputId": "a96d22c3-7980-47aa-c510-e5fba2b7a63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mulai pelatihan Policy Gradients...\n",
            "Iterasi 1/50, Reward Rata-rata: 19.10\n",
            "Iterasi 2/50, Reward Rata-rata: 23.70\n",
            "Iterasi 3/50, Reward Rata-rata: 26.20\n",
            "Iterasi 4/50, Reward Rata-rata: 32.00\n",
            "Iterasi 5/50, Reward Rata-rata: 22.90\n",
            "Iterasi 6/50, Reward Rata-rata: 28.50\n",
            "Iterasi 7/50, Reward Rata-rata: 28.30\n",
            "Iterasi 8/50, Reward Rata-rata: 26.00\n",
            "Iterasi 9/50, Reward Rata-rata: 38.60\n",
            "Iterasi 10/50, Reward Rata-rata: 41.90\n",
            "Iterasi 11/50, Reward Rata-rata: 41.20\n",
            "Iterasi 12/50, Reward Rata-rata: 32.40\n",
            "Iterasi 13/50, Reward Rata-rata: 42.80\n",
            "Iterasi 14/50, Reward Rata-rata: 49.90\n",
            "Iterasi 15/50, Reward Rata-rata: 39.70\n",
            "Iterasi 16/50, Reward Rata-rata: 50.50\n",
            "Iterasi 17/50, Reward Rata-rata: 41.60\n",
            "Iterasi 18/50, Reward Rata-rata: 53.40\n",
            "Iterasi 19/50, Reward Rata-rata: 40.50\n",
            "Iterasi 20/50, Reward Rata-rata: 37.70\n",
            "Iterasi 21/50, Reward Rata-rata: 45.80\n",
            "Iterasi 22/50, Reward Rata-rata: 52.90\n",
            "Iterasi 23/50, Reward Rata-rata: 52.70\n",
            "Iterasi 24/50, Reward Rata-rata: 41.40\n",
            "Iterasi 25/50, Reward Rata-rata: 36.10\n",
            "Iterasi 26/50, Reward Rata-rata: 52.50\n",
            "Iterasi 27/50, Reward Rata-rata: 49.10\n",
            "Iterasi 28/50, Reward Rata-rata: 57.80\n",
            "Iterasi 29/50, Reward Rata-rata: 52.50\n",
            "Iterasi 30/50, Reward Rata-rata: 64.80\n",
            "Iterasi 31/50, Reward Rata-rata: 61.90\n",
            "Iterasi 32/50, Reward Rata-rata: 60.90\n",
            "Iterasi 33/50, Reward Rata-rata: 53.80\n",
            "Iterasi 34/50, Reward Rata-rata: 59.10\n",
            "Iterasi 35/50, Reward Rata-rata: 64.10\n",
            "Iterasi 36/50, Reward Rata-rata: 55.60\n",
            "Iterasi 37/50, Reward Rata-rata: 70.60\n",
            "Iterasi 38/50, Reward Rata-rata: 62.70\n",
            "Iterasi 39/50, Reward Rata-rata: 61.70\n",
            "Iterasi 40/50, Reward Rata-rata: 65.00\n",
            "Iterasi 41/50, Reward Rata-rata: 63.30\n",
            "Iterasi 42/50, Reward Rata-rata: 87.10\n",
            "Iterasi 43/50, Reward Rata-rata: 56.40\n",
            "Iterasi 44/50, Reward Rata-rata: 75.30\n",
            "Iterasi 45/50, Reward Rata-rata: 78.80\n",
            "Iterasi 46/50, Reward Rata-rata: 63.10\n",
            "Iterasi 47/50, Reward Rata-rata: 68.40\n",
            "Iterasi 48/50, Reward Rata-rata: 85.00\n",
            "Iterasi 49/50, Reward Rata-rata: 80.90\n",
            "Iterasi 50/50, Reward Rata-rata: 77.50\n",
            "Pelatihan Policy Gradients Selesai.\n",
            "\n",
            "Mengevaluasi policy yang telah dilatih...\n",
            "Reward rata-rata episode setelah pelatihan: 99.54\n",
            "Reward maksimal episode setelah pelatihan: 200.00\n"
          ]
        }
      ],
      "source": [
        "# Inisialisasi lingkungan CartPole (jika belum)\n",
        "try:\n",
        "    env.reset()\n",
        "except NameError:\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.reset()\n",
        "\n",
        "n_inputs = env.observation_space.shape[0]\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "# Model Policy (Policy Network) - seperti yang dibuat di Bagian 5\n",
        "model_pg = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"), # Output probabilitas aksi 'kiri'\n",
        "])\n",
        "\n",
        "# Fungsi untuk menghitung discounted rewards (dari Bagian 6)\n",
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "# Fungsi untuk menormalisasi discounted rewards (dari Bagian 6)\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / (reward_std + 1e-8)\n",
        "            for discounted_rewards in all_discounted_rewards]\n",
        "\n",
        "# 1. Fungsi untuk memainkan satu langkah dan menghitung gradient\n",
        "def play_one_step_pg(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Prediksi probabilitas aksi 'kiri'\n",
        "        # obs[np.newaxis] mengubah (4,) menjadi (1,4) agar sesuai dengan input batch model\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "\n",
        "        # Pilih aksi secara acak berdasarkan probabilitas\n",
        "        # tf.random.uniform([1, 1]) > left_proba menghasilkan True/False\n",
        "        # tf.cast mengubah True/False menjadi 1/0\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "\n",
        "        # Tentukan target probabilitas: 1 jika aksi 'kiri' dipilih, 0 jika 'kanan' dipilih\n",
        "        # Ini membuat loss_fn mencoba meningkatkan probabilitas aksi yang dipilih\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "\n",
        "        # Hitung loss (binary cross-entropy antara target dan probabilitas prediksi)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "\n",
        "    # Hitung gradient dari loss terhadap variabel trainable model\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Lakukan aksi di lingkungan\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "\n",
        "    return obs, reward, done, grads\n",
        "\n",
        "# 2. Fungsi untuk memainkan beberapa episode\n",
        "def play_multiple_episodes_pg(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step_pg(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads) # grads adalah list of tensors\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads) # all_grads adalah list of list of list of tensors\n",
        "    return all_rewards, all_grads\n",
        "\n",
        "\n",
        "# Hyperparameter Pelatihan PG\n",
        "n_iterations = 50 # Jumlah iterasi pelatihan\n",
        "n_episodes_per_update = 10 # Jumlah episode yang dimainkan per iterasi\n",
        "n_max_steps = 200 # Maksimal langkah per episode\n",
        "discount_factor = 0.95 # Faktor diskon untuk rewards\n",
        "\n",
        "# Optimizer dan Loss Function\n",
        "optimizer_pg = keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn_pg = keras.losses.binary_crossentropy\n",
        "\n",
        "print(\"Mulai pelatihan Policy Gradients...\")\n",
        "for iteration in range(n_iterations):\n",
        "    # Langkah 1: Mainkan episode dan kumpulkan rewards dan gradient\n",
        "    all_rewards, all_grads = play_multiple_episodes_pg(\n",
        "        env, n_episodes_per_update, n_max_steps, model_pg, loss_fn_pg)\n",
        "\n",
        "    # Langkah 2: Hitung dan normalisasi discounted rewards (advantages)\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "\n",
        "    # Langkah 3: Sesuaikan gradient dengan advantages\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model_pg.trainable_variables)): # Iterasi per variabel model\n",
        "        # Hitung rata-rata gradient yang disesuaikan untuk variabel ini\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "             for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "\n",
        "    # Langkah 4: Terapkan gradient ke model\n",
        "    optimizer_pg.apply_gradients(zip(all_mean_grads, model_pg.trainable_variables))\n",
        "\n",
        "    # Opsional: Cetak rata-rata reward dari episode yang baru saja dimainkan\n",
        "    mean_episode_reward = np.mean([sum(rewards) for rewards in all_rewards])\n",
        "    print(f\"Iterasi {iteration+1}/{n_iterations}, Reward Rata-rata: {mean_episode_reward:.2f}\")\n",
        "\n",
        "print(\"Pelatihan Policy Gradients Selesai.\")\n",
        "\n",
        "# Evaluasi akhir model_pg\n",
        "print(\"\\nMengevaluasi policy yang telah dilatih...\")\n",
        "final_rewards = []\n",
        "for episode in range(100): # Evaluasi selama 100 episode\n",
        "    episode_reward = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200): # Maksimal 200 langkah\n",
        "        # Tidak ada eksplorasi (epsilon=0) untuk evaluasi\n",
        "        left_proba = model_pg(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    final_rewards.append(episode_reward)\n",
        "\n",
        "print(f\"Reward rata-rata episode setelah pelatihan: {np.mean(final_rewards):.2f}\")\n",
        "print(f\"Reward maksimal episode setelah pelatihan: {np.max(final_rewards):.2f}\")\n",
        "\n",
        "# Tutup lingkungan\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rRefGBnBrlb"
      },
      "source": [
        "**Diskusi Hasil Policy Gradients:**\n",
        "Implementasi Policy Gradients berhasil melatih agen untuk menyeimbangkan tiang di CartPole. Anda akan melihat bahwa *reward* rata-rata per episode meningkat secara signifikan seiring iterasi, mendekati nilai maksimal 200 (yang merupakan batas default lingkungan ini). Meskipun algoritma ini memecahkan tugas CartPole, perlu dicatat bahwa ia mungkin **kurang efisien dalam sampel** (membutuhkan banyak interaksi dengan lingkungan) untuk tugas-tugas yang lebih besar dan kompleks. Ini menjadi dasar bagi algoritma yang lebih canggih seperti *Actor-Critic algorithms*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEyutjhRBrlc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDM2kojBrlc"
      },
      "source": [
        "### **8. *Markov Decision Processes (MDPs)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hhQiQ3CBrld"
      },
      "source": [
        "**Markov Decision Process (MDP)** adalah kerangka matematis untuk memodelkan pengambilan keputusan di mana hasil sebagian acak dan sebagian di bawah kendali pembuat keputusan. MDP mirip dengan **Markov Chains**, tetapi dengan tambahan aksi dan *reward*.\n",
        "\n",
        "**Elemen-elemen Kunci MDP:**\n",
        "* **State ($s$)**: Kondisi saat ini dari lingkungan.\n",
        "* **Action ($a$)**: Pilihan yang dapat diambil agen dalam *state* tertentu.\n",
        "* **Transition Probability ($T(s, a, s')$)**: Probabilitas berpindah dari *state* $s$ ke *state* $s'$ setelah mengambil aksi $a$.\n",
        "* **Reward ($R(s, a, s')$)**: Hadiah (numerik) yang diterima agen ketika berpindah dari *state* $s$ ke *state* $s'$ setelah mengambil aksi $a$.\n",
        "* **Discount Factor ($\\gamma$)**: Faktor yang menentukan seberapa besar *reward* di masa depan dihargai.\n",
        "* **Policy ($\\pi$)**: Aturan yang menentukan aksi apa yang harus diambil agen dalam *state* tertentu.\n",
        "\n",
        "Tujuan dalam MDP adalah menemukan **optimal policy ($\\pi^*$)** yang akan memaksimalkan total *reward* yang didiskon di masa depan.\n",
        "\n",
        "**Optimal State Value ($V^*(s)$)**: Jumlah semua *reward* di masa depan yang didiskon yang dapat diharapkan agen rata-rata setelah mencapai *state* $s$, dengan asumsi agen bertindak secara optimal. Ini didefinisikan oleh **Bellman Optimality Equation**:\n",
        "\n",
        "$$ V^*(s) = \\max_a \\sum_{s'} T(s,a,s') [R(s,a,s') + \\gamma V^*(s')] \\quad \\text{for all } s $$\n",
        "\n",
        "**Value Iteration Algorithm**: Sebuah algoritma iteratif yang dapat secara tepat memperkirakan *optimal state value* dari setiap *state* yang mungkin. Algoritma ini menjamin konvergensi ke nilai *state* optimal.\n",
        "\n",
        "**Optimal State-Action Values (Q-Values, $Q^*(s,a)$)**: Jumlah *reward* di masa depan yang didiskon yang dapat diharapkan agen rata-rata setelah mencapai *state* $s$ dan memilih aksi $a$, sebelum melihat hasil dari aksi ini, dengan asumsi agen bertindak secara optimal setelah aksi tersebut.\n",
        "\n",
        "**Q-Value Iteration Algorithm**: Mirip dengan Value Iteration, tetapi memperbarui estimasi Q-Value:\n",
        "\n",
        "$$ Q_{k+1}(s,a) = \\sum_{s'} T(s,a,s') [R(s,a,s') + \\gamma \\max_{a'} Q_k(s',a')] $$\n",
        "\n",
        "Setelah Q-Value optimal ($Q^*$) ditemukan, **optimal policy ($\\pi^*(s)$)** dapat ditentukan dengan memilih aksi yang memiliki Q-Value tertinggi untuk *state* tersebut: $\\pi^*(s) = \\operatorname{argmax}_a Q^*(s,a)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44Qcgv0-Brld"
      },
      "source": [
        "### Kode Praktik: Implementasi Q-Value Iteration untuk MDP Sederhana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IvpzM_CBrld",
        "outputId": "0afd14ca-8185-4731-9b35-acc87ad0754b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Values Awal:\n",
            "[[  0.   0.   0.]\n",
            " [  0. -inf   0.]\n",
            " [-inf   0. -inf]]\n",
            "\n",
            "Mulai Algoritma Q-Value Iteration...\n",
            "\n",
            "Q-Values Akhir Setelah Iterasi:\n",
            "[[18.91891892 17.02702702 13.62162162]\n",
            " [ 0.                -inf -4.87971488]\n",
            " [       -inf 50.13365013        -inf]]\n",
            "\n",
            "Optimal Policy (Aksi terbaik untuk setiap State): [0 0 1]\n",
            "\n",
            "Mencoba faktor diskon berbeda (gamma = 0.95) dan melihat perubahan policy...\n",
            "Optimal Policy (gamma=0.95): [0 2 1]\n"
          ]
        }
      ],
      "source": [
        "# Mendefinisikan MDP (seperti di buku )\n",
        "transition_probabilities = [ # shape=[s, a, s']\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], # State 0: (a0->s0,s1), (a1->s0), (a2->s0,s1)\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]], # State 1: (a0->s1), (a1-impossible), (a2->s2)\n",
        "    [None, [0.8, 0.1, 0.1], None] # State 2: (a0-impossible), (a1->s0,s1,s2), (a2-impossible)\n",
        "]\n",
        "\n",
        "rewards = [ # shape=[s, a, s']\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]], # State 0\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]], # State 1\n",
        "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]] # State 2\n",
        "]\n",
        "\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]] # Aksi yang mungkin untuk setiap state\n",
        "\n",
        "# Inisialisasi semua Q-Values ke 0 (kecuali aksi yang tidak mungkin ke -inf)\n",
        "Q_values_iteration = np.full((3, 3), -np.inf) # 3 states, 3 possible actions (global index)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values_iteration[state, actions] = 0.0\n",
        "\n",
        "print(\"Q-Values Awal:\")\n",
        "print(Q_values_iteration)\n",
        "\n",
        "gamma = 0.90 # Faktor diskon\n",
        "n_iterations_q_value_iteration = 50 # Jumlah iterasi untuk konvergensi\n",
        "\n",
        "print(\"\\nMulai Algoritma Q-Value Iteration...\")\n",
        "for iteration in range(n_iterations_q_value_iteration):\n",
        "    Q_prev = Q_values_iteration.copy() # Simpan Q-Values dari iterasi sebelumnya\n",
        "    for s in range(3): # Iterasi untuk setiap state\n",
        "        for a in possible_actions[s]: # Iterasi untuk setiap aksi yang mungkin di state tersebut\n",
        "            # Hitung Q-Value baru menggunakan Bellman Equation\n",
        "            Q_values_iteration[s, a] = np.sum([ # Sum di atas s'\n",
        "                transition_probabilities[s][a][sp] # P(s'|s,a)\n",
        "                * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) # R(s,a,s') + gamma * max_a' Q_prev(s',a')\n",
        "                for sp in range(3) # Iterasi untuk setiap next_state (sp)\n",
        "            ])\n",
        "    # print(f\"Iterasi {iteration+1}, Q-Values:\\n{Q_values_iteration}\")\n",
        "\n",
        "print(\"\\nQ-Values Akhir Setelah Iterasi:\")\n",
        "print(Q_values_iteration)\n",
        "\n",
        "optimal_policy = np.argmax(Q_values_iteration, axis=1)\n",
        "print(f\"\\nOptimal Policy (Aksi terbaik untuk setiap State): {optimal_policy}\")\n",
        "\n",
        "print(\"\\nMencoba faktor diskon berbeda (gamma = 0.95) dan melihat perubahan policy...\")\n",
        "gamma_new = 0.95\n",
        "Q_values_iteration_high_gamma = np.full((3, 3), -np.inf)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values_iteration_high_gamma[state, actions] = 0.0\n",
        "\n",
        "for iteration in range(n_iterations_q_value_iteration):\n",
        "    Q_prev = Q_values_iteration_high_gamma.copy()\n",
        "    for s in range(3):\n",
        "        for a in possible_actions[s]:\n",
        "            Q_values_iteration_high_gamma[s, a] = np.sum([\n",
        "                transition_probabilities[s][a][sp]\n",
        "                * (rewards[s][a][sp] + gamma_new * np.max(Q_prev[sp]))\n",
        "                for sp in range(3)\n",
        "            ])\n",
        "\n",
        "optimal_policy_high_gamma = np.argmax(Q_values_iteration_high_gamma, axis=1)\n",
        "print(f\"Optimal Policy (gamma={gamma_new}): {optimal_policy_high_gamma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qR6N-NsBrle"
      },
      "source": [
        "**Diskusi Hasil Q-Value Iteration:**\n",
        "Algoritma Q-Value Iteration berhasil menghitung Q-Values optimal untuk setiap pasangan *state-action* dan menyimpulkan *optimal policy*. Seperti yang ditunjukkan oleh contoh di atas, perubahan faktor diskon ($\\gamma$) dari 0.90 menjadi 0.95 dapat mengubah *optimal policy*. Ini menunjukkan bahwa seberapa besar kita menghargai *reward* di masa depan sangat memengaruhi keputusan optimal agen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-iYB2OFBrlf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oT7SyKrBrlf"
      },
      "source": [
        "### **9. *Temporal Difference (TD) Learning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0VdN540Brlf"
      },
      "source": [
        "Dalam banyak masalah Reinforcement Learning di dunia nyata, agen tidak memiliki pengetahuan awal tentang probabilitas transisi ($T(s, a, s')$) atau *reward* ($R(s, a, s')$). Agen hanya mengetahui *state* dan aksi yang mungkin, dan harus belajar melalui pengalaman.\n",
        "\n",
        "**Temporal Difference (TD) Learning** adalah algoritma yang mirip dengan Value Iteration, tetapi disesuaikan untuk situasi di mana agen hanya memiliki pengetahuan parsial tentang MDP. TD Learning memperbarui estimasi nilai *state* ($V(s)$) berdasarkan transisi dan *reward* yang benar-benar diamati.\n",
        "\n",
        "**Rumus TD Learning:**\n",
        "\n",
        "$$ V_{k+1}(s) = (1 - \\alpha) V_k(s) + \\alpha (r + \\gamma V_k(s')) $$\n",
        "atau, secara ekuivalen:\n",
        "$$ V_{k+1}(s) = V_k(s) + \\alpha \\cdot \\delta_k(s,r,s') $$\n",
        "dengan **TD Error ($\\delta_k(s,r,s')$)** didefinisikan sebagai:\n",
        "$$ \\delta_k(s,r,s') = r + \\gamma V_k(s') - V_k(s) $$\n",
        "\n",
        "Di mana:\n",
        "* $\\alpha$ adalah *learning rate* (misalnya, 0,01).\n",
        "* $r + \\gamma V_k(s')$ disebut **TD Target**.\n",
        "\n",
        "TD Learning memiliki banyak kesamaan dengan *Stochastic Gradient Descent* (SGD), yaitu memproses satu sampel pada satu waktu. Ia hanya dapat benar-benar konvergen jika *learning rate* berkurang secara bertahap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63tT0meYBrlf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7euRuIzBrlg"
      },
      "source": [
        "### **10. *Q-Learning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHPBjMeDBrlg"
      },
      "source": [
        "**Q-Learning** adalah adaptasi dari algoritma Q-Value Iteration untuk situasi di mana probabilitas transisi dan *reward* awalnya tidak diketahui. Q-Learning bekerja dengan mengamati agen bermain (misalnya, secara acak) dan secara bertahap meningkatkan estimasi Q-Value-nya. Setelah memiliki estimasi Q-Value yang akurat, *optimal policy* adalah memilih aksi yang memiliki Q-Value tertinggi (yaitu, *greedy policy*).\n",
        "\n",
        "**Rumus Q-Learning:**\n",
        "\n",
        "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)] $$\n",
        "atau, yang lebih ringkas:\n",
        "$$ Q(s,a) \\leftarrow (1 - \\alpha) Q(s,a) + \\alpha (r + \\gamma \\max_{a'} Q(s',a')) $$\n",
        "\n",
        "Di mana:\n",
        "* $Q(s,a)$ adalah estimasi Q-Value untuk *state* $s$ dan aksi $a$.\n",
        "* $\\alpha$ adalah *learning rate*.\n",
        "* $r$ adalah *reward* yang diamati.\n",
        "* $\\gamma$ adalah faktor diskon.\n",
        "* $\\max_{a'} Q(s',a')$ adalah Q-Value maksimum untuk *state* berikutnya $s'$, yang menunjukkan asumsi bahwa agen akan bertindak secara optimal dari *state* tersebut.\n",
        "\n",
        "Q-Learning disebut algoritma **off-policy** karena *policy* yang dilatih (memilih aksi dengan Q-Value tertinggi) tidak selalu sama dengan *policy* yang dieksekusi untuk eksplorasi. Ini memungkinkan agen untuk belajar *optimal policy* bahkan dengan mengamati perilaku acak.\n",
        "\n",
        "### Kode Praktik: Implementasi Q-Learning (untuk MDP Sederhana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di7IlRWHBrlg",
        "outputId": "98d78c17-e2a1-4f00-fbd1-12eed52db9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Values Awal (Q-Learning):\n",
            "[[  0.   0.   0.]\n",
            " [  0. -inf   0.]\n",
            " [-inf   0. -inf]]\n",
            "\n",
            "Mulai Algoritma Q-Learning...\n",
            "\n",
            "Q-Values Akhir Setelah Pelatihan Q-Learning:\n",
            "[[17.23317334 14.96064772 11.84343144]\n",
            " [ 0.                -inf -9.44280078]\n",
            " [       -inf 47.50311172        -inf]]\n",
            "\n",
            "Optimal Policy (Q-Learning): [0 0 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Mendefinisikan MDP (sama seperti Bagian 8)\n",
        "transition_probabilities_ql = [\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]\n",
        "]\n",
        "rewards_ql = [\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
        "]\n",
        "possible_actions_ql = [[0, 1, 2], [0, 2], [1]]\n",
        "\n",
        "# Fungsi untuk melakukan satu langkah di lingkungan MDP sederhana\n",
        "def step_ql(state, action):\n",
        "    probas = transition_probabilities_ql[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "    reward = rewards_ql[state][action][next_state]\n",
        "    return next_state, reward\n",
        "\n",
        "# Fungsi untuk kebijakan eksplorasi acak (dari buku )\n",
        "def exploration_policy_ql(state):\n",
        "    return np.random.choice(possible_actions_ql[state])\n",
        "\n",
        "# Hyperparameter Q-Learning\n",
        "alpha0 = 0.05 # Initial learning rate\n",
        "decay = 0.005 # Learning rate decay\n",
        "gamma = 0.90 # Discount factor\n",
        "state = 0 # Initial state\n",
        "\n",
        "# Inisialisasi Q-Values (sama seperti Q-Value Iteration)\n",
        "Q_values_ql = np.full((3, 3), -np.inf)\n",
        "for s_init, actions_init in enumerate(possible_actions_ql):\n",
        "    Q_values_ql[s_init, actions_init] = 0.0\n",
        "\n",
        "print(\"Q-Values Awal (Q-Learning):\")\n",
        "print(Q_values_ql)\n",
        "\n",
        "print(\"\\nMulai Algoritma Q-Learning...\")\n",
        "for iteration in range(10000):\n",
        "    action = exploration_policy_ql(state) # Pilih aksi dengan kebijakan eksplorasi\n",
        "    next_state, reward = step_ql(state, action) # Lakukan aksi di lingkungan\n",
        "\n",
        "    # Hitung next_value (max Q-Value di next_state)\n",
        "    next_value = np.max(Q_values_ql[next_state])\n",
        "\n",
        "    # Perbarui learning rate (power scheduling)\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "\n",
        "    # Perbarui Q-Value menggunakan rumus Q-Learning\n",
        "    Q_values_ql[state, action] = (1 - alpha) * Q_values_ql[state, action] + alpha * (reward + gamma * next_value)\n",
        "\n",
        "    state = next_state # Pindah ke state berikutnya\n",
        "\n",
        "print(\"\\nQ-Values Akhir Setelah Pelatihan Q-Learning:\")\n",
        "print(Q_values_ql)\n",
        "\n",
        "optimal_policy_ql = np.argmax(Q_values_ql, axis=1)\n",
        "print(f\"\\nOptimal Policy (Q-Learning): {optimal_policy_ql}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0HPxFg5Brlg"
      },
      "source": [
        "**Diskusi Hasil Q-Learning:**\n",
        "Algoritma Q-Learning berhasil memperkirakan Q-Values optimal dan menemukan *optimal policy* untuk MDP sederhana. Perlu diperhatikan bahwa Q-Learning membutuhkan lebih banyak iterasi untuk konvergen dibandingkan Q-Value Iteration. Ini karena Q-Learning harus belajar probabilitas transisi dan *reward* dari pengalaman, sedangkan Q-Value Iteration sudah memiliki informasi tersebut. Meskipun demikian, Q-Learning mampu belajar kebijakan optimal hanya dengan mengamati agen bertindak secara acak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUwrKyksBrlh"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7e2f0hGBrlh"
      },
      "source": [
        "### **11. Kebijakan Eksplorasi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA_RNPqRBrlh"
      },
      "source": [
        "Q-Learning hanya dapat berfungsi jika **kebijakan eksplorasi** menjelajahi MDP secara menyeluruh. Meskipun kebijakan acak murni dijamin akan mengunjungi setiap *state* dan transisi pada akhirnya, ini mungkin memakan waktu yang sangat lama. Oleh karena itu, opsi yang lebih baik adalah menggunakan **kebijakan $\\epsilon$-greedy** ($\\epsilon$ adalah epsilon).\n",
        "\n",
        "**Kebijakan $\\epsilon$-greedy:**\n",
        "Pada setiap langkah, agen:\n",
        "* Bertindak secara acak dengan probabilitas $\\epsilon$.\n",
        "* Bertindak secara serakah (memilih aksi dengan Q-Value tertinggi) dengan probabilitas $1 - \\epsilon$.\n",
        "\n",
        "Keuntungan kebijakan $\\epsilon$-greedy adalah bahwa ia akan menghabiskan lebih banyak waktu untuk menjelajahi bagian-bagian lingkungan yang menarik (saat estimasi Q-Value membaik), sambil tetap meluangkan waktu untuk mengunjungi wilayah MDP yang tidak dikenal. Umumnya, nilai $\\epsilon$ dimulai dari tinggi (misalnya, 1.0) dan kemudian secara bertahap dikurangi (misalnya, hingga 0.05).\n",
        "\n",
        "**Alternatif untuk Eksplorasi:**\n",
        "Daripada hanya mengandalkan peluang, pendekatan lain adalah mendorong *policy* eksplorasi untuk mencoba aksi-aksi yang belum banyak dicoba sebelumnya. Ini dapat diimplementasikan sebagai bonus yang ditambahkan ke estimasi Q-Value:\n",
        "\n",
        "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot [r + \\gamma \\max_{a'} f(Q(s',a'), N(s',a')) - Q(s,a)] $$\n",
        "\n",
        "Di mana:\n",
        "* $N(s',a')$ menghitung berapa kali aksi $a'$ dipilih dalam *state* $s'$.\n",
        "* $f(Q, N)$ adalah fungsi eksplorasi, seperti $f(Q, N) = Q + \\kappa / (1 + N)$, di mana $\\kappa$ adalah *hyperparameter* 'rasa ingin tahu' yang mengukur seberapa besar agen tertarik pada hal yang tidak diketahui."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34RatSpBrlh"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btjjC5xLBrlh"
      },
      "source": [
        "### **12. *Approximate Q-Learning* dan *Deep Q-Learning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIf9aGNBrli"
      },
      "source": [
        "Masalah utama Q-Learning adalah skalabilitasnya yang buruk untuk MDP yang besar atau bahkan menengah, yang memiliki banyak *state* dan aksi. Untuk mengatasi ini, digunakan **Approximate Q-Learning**, di mana fungsi $Q_\\theta(s,a)$ mengaproksimasi Q-Value dari setiap pasangan *state-action* menggunakan sejumlah parameter yang dapat dikelola.\n",
        "\n",
        "**Deep Q-Network (DQN)** adalah Jaringan Saraf Tiruan (DNN) yang digunakan untuk mengaproksimasi Q-Values. Menggunakan DQN untuk Approximate Q-Learning disebut **Deep Q-Learning**.\n",
        "\n",
        "**Melatih DQN:**\n",
        "DQN dilatih dengan meminimalkan *squared error* antara Q-Value yang diperkirakan ($Q(s,a)$) dan *target Q-Value* ($Q_{target}(s,a)$). *Target Q-Value* dihitung berdasarkan *reward* yang diamati ($r$) ditambah nilai didiskon dari bermain optimal di *state* berikutnya ($s'$). Rumus *target Q-Value* adalah:\n",
        "\n",
        "$$ Q_{target}(s,a) = r + \\gamma \\cdot \\max_{a'} Q_\\theta(s',a') $$\n",
        "\n",
        "Di mana:\n",
        "* $r$ adalah *reward* yang diamati.\n",
        "* $\\gamma$ adalah faktor diskon.\n",
        "* $\\max_{a'} Q_\\theta(s',a')$ adalah Q-Value maksimum yang diprediksi oleh DQN untuk *state* berikutnya $s'$ dari semua aksi $a'$ yang mungkin.\n",
        "\n",
        "Untuk menstabilkan pelatihan, pengalaman (transisi *state*, aksi, *reward*, *next_state*) disimpan dalam **replay buffer**. Batch pelatihan acak kemudian diambil dari *replay buffer* pada setiap iterasi pelatihan, yang membantu mengurangi korelasi antar pengalaman dan sangat membantu stabilitas pelatihan.\n",
        "\n",
        "### Kode Praktik: Implementasi *Deep Q-Learning* (CartPole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "XsZCuYunBrli",
        "outputId": "33978036-4f01-4792-edb8-54f985e80f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mulai pelatihan Deep Q-Learning (Basic DQN)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'bool8'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-921417450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Epsilon decay: dari 1.0 ke 0.01 dalam 500 episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_step_dqn_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_dqn_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mcurrent_episode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-921417450.py\u001b[0m in \u001b[0;36mplay_one_step_dqn_full\u001b[0;34m(env, state, epsilon)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_one_step_dqn_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_policy_dqn_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Simpan pengalaman dalam replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mreplay_buffer_dqn_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             logger.warn(\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34mf\"Expects `terminated` signal to be a boolean, actual type: {type(terminated)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchararray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
          ]
        }
      ],
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Inisialisasi lingkungan CartPole\n",
        "env_dqn_full = gym.make(\"CartPole-v0\")\n",
        "\n",
        "input_shape_dqn = env_dqn_full.observation_space.shape\n",
        "n_outputs_dqn = env_dqn_full.action_space.n\n",
        "\n",
        "# Membangun Deep Q-Network (DQN)\n",
        "model_dqn_full = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape_dqn),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs_dqn) # Output Q-Value untuk setiap aksi\n",
        "])\n",
        "\n",
        "# Kebijakan epsilon-greedy\n",
        "def epsilon_greedy_policy_dqn_full(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs_dqn)\n",
        "    else:\n",
        "        # Prediksi Q-values untuk state saat ini\n",
        "        Q_values = model_dqn_full.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "# Replay Buffer\n",
        "replay_buffer_dqn_full = deque(maxlen=2000) # Ukuran buffer dari buku\n",
        "\n",
        "# Fungsi untuk mengambil sampel pengalaman dari replay buffer\n",
        "def sample_experiences_dqn_full(batch_size):\n",
        "    # Pilih indeks acak dari replay buffer\n",
        "    indices = np.random.randint(len(replay_buffer_dqn_full), size=batch_size)\n",
        "    # Ambil batch pengalaman\n",
        "    batch = [replay_buffer_dqn_full[index] for index in indices]\n",
        "    # Pisahkan komponen pengalaman ke dalam array terpisah\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones\n",
        "\n",
        "# Fungsi untuk memainkan satu langkah dan menyimpan pengalaman\n",
        "def play_one_step_dqn_full(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy_dqn_full(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    # Simpan pengalaman dalam replay buffer\n",
        "    replay_buffer_dqn_full.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "# Hyperparameter dan Optimizer/Loss Function\n",
        "batch_size_dqn = 32\n",
        "discount_factor_dqn = 0.95\n",
        "optimizer_dqn_full = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn_dqn_full = keras.losses.mse\n",
        "\n",
        "# Fungsi untuk melakukan satu langkah pelatihan\n",
        "def training_step_dqn_full(batch_size):\n",
        "    experiences = sample_experiences_dqn_full(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "    # Hitung next_Q_values menggunakan model (ini akan diubah nanti untuk Fixed Target)\n",
        "    next_Q_values = model_dqn_full.predict(next_states)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "\n",
        "    # Hitung target Q-Values\n",
        "    # (1 - dones) * ... memastikan bahwa jika episode selesai, tidak ada Q-Value masa depan yang ditambahkan\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_factor_dqn * max_next_Q_values)\n",
        "\n",
        "    # Buat mask untuk memilih Q-Values yang relevan dari aksi yang diambil\n",
        "    mask = tf.one_hot(actions, n_outputs_dqn)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model_dqn_full(states) # Prediksi semua Q-Values untuk state yang diamati\n",
        "        # Pilih hanya Q-Value untuk aksi yang diambil\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        # Hitung loss antara target dan prediksi Q-Values\n",
        "        loss = tf.reduce_mean(loss_fn_dqn_full(target_Q_values, Q_values))\n",
        "\n",
        "    # Hitung gradient dan terapkan\n",
        "    grads = tape.gradient(loss, model_dqn_full.trainable_variables)\n",
        "    optimizer_dqn_full.apply_gradients(zip(grads, model_dqn_full.trainable_variables))\n",
        "\n",
        "# Loop Pelatihan Utama Deep Q-Learning\n",
        "print(\"\\nMulai pelatihan Deep Q-Learning (Basic DQN)...\")\n",
        "episode_rewards_dqn = []\n",
        "for episode in range(600): # Jumlah episode dari buku\n",
        "    obs = env_dqn_full.reset()\n",
        "    current_episode_reward = 0\n",
        "    for step in range(200): # Maksimal langkah per episode\n",
        "        # Epsilon decay: dari 1.0 ke 0.01 dalam 500 episode\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step_dqn_full(env_dqn_full, obs, epsilon)\n",
        "        current_episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards_dqn.append(current_episode_reward)\n",
        "\n",
        "    if episode > 50: # Mulai pelatihan setelah replay buffer terisi sedikit\n",
        "        training_step_dqn_full(batch_size_dqn)\n",
        "\n",
        "    if (episode + 1) % 50 == 0: # Cetak rata-rata reward setiap 50 episode\n",
        "        print(f\"Episode {episode + 1}/{600}, Reward Rata-rata (50 episode terakhir): {np.mean(episode_rewards_dqn[-50:]):.2f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"\\nPelatihan Deep Q-Learning Selesai.\")\n",
        "\n",
        "# Plot Learning Curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(episode_rewards_dqn)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Learning Curve Deep Q-Learning (Basic DQN)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Tutup lingkungan\n",
        "env_dqn_full.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lQSDjC9Brli"
      },
      "source": [
        "**Diskusi Hasil Basic DQN:**\n",
        "Dari *learning curve* yang dihasilkan, dapat diamati bahwa kinerja agen mungkin tidak menunjukkan kemajuan yang jelas pada awalnya (terutama karena nilai $\\epsilon$ yang tinggi untuk eksplorasi). Namun, setelah beberapa episode, kinerja dapat tiba-tiba melonjak mendekati nilai maksimal 200. Ini adalah indikasi bahwa algoritma mulai belajar secara efektif. Namun, Anda mungkin juga melihat fluktuasi besar, atau bahkan penurunan tajam dalam kinerja setelah mencapai puncaknya. Fenomena ini dikenal sebagai ***catastrophic forgetting***. Ini adalah masalah umum dalam RL, di mana agen melupakan apa yang telah dipelajari di satu bagian lingkungan saat belajar di bagian lain, karena pengalaman seringkali sangat berkorelasi.\n",
        "\n",
        "**Penting:** Performa DQN sangat sensitif terhadap *hyperparameter* dan *random seed*. *Loss* bukanlah indikator performa yang baik dalam RL; yang utama adalah total *reward* yang diperoleh agen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzlvfmwoBrlj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_4WO5yBrlj"
      },
      "source": [
        "### **13. Varian *Deep Q-Learning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724Uf0kyBrlj"
      },
      "source": [
        "Algoritma Deep Q-Learning dasar cenderung tidak stabil. Oleh karena itu, DeepMind dan peneliti lain telah mengembangkan beberapa varian untuk menstabilkan dan mempercepat pelatihan.\n",
        "\n",
        "#### **a. *Fixed Q-Value Targets***\n",
        "Dalam DQN dasar, model yang sama digunakan untuk membuat prediksi dan menentukan targetnya sendiri, menciptakan *feedback loop* yang tidak stabil. Untuk mengatasinya, digunakan dua DQN:\n",
        "* **Online Model**: Model yang belajar pada setiap langkah dan digunakan untuk menggerakkan agen.\n",
        "* **Target Model**: Sebuah klon dari *online model*, hanya digunakan untuk mendefinisikan target Q-Value. *Target model* diperbarui lebih jarang (misalnya, setiap 10.000 langkah).\n",
        "\n",
        "Pendekatan ini menstabilkan target Q-Value, meredam *feedback loop*.\n",
        "\n",
        "#### **b. *Double DQN***\n",
        "Varian ini mengatasi masalah *overestimation* Q-Values oleh *target network*. Dalam Double DQN, *online model* digunakan untuk **memilih aksi terbaik** untuk *state* berikutnya ($s'$), sedangkan *target model* digunakan untuk **memperkirakan Q-Value** untuk aksi terbaik yang dipilih tersebut.\n",
        "\n",
        "#### **c. *Prioritized Experience Replay (PER)***\n",
        "Alih-alih mengambil sampel pengalaman secara seragam dari *replay buffer*, PER mengambil sampel pengalaman yang \"penting\" (yang kemungkinan besar akan mengarah pada kemajuan pembelajaran yang cepat) lebih sering. Pengalaman dianggap penting jika memiliki **TD error ($\\delta$)** yang besar. Prioritas pengalaman diatur berdasarkan $|\\delta|$, dan probabilitas sampling sebanding dengan prioritas ini. Selama pelatihan, pengalaman yang diambil sampelnya harus ditimbang ulang untuk mengkompensasi bias sampling ini.\n",
        "\n",
        "#### **d. *Dueling DQN***\n",
        "Dueling DQN menguraikan Q-Value menjadi dua komponen: **nilai *state* ($V(s)$)** dan **keuntungan aksi ($A(s,a)$)**.\n",
        "$$ Q(s,a) = V(s) + A(s,a) $$\n",
        "\n",
        "Model ini memperkirakan kedua komponen ini secara terpisah, kemudian menggabungkannya untuk mendapatkan Q-Value. Ini membantu model belajar lebih efektif karena memisahkan estimasi seberapa baik sebuah *state* itu sendiri dari seberapa baik sebuah aksi spesifik dalam *state* tersebut.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wRH41IJBrlj"
      },
      "source": [
        "### Kode Praktik: Implementasi Varian Deep Q-Learning (Update Training Step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "93TlTANyBrlj",
        "outputId": "cf0979d9-fddd-4d55-9b98-19804759ffa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mulai pelatihan Deep Q-Learning (Double DQN dengan Fixed Targets)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'bool8'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-2982116573.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_step_variants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_dqn_variants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mcurrent_episode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12-2982116573.py\u001b[0m in \u001b[0;36mplay_one_step_variants\u001b[0;34m(env, state, epsilon)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_one_step_variants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_policy_variants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mreplay_buffer_variants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             logger.warn(\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34mf\"Expects `terminated` signal to be a boolean, actual type: {type(terminated)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchararray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Inisialisasi lingkungan CartPole\n",
        "env_dqn_variants = gym.make(\"CartPole-v0\")\n",
        "\n",
        "input_shape_dqn = env_dqn_variants.observation_space.shape\n",
        "n_outputs_dqn = env_dqn_variants.action_space.n\n",
        "\n",
        "# Membangun Deep Q-Network (DQN) - Online Model\n",
        "model_online = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape_dqn),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs_dqn)\n",
        "])\n",
        "\n",
        "# Membangun Target Model (Klon dari Online Model)\n",
        "target_model = keras.models.clone_model(model_online)\n",
        "target_model.set_weights(model_online.get_weights())\n",
        "\n",
        "# Kebijakan epsilon-greedy (sama seperti sebelumnya)\n",
        "def epsilon_greedy_policy_variants(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs_dqn)\n",
        "    else:\n",
        "        Q_values = model_online.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "# Replay Buffer (sama seperti sebelumnya)\n",
        "replay_buffer_variants = deque(maxlen=2000)\n",
        "def sample_experiences_variants(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer_variants), size=batch_size)\n",
        "    batch = [replay_buffer_variants[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones\n",
        "def play_one_step_variants(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy_variants(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_buffer_variants.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "# Hyperparameter dan Optimizer/Loss Function\n",
        "batch_size_dqn = 32\n",
        "discount_factor_dqn = 0.95\n",
        "optimizer_dqn_variants = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn_dqn_variants = keras.losses.mse\n",
        "\n",
        "# === UPDATE FUNGSI training_step untuk Double DQN ===\n",
        "def training_step_double_dqn(batch_size):\n",
        "    experiences = sample_experiences_variants(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "    # Double DQN: Pilih aksi terbaik menggunakan online model\n",
        "    next_Q_values_online = model_online.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values_online, axis=1)\n",
        "\n",
        "    # Double DQN: Perkirakan Q-value untuk aksi tersebut menggunakan target model\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs_dqn).numpy()\n",
        "    next_best_Q_values = (target_model.predict(next_states) * next_mask).sum(axis=1)\n",
        "\n",
        "    # Hitung target Q-Values\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_factor_dqn * next_best_Q_values)\n",
        "\n",
        "    mask = tf.one_hot(actions, n_outputs_dqn)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model_online(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn_dqn_variants(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model_online.trainable_variables)\n",
        "    optimizer_dqn_variants.apply_gradients(zip(grads, model_online.trainable_variables))\n",
        "\n",
        "# === Loop Pelatihan Utama dengan Fixed Target dan Double DQN ===\n",
        "print(\"\\nMulai pelatihan Deep Q-Learning (Double DQN dengan Fixed Targets)...\")\n",
        "episode_rewards_double_dqn = []\n",
        "for episode in range(600):\n",
        "    obs = env_dqn_variants.reset()\n",
        "    current_episode_reward = 0\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step_variants(env_dqn_variants, obs, epsilon)\n",
        "        current_episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards_double_dqn.append(current_episode_reward)\n",
        "\n",
        "    # Perbarui target model secara berkala\n",
        "    if episode % 50 == 0:\n",
        "        target_model.set_weights(model_online.get_weights())\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step_double_dqn(batch_size_dqn)\n",
        "\n",
        "    if (episode + 1) % 50 == 0:\n",
        "        print(f\"Episode {episode + 1}/{600}, Reward Rata-rata (50 episode terakhir): {np.mean(episode_rewards_double_dqn[-50:]):.2f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"\\nPelatihan Double DQN Selesai.\")\n",
        "\n",
        "# Plot Learning Curve untuk Double DQN\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(episode_rewards_double_dqn)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Learning Curve Deep Q-Learning (Double DQN with Fixed Targets)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === Membangun Model Dueling DQN ===\n",
        "print(\"\\nMembangun model Dueling DQN...\")\n",
        "K = keras.backend\n",
        "\n",
        "input_states_dueling = keras.layers.Input(shape=input_shape_dqn)\n",
        "hidden1_dueling = keras.layers.Dense(32, activation=\"elu\")(input_states_dueling)\n",
        "hidden2_dueling = keras.layers.Dense(32, activation=\"elu\")(hidden1_dueling)\n",
        "\n",
        "state_values = keras.layers.Dense(1)(hidden2_dueling) # Output value V(s)\n",
        "raw_advantages = keras.layers.Dense(n_outputs_dqn)(hidden2_dueling) # Output advantage A(s,a)\n",
        "\n",
        "# Normalisasi advantages: A(s,a*) = 0\n",
        "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "\n",
        "# Q(s,a) = V(s) + A(s,a)\n",
        "Q_values_dueling_model = state_values + advantages\n",
        "\n",
        "model_dueling_dqn = keras.Model(inputs=[input_states_dueling], outputs=[Q_values_dueling_model])\n",
        "model_dueling_dqn.summary()\n",
        "\n",
        "# Tutup lingkungan\n",
        "env_dqn_variants.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLV3biAlBrlk"
      },
      "source": [
        "**Diskusi Hasil Varian DQN:**\n",
        "Implementasi Double DQN dengan Fixed Targets menunjukkan peningkatan stabilitas pelatihan dan berpotensi mencapai kinerja yang lebih baik dibandingkan Basic DQN. *Catastrophic forgetting* dapat diredam, dan *learning curve* mungkin terlihat lebih halus.\n",
        "\n",
        "Model Dueling DQN mengubah arsitektur jaringan untuk memisahkan estimasi nilai *state* dan keuntungan aksi. Ini adalah contoh bagaimana modifikasi arsitektur dapat meningkatkan kinerja dan stabilitas agen RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GTSqv2GBrll"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qGduSJjBrll"
      },
      "source": [
        "### **14. Pustaka TF-Agents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFyuDztFBrll"
      },
      "source": [
        "**TF-Agents** adalah pustaka Reinforcement Learning yang dikembangkan oleh Google dan dirilis sebagai *open source* pada tahun 2018. Pustaka ini dibangun di atas TensorFlow dan menyediakan kerangka kerja yang kuat dan fleksibel untuk membangun, melatih, dan mengevaluasi agen RL skala besar.\n",
        "\n",
        "**Fitur Utama TF-Agents:**\n",
        "* **Lingkungan Siap Pakai**: Mendukung berbagai lingkungan, termasuk *wrapper* untuk lingkungan OpenAI Gym, PyBullet (simulasi fisik 3D), DeepMind's DM Control, dan Unity's ML-Agents.\n",
        "* **Algoritma RL**: Mengimplementasikan banyak algoritma RL populer seperti REINFORCE, DQN, dan Double DQN (DDQN).\n",
        "* **Komponen Modular**: Menyediakan komponen RL yang efisien seperti *replay buffers* dan metrik.\n",
        "* **Fleksibilitas dan Kustomisasi**: Memungkinkan pengguna membuat lingkungan kustom, jaringan saraf kustom, dan menyesuaikan hampir setiap komponen.\n",
        "* **Performa dan Skalabilitas**: Dirancang untuk kinerja tinggi dan skalabilitas, cocok untuk pelatihan model yang kompleks.\n",
        "\n",
        "Kita akan menggunakan TF-Agents untuk melatih agen bermain Breakout, sebuah game Atari klasik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwezNI1mBrll"
      },
      "source": [
        "### Kode Praktik: TF-Agents - Melatih Agen Breakout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZh8xaQtBrlm",
        "outputId": "78ef6327-e790-48f4-d9f7-aae86a45a163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (3.1.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (2.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf_agents) (11.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (1.17.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (5.29.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (1.17.2)\n",
            "Collecting typing-extensions==4.5.0 (from tf_agents)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting tensorflow-probability~=0.23.0 (from tf_agents)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability~=0.23.0->tf_agents) (25.3.0)\n",
            "Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697742 sha256=8dd0169260c4db779dba559af4785ef89fd12168958c843a608403b615dce325\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/19/ce/d2b762b6d61115bf0b4260ca59650ba2d55d49f34f61e095f6\n",
            "Successfully built gym\n",
            "Installing collected packages: typing-extensions, pygame, gym, tensorflow-probability, tf_agents\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.25.0\n",
            "    Uninstalling tensorflow-probability-0.25.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.25.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.11.7 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "optree 0.16.0 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "fastapi 0.115.12 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "sqlalchemy 2.0.41 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "wandb 0.20.1 requires typing-extensions<5,>=4.8, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.86.0 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "google-genai 1.20.0 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "langchain-core 0.3.65 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.23.0 tf_agents-0.19.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym",
                  "pygame"
                ]
              },
              "id": "a3661bd324e24e2b8b6efe1ef54df23d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_agents/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras._tf_keras.keras' has no attribute '__internal__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-1095513609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# TF-Agents specific imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tf_agents'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuite_gym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActionRepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_py_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFPyEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbandits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/agents/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Module importing all agents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbehavioral_cloning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcategorical_dqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/agents/behavioral_cloning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"A Behavioral Cloning agent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavioral_cloning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbehavioral_cloning_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/agents/behavioral_cloning/behavioral_cloning_agent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_converter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# `python/__init__.py` as necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubstrates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/substrates/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"TensorFlow Probability alternative substrates.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_loader\u001b[0m  \u001b[0;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Non-lazy load of packages that register with tensorflow or keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpkg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_maybe_nonlazy_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forces loading the package from its lazy loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbayesopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/experimental/bayesopt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"TensorFlow Probability experimental Bayesopt package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macquisition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcquisitionFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMCMCReducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianProcessExpectedImprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelExpectedImprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStudentTProcessExpectedImprovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/experimental/bayesopt/acquisition/expected_improvement.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstudent_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayesopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macquisition_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpareto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPareto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPixelCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplackett_luce\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPlackettLuce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoisson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoisson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/distributions/pixel_cnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreparameterization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorshape_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweight_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_variational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseReparameterization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_variational_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseVariational\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalMixtureOfOneHotCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributionLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndependentBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/layers/distribution_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_symbolic_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorCoercible\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras' has no attribute '__internal__'"
          ]
        }
      ],
      "source": [
        "# Instalasi TF-Agents dan dependensinya (jika belum)\n",
        "# !pip install -U tf-agents\n",
        "# !pip install -U 'gym[atari, accept-rom-license]'\n",
        "# !apt-get update\n",
        "# !apt-get install -y libglu1-mesa-dev freeglut3-dev mesa-common-dev\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "# TF-Agents specific imports\n",
        "!pip install tf_agents\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments.wrappers import ActionRepeat\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.environments import suite_atari\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.eval.metric_utils import log_metrics\n",
        "from tf_agents.utils import common\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "print(\"\\n--- 1. Lingkungan TF-Agents (Breakout) ---\")\n",
        "max_episode_steps = 27000  # <=> 108k ALE frames (1 step = 4 frames)\n",
        "environment_name = \"BreakoutNoFrameskip-v4\" # Menggunakan noframeskip\n",
        "\n",
        "env_breakout = suite_atari.load(\n",
        "    environment_name,\n",
        "    max_episode_steps=max_episode_steps,\n",
        "    gym_env_wrappers=[AtariPreprocessing], # Praproses gambar\n",
        "    env_wrappers=[lambda env: FrameStack4(env, 4)] # Stack 4 frame terakhir\n",
        ")\n",
        "tf_env = TFPyEnvironment(env_breakout)\n",
        "\n",
        "print(f\"Observation Spec: {tf_env.observation_spec()}\")\n",
        "print(f\"Action Spec: {tf_env.action_spec()}\")\n",
        "print(f\"Action Meanings: {env_breakout.gym.get_action_meanings()}\")\n",
        "\n",
        "current_time_step = tf_env.reset()\n",
        "print(f\"Initial Time Step Observation Shape: {current_time_step.observation.shape}\")\n",
        "\n",
        "# Render contoh observasi (opsional, mungkin perlu X Server)\n",
        "try:\n",
        "    # Observasi adalah tensor TensorFlow, ubah ke numpy untuk imshow\n",
        "    obs_np = current_time_step.observation.numpy()\n",
        "    # Asumsi obs_np memiliki shape (batch_size, height, width, channels) -> (1, 84, 84, 4)\n",
        "    # Untuk visualisasi, ambil frame terakhir atau gabungkan channel\n",
        "    img_display = obs_np[0, :, :, -1] # Ambil frame terakhir (greyscale)\n",
        "    plt.imshow(img_display, cmap='gray')\n",
        "    plt.title(\"Contoh Observasi Breakout (Greyscale, Framestacked)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Tidak dapat me-render contoh observasi: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 2. Membuat Deep Q-Network (QNetwork) ---\")\n",
        "preprocessing_layer_q_net = keras.layers.Lambda(\n",
        "    lambda obs: tf.cast(obs, tf.float32) / 255.  # Normalisasi ke [0, 1]\n",
        ")\n",
        "\n",
        "conv_layer_params=[(32, (8, 8), 4),  # Filters, Kernel Size, Stride\n",
        "                   (64, (4, 4), 2),\n",
        "                   (64, (3, 3), 1)]\n",
        "fc_layer_params=[512] # Neuron di fully connected layer setelah konvolusi\n",
        "\n",
        "q_net = QNetwork(\n",
        "    tf_env.observation_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    preprocessing_layers=preprocessing_layer_q_net,\n",
        "    conv_layer_params=conv_layer_params,\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "q_net.create_variables()\n",
        "print(\"QNetwork berhasil dibuat.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 3. Membuat DQN Agent ---\")\n",
        "train_step = tf.Variable(0)\n",
        "update_period = 4  # Latih model setiap 4 langkah pengumpulan\n",
        "optimizer_tf_agents = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,\n",
        "                                               epsilon=0.00001, centered=True) # Hyperparameter dari paper DQN\n",
        "\n",
        "# Epsilon-greedy decay: dari 1.0 ke 0.01 dalam 1 juta ALE frames\n",
        "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=1.0,  # Epsilon awal\n",
        "    decay_steps=250000 // update_period,  # <=> 1,000,000 ALE frames / 4 frames per step = 250,000 steps\n",
        "    end_learning_rate=0.01)  # Epsilon akhir\n",
        "\n",
        "agent_tf_agents = DqnAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer_tf_agents,\n",
        "    target_update_period=2000,  # Perbarui target model setiap 2000 langkah pengumpulan\n",
        "    td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"), # Huber loss, tanpa reduksi\n",
        "    gamma=0.99,  # Faktor diskon\n",
        "    train_step_counter=train_step,\n",
        "    epsilon_greedy=lambda: epsilon_fn(train_step) # Epsilon decay untuk eksplorasi\n",
        ")\n",
        "agent_tf_agents.initialize()\n",
        "print(\"DQN Agent berhasil dibuat dan diinisialisasi.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 4. Membuat Replay Buffer dan Observer ---\")\n",
        "replay_buffer_tf_agents = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent_tf_agents.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size, # Biasanya 1 untuk single env\n",
        "    max_length=1000000 # 1 juta pengalaman\n",
        ")\n",
        "replay_buffer_observer = replay_buffer_tf_agents.add_batch\n",
        "print(\"Replay Buffer dan Observer berhasil dibuat.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 5. Membuat Metrik Pelatihan ---\")\n",
        "train_metrics = [\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    tf_metrics.AverageReturnMetric(), # Reward rata-rata per episode (tanpa diskon)\n",
        "    tf_metrics.AverageEpisodeLengthMetric(), # Panjang rata-rata episode\n",
        "]\n",
        "print(\"Metrik pelatihan berhasil dibuat.\")\n",
        "\n",
        "print(\"\\n--- 6. Membuat Collect Driver dan Memanaskan Replay Buffer ---\")\n",
        "collect_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent_tf_agents.collect_policy, # Policy pengumpulan dari agent (epsilon-greedy)\n",
        "    observers=[replay_buffer_observer] + train_metrics,\n",
        "    num_steps=update_period # Jumlah langkah pengumpulan per iterasi pelatihan\n",
        ")\n",
        "\n",
        "# Memanaskan replay buffer dengan kebijakan acak\n",
        "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\n",
        "init_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers=[replay_buffer_tf_agents.add_batch], # Hanya tambahkan ke buffer\n",
        "    num_steps=20000 # Kumpulkan 20.000 langkah acak untuk pemanasan\n",
        ")\n",
        "\n",
        "print(\"Memanaskan replay buffer dengan 20.000 langkah acak...\")\n",
        "final_time_step, final_policy_state = init_driver.run()\n",
        "print(f\"Replay buffer memiliki {replay_buffer_tf_agents.num_frames()} frame setelah pemanasan.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 7. Membuat Dataset dari Replay Buffer ---\")\n",
        "dataset = replay_buffer_tf_agents.as_dataset(\n",
        "    sample_batch_size=64, # Ukuran batch sampel untuk pelatihan\n",
        "    num_steps=2, # 2 langkah = 1 transisi penuh (s, a, r, s')\n",
        "    num_parallel_calls=3 # Paralelisasi pemrosesan dataset\n",
        ").prefetch(3) # Prefetch 3 batch untuk efisiensi\n",
        "\n",
        "print(\"Dataset pelatihan berhasil dibuat.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 8. Loop Pelatihan Utama ---\")\n",
        "\n",
        "# Mengubah fungsi menjadi TF Function untuk performa yang lebih baik\n",
        "collect_driver.run = common.function(collect_driver.run)\n",
        "agent_tf_agents.train = common.function(agent_tf_agents.train)\n",
        "\n",
        "def train_agent_tf_agents(n_iterations):\n",
        "    time_step = None\n",
        "    # Dapatkan initial state policy (kosong untuk stateless policy seperti DQN)\n",
        "    policy_state = agent_tf_agents.collect_policy.get_initial_state(tf_env.batch_size)\n",
        "    iterator = iter(dataset)\n",
        "\n",
        "    print(f\"Mulai pelatihan selama {n_iterations} iterasi. Mungkin membutuhkan waktu yang sangat lama...\")\n",
        "    for iteration in range(n_iterations):\n",
        "        # Langkah 1: Kumpulkan pengalaman menggunakan collect driver\n",
        "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "\n",
        "        # Langkah 2: Ambil batch pengalaman dari dataset\n",
        "        try:\n",
        "            trajectories, buffer_info = next(iterator)\n",
        "        except StopIteration:\n",
        "            # Jika dataset habis, buat iterator baru (penting jika dataset tidak berulang)\n",
        "            iterator = iter(dataset)\n",
        "            trajectories, buffer_info = next(iterator)\n",
        "\n",
        "        # Langkah 3: Latih agen menggunakan batch pengalaman\n",
        "        train_loss = agent_tf_agents.train(trajectories)\n",
        "\n",
        "        # Tampilkan progress\n",
        "        print(f\"\\rIterasi {iteration+1}/{n_iterations}, Loss Pelatihan: {train_loss.loss.numpy():.5f}\", end=\"\")\n",
        "\n",
        "        # Log metrik secara berkala\n",
        "        if (iteration + 1) % 1000 == 0: # Log setiap 1000 iterasi\n",
        "            print(\"\\n\") # Pindah baris sebelum log metrics\n",
        "            log_metrics(train_metrics)\n",
        "\n",
        "    print(\"\\nPelatihan selesai.\")\n",
        "    # Cetak metrik akhir\n",
        "    log_metrics(train_metrics)\n",
        "\n",
        "# Jalankan pelatihan (PERHATIAN: ini akan sangat lama dan membutuhkan GPU)\n",
        "# n_training_iterations = 10000000 # Jumlah iterasi dari buku\n",
        "# Untuk demo singkat, Anda bisa mencoba jumlah iterasi yang lebih kecil, misal 1000 atau 10000\n",
        "n_training_iterations_demo = 1000 # Contoh demo singkat\n",
        "train_agent_tf_agents(n_training_iterations_demo)\n",
        "\n",
        "# Jangan lupa untuk menutup lingkungan setelah selesai\n",
        "tf_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdD2bA7cBrlm"
      },
      "source": [
        "**Diskusi Hasil Pelatihan TF-Agents:**\n",
        "Melatih agen di lingkungan Atari seperti Breakout adalah tugas yang sangat kompleks dan membutuhkan sumber daya komputasi yang besar serta waktu yang lama (bisa berjam-jam atau bahkan berhari-hari pada GPU). *Learning curve* akan menunjukkan peningkatan bertahap dalam *reward* rata-rata seiring agen belajar *policy* yang lebih baik.\n",
        "\n",
        "TF-Agents menyederhanakan banyak kompleksitas ini dengan menyediakan komponen-komponen yang telah teruji dan teroptimasi, seperti *QNetwork*, *DqnAgent*, *TFUniformReplayBuffer*, dan *DynamicStepDriver*. Menggunakan `tf.function` pada fungsi-fungsi utama juga sangat krusial untuk performa yang optimal karena mengubahnya menjadi grafik komputasi TensorFlow yang dapat dieksekusi secara efisien.\n",
        "\n",
        "Meskipun tantangan tetap ada dalam menyetel *hyperparameter* dan mengatasi masalah *stochasticity* dalam RL, TF-Agents memberikan fondasi yang kokoh untuk mengembangkan sistem RL yang kuat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTtuzqc1Brlm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klP0hPiGBrln"
      },
      "source": [
        "### **15. Ikhtisar Beberapa Algoritma RL Populer Lainnya**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1TdqgRgBrln"
      },
      "source": [
        "Dunia Reinforcement Learning terus berkembang dengan pesat, dan banyak algoritma baru terus bermunculan. Berikut adalah beberapa algoritma populer lainnya yang penting untuk diketahui:\n",
        "\n",
        "#### **a. *Actor-Critic Algorithms***\n",
        "Keluarga algoritma RL yang menggabungkan ide **Policy Gradients** dengan **Deep Q-Networks**. Agen *Actor-Critic* memiliki dua jaringan saraf:\n",
        "* **Actor (Jaringan Policy)**: Mempelajari *policy* untuk memilih aksi. Ia belajar lebih cepat daripada PG biasa karena mengandalkan estimasi nilai aksi dari *critic*.\n",
        "* **Critic (DQN)**: Mempelajari untuk memperkirakan nilai *state* atau nilai aksi (Q-Values). Ini seperti seorang atlet (*actor*) yang belajar dengan bantuan seorang pelatih (*critic*).\n",
        "\n",
        "#### **b. *Asynchronous Advantage Actor-Critic (A3C)***\n",
        "Varian *Actor-Critic* yang penting yang diperkenalkan oleh peneliti DeepMind pada tahun 2016. Fitur utamanya adalah:\n",
        "* **Pembelajaran Paralel**: Banyak agen belajar secara paralel, menjelajahi salinan lingkungan yang berbeda.\n",
        "* **Pembaruan Asinkron**: Setiap agen secara berkala (tetapi asinkron) mendorong pembaruan bobot ke jaringan master, lalu menarik bobot terbaru dari jaringan master.\n",
        "* **Estimasi Advantage**: Critic memperkirakan *advantage* dari setiap aksi, yang menstabilkan pelatihan.\n",
        "\n",
        "#### **c. *Advantage Actor-Critic (A2C)***\n",
        "Varian dari A3C yang menghilangkan asinkronisitas. Semua pembaruan model sinkron, memungkinkan pembaruan *gradient* dilakukan dalam *batch* yang lebih besar, yang lebih efisien memanfaatkan kekuatan GPU.\n",
        "\n",
        "#### **d. *Soft Actor-Critic (SAC)***\n",
        "Varian *Actor-Critic* yang diusulkan pada tahun 2018. SAC belajar tidak hanya untuk memaksimalkan *reward*, tetapi juga untuk memaksimalkan *entropy* dari aksinya. Ini berarti agen mencoba untuk tidak dapat diprediksi sebaik mungkin sambil tetap mendapatkan *reward* sebanyak mungkin. Ini mendorong agen untuk mengeksplorasi lingkungan, mempercepat pelatihan, dan membuatnya cenderung tidak mengulang aksi yang sama ketika estimasi DQN tidak sempurna.\n",
        "\n",
        "#### **e. *Proximal Policy Optimization (PPO)***\n",
        "Algoritma yang didasarkan pada A2C yang membatasi fungsi *loss* untuk menghindari pembaruan bobot yang terlalu besar (yang sering menyebabkan ketidakstabilan pelatihan). PPO adalah penyederhanaan dari algoritma sebelumnya, *Trust Region Policy Optimization* (TRPO). OpenAI Five, AI yang mengalahkan juara dunia Dota 2, didasarkan pada algoritma PPO.\n",
        "\n",
        "#### **f. *Curiosity-Based Exploration***\n",
        "Pendekatan untuk mengatasi masalah *reward sparsity*. Agen tidak hanya mengandalkan *reward* eksternal, tetapi juga didorong oleh \"rasa ingin tahu\" internal untuk menjelajahi lingkungan. Agen terus-menerus mencoba memprediksi hasil dari aksinya, dan mencari situasi di mana hasil tersebut tidak sesuai dengan prediksinya.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}