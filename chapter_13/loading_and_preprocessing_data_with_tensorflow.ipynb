{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e96f1d2e"
      },
      "source": [
        "# Bab 13: Memuat dan Melakukan Pra-pemrosesan Data dengan TensorFlow\n",
        "\n",
        "Bab ini akan membahas secara mendalam bagaimana cara memuat (loading) dan melakukan pra-pemrosesan (preprocessing) data secara efisien menggunakan TensorFlow, khususnya dengan fokus pada `tf.data` API dan format TFRecord. Kita juga akan melihat bagaimana mengintegrasikan langkah-langkah pra-pemrosesan ini langsung ke dalam model Keras kita."
      ],
      "id": "e96f1d2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3e94a5c"
      },
      "source": [
        "## 1. Pendahuluan\n",
        "\n",
        "Sistem Machine Learning dan Deep Learning seringkali dilatih dengan dataset yang sangat besar, yang mungkin tidak muat dalam memori RAM. Memuat dan melakukan pra-pemrosesan data secara efisien menjadi kunci performa model. TensorFlow menyediakan `tf.data` API untuk mengatasi tantangan ini. Selain itu, kita akan menjelajahi format TFRecord yang merupakan format biner efisien untuk menyimpan data.\n",
        "\n",
        "### Mengapa Pra-pemrosesan Efisien Penting?\n",
        "\n",
        "* **Mengatasi Batasan Memori:** Dataset besar seringkali tidak bisa dimuat sepenuhnya ke dalam RAM. `tf.data` memungkinkan data di-stream dari disk.\n",
        "* **Performa Pelatihan:** Pra-pemrosesan yang lambat dapat menjadi *bottleneck* pelatihan, membuat GPU/TPU menganggur. Pipeline data yang efisien memastikan data selalu siap saat dibutuhkan.\n",
        "* **Konsistensi Pra-pemrosesan:** Mengintegrasikan pra-pemrosesan ke dalam model atau pipeline memastikan konsistensi antara pelatihan dan inferensi."
      ],
      "id": "b3e94a5c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1e70d4d"
      },
      "source": [
        "## 2. The Data API (`tf.data`)\n",
        "\n",
        "`tf.data` API berpusat pada konsep `tf.data.Dataset`, yang merepresentasikan urutan item data. Dataset ini dapat membaca data secara bertahap dari disk dan menerapkan berbagai transformasi."
      ],
      "id": "d1e70d4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6e70d4d"
      },
      "source": [
        "### 2.1. Membuat Dataset Sederhana\n",
        "\n",
        "Untuk memulai, mari kita buat dataset sederhana di dalam RAM menggunakan `tf.data.Dataset.from_tensor_slices()`. Ini akan menghasilkan dataset di mana setiap elemen adalah irisan dari tensor input."
      ],
      "id": "d6e70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f5f70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac249da-7235-47a0-92fd-4c8a79e0853b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
            "Items dalam dataset:\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Membuat tensor sederhana\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "\n",
        "print(dataset) # Output: <TensorSliceDataset shapes: (), types: tf.int32>\n",
        "\n",
        "# Melakukan iterasi pada dataset\n",
        "print(\"Items dalam dataset:\")\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "id": "f5f70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1f70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "`tf.data.Dataset.from_tensor_slices(X)` membuat sebuah dataset di mana setiap elemennya adalah \"irisan\" (slice) dari dimensi pertama `X`. Dalam kasus ini, karena `X` adalah tensor 1D dari 0 sampai 9, dataset akan berisi 10 elemen skalar, yaitu 0, 1, ..., 9."
      ],
      "id": "c1f70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1g70d4d"
      },
      "source": [
        "### 2.2. Chaining Transformations (Rantai Transformasi)\n",
        "\n",
        "Setelah memiliki dataset, kita dapat menerapkan berbagai transformasi padanya dengan memanggil metode transformasi dataset. Setiap metode ini mengembalikan dataset baru, sehingga Anda dapat merangkai transformasi."
      ],
      "id": "d1g70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f5g70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf855be-574f-4bd6-e4d8-67bb7bdbd482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset setelah filter individual dan batch(7):\n",
            "tf.Tensor([0 2 4 6 8 0 2], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 6 8 0 2 4 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8], shape=(1,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Versi alternatif: filter elemen individual sebelum di-batch\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "dataset = dataset.repeat(3)\n",
        "dataset = dataset.map(lambda x: x * 2)\n",
        "\n",
        "# Memfilter elemen individual yang kurang dari 10 (sebelum batching)\n",
        "dataset = dataset.filter(lambda x: x < 10) # x di sini adalah skalar\n",
        "\n",
        "# Kemudian baru batch\n",
        "dataset = dataset.batch(7)\n",
        "\n",
        "print(\"\\nDataset setelah filter individual dan batch(7):\")\n",
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "id": "f5g70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1g70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `repeat(N)`: Mengulang dataset `N` kali. Jika `N` tidak ditentukan, dataset akan diulang selamanya. Ini tidak menyalin data ke memori, melainkan hanya mengubah logika iterasi.\n",
        "* `batch(batch_size)`: Mengelompokkan elemen-elemen dataset menjadi *mini-batch* dengan ukuran `batch_size`. *Batch* terakhir mungkin lebih kecil jika jumlah total elemen bukan kelipatan dari `batch_size`. Anda bisa menggunakan `drop_remainder=True` untuk memastikan semua *batch* memiliki ukuran yang sama.\n",
        "* `map(function)`: Menerapkan fungsi transformasi pada setiap elemen dataset. Fungsi ini harus berupa fungsi TensorFlow yang dapat dikonversi ke `tf.function` (lihat Bab 12). Anda dapat menggunakan `num_parallel_calls` untuk memparalelkan eksekusi fungsi ini di beberapa *thread*.\n",
        "* `filter(predicate)`: Menyaring elemen-elemen dataset berdasarkan fungsi predikat. Hanya elemen yang mengembalikan `True` yang akan dipertahankan.\n",
        "* `take(N)`: Hanya mengambil `N` elemen pertama dari dataset. Berguna untuk inspeksi atau debugging."
      ],
      "id": "c1g70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1h70d4d"
      },
      "source": [
        "### 2.3. Mengacak Data (Shuffling)\n",
        "\n",
        "Untuk pelatihan dengan Gradient Descent yang optimal, instance dalam *training set* harus independen dan terdistribusi secara identik (IID). Mengacak (shuffling) data adalah cara sederhana untuk memastikan ini."
      ],
      "id": "d1h70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f5h70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7032671c-0215-46ff-e7fe-9df4c84371b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset setelah shuffle dan batch (dengan random seed):\n",
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Membuat dataset dari 0 sampai 9, diulang 3 kali\n",
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "\n",
        "# Mengacak dataset dengan buffer_size 5 dan seed 42\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "\n",
        "print(\"\\nDataset setelah shuffle dan batch (dengan random seed):\")\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "id": "f5h70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1h70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `shuffle(buffer_size)`: Membuat *buffer* internal berukuran `buffer_size`. Saat dataset diminta elemen, ia akan mengambil elemen secara acak dari *buffer* dan menggantinya dengan elemen baru dari dataset sumber. Semakin besar `buffer_size`, semakin efektif pengacakannya.\n",
        "* Untuk dataset yang tidak muat dalam memori, teknik pengacakan yang lebih canggih melibatkan pembagian data ke dalam beberapa file dan membaca file-file tersebut secara acak, kemudian melakukan *interleaving* (membaca baris secara bergantian) dari file-file tersebut."
      ],
      "id": "c1h70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1i70d4d"
      },
      "source": [
        "### 2.4. Interleaving Lines from Multiple Files (Interleaving Baris dari Banyak File)\n",
        "\n",
        "Ketika data terlalu besar untuk satu file, atau untuk meningkatkan efisiensi pengacakan, Anda dapat membagi data ke dalam beberapa file dan membaca baris-barisnya secara bersamaan."
      ],
      "id": "d1i70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f5i70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5503a8fb-678b-484e-d51e-5b46a8be87f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset setelah interleaving (mengambil 5 baris pertama):\n",
            "b'value_0,value_0'\n",
            "b'value_0,value_0'\n",
            "b'value_1,value_2'\n",
            "b'value_1,value_2'\n",
            "b'value_2,value_4'\n"
          ]
        }
      ],
      "source": [
        "# Contoh: Jika Anda memiliki file CSV, unduh California Housing Dataset\n",
        "# dan bagi menjadi beberapa file CSV kecil.\n",
        "# Ini adalah bagian yang akan Anda jalankan secara manual atau dengan skrip terpisah\n",
        "# karena melibatkan operasi file sistem dan pengunduhan data.\n",
        "\n",
        "# Asumsikan Anda memiliki file-file seperti 'my_train_00.csv', 'my_train_01.csv', dll.\n",
        "# Untuk demonstrasi, kita akan membuat dummy file paths.\n",
        "train_filepaths = [f\"dummy_data/my_train_{i:02d}.csv\" for i in range(5)]\n",
        "# Buat beberapa dummy file agar kode ini bisa dijalankan\n",
        "os.makedirs(\"dummy_data\", exist_ok=True)\n",
        "for filepath in train_filepaths:\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(\"header\\n\") # Tulis header dummy\n",
        "        for i in range(10):\n",
        "            f.write(f\"value_{i},value_{i*2}\\n\")\n",
        "\n",
        "# Membuat dataset dari daftar jalur file\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
        "\n",
        "# Menginterleave baris dari 2 file sekaligus (melewatkan header)\n",
        "n_readers = 2\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers\n",
        ")\n",
        "\n",
        "print(\"\\nDataset setelah interleaving (mengambil 5 baris pertama):\")\n",
        "for line in dataset.take(5):\n",
        "    print(line.numpy())\n",
        "\n",
        "# Membersihkan dummy files\n",
        "for filepath in train_filepaths:\n",
        "    os.remove(filepath)\n",
        "os.rmdir(\"dummy_data\")"
      ],
      "id": "f5i70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1i70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.data.Dataset.list_files(filepaths)`: Membuat dataset yang berisi jalur (path) ke file-file. Secara *default*, ini akan mengacak urutan jalur file.\n",
        "* `interleave(map_func, cycle_length, num_parallel_calls)`: Transformasi yang memungkinkan Anda membaca data dari beberapa sumber secara bersamaan dan menginterleave (mencampur) elemen-elemennya.\n",
        "    * `map_func`: Sebuah fungsi yang menerima `filepath` dan mengembalikan `Dataset` baru (misalnya, `tf.data.TextLineDataset`).\n",
        "    * `cycle_length`: Berapa banyak dataset sumber yang akan diinterleave secara bersamaan.\n",
        "    * `num_parallel_calls`: Berapa banyak *thread* CPU yang akan digunakan untuk membaca file secara paralel. `tf.data.experimental.AUTOTUNE` dapat digunakan untuk otomatisasi."
      ],
      "id": "c1i70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1j70d4d"
      },
      "source": [
        "### 2.5. Preprocessing the Data (Pra-pemrosesan Data)\n",
        "\n",
        "Data mentah, seperti string CSV, perlu diuraikan (parsed) dan dinormalisasi agar dapat digunakan oleh model Machine Learning."
      ],
      "id": "d1j70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f5j70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb00501-e6b3-4335-fa94-82cc2d407203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output pra-pemrosesan:\n",
            "Fitur: tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1.], shape=(8,), dtype=float32)\n",
            "Label: tf.Tensor([99.], shape=(1,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Untuk demonstrasi, kita akan membuat fungsi preprocess yang menguraikan string byte CSV\n",
        "# dan menskalakan fitur. Untuk dataset nyata, Anda perlu menghitung mean dan std\n",
        "# dari training set terlebih dahulu.\n",
        "\n",
        "# Dummy mean dan std untuk 8 fitur\n",
        "X_mean = np.array([10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0], dtype=np.float32)\n",
        "X_std = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], dtype=np.float32)\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess_csv_line(line):\n",
        "    # Asumsikan format CSV: feature1,feature2,...,feature8,target\n",
        "    # Misal record_defaults untuk 8 fitur float dan 1 target float\n",
        "    # Gunakan tf.constant([]) untuk target agar error jika ada nilai hilang\n",
        "    # Penjelasan nilai default: Jika kolom tidak ada, gunakan 0.0. Jika kolom target tidak ada, itu akan menjadi kesalahan.\n",
        "    defs = [0.0] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "\n",
        "    # Skalakan fitur\n",
        "    x_scaled = (x - X_mean) / X_std\n",
        "    return x_scaled, y\n",
        "\n",
        "# Contoh penggunaan\n",
        "dummy_csv_line = b\"11.0,22.0,33.0,44.0,55.0,66.0,77.0,88.0,99.0\"\n",
        "x_processed, y_processed = preprocess_csv_line(dummy_csv_line)\n",
        "print(\"\\nOutput pra-pemrosesan:\")\n",
        "print(\"Fitur:\", x_processed)\n",
        "print(\"Label:\", y_processed)"
      ],
      "id": "f5j70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1j70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.io.decode_csv(line, record_defaults)`: Menguraikan satu baris string CSV menjadi daftar tensor. `record_defaults` digunakan untuk menentukan tipe data setiap kolom dan nilai *default* jika ada nilai yang hilang. Jika `tf.constant([])` digunakan sebagai *default*, `decode_csv` akan menimbulkan *error* jika ada nilai yang hilang di kolom tersebut.\n",
        "* `tf.stack(tensors)`: Menggabungkan daftar tensor menjadi satu tensor baru di sepanjang sumbu baru."
      ],
      "id": "c1j70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1k70d4d"
      },
      "source": [
        "### 2.6. Putting Everything Together (Menggabungkan Semuanya)\n",
        "\n",
        "Mari kita satukan semua langkah pra-pemrosesan ke dalam satu fungsi pembantu yang akan membuat dan mengembalikan dataset yang efisien."
      ],
      "id": "d1k70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f5k70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba128ed-7e29-48eb-c642-5ff519b791c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output dari fungsi create_housing_dataset (mengambil 2 batch):\n",
            "Batch X (processed): tf.Tensor(\n",
            "[[ -9. -10.]\n",
            " [ -8.  -9.]\n",
            " [ -6.  -7.]\n",
            " [ -5.  -6.]\n",
            " [ -5.  -6.]\n",
            " [ -8.  -9.]\n",
            " [ -9. -10.]\n",
            " [ -7.  -8.]\n",
            " [ -9. -10.]\n",
            " [ -6.  -7.]\n",
            " [ -6.  -7.]\n",
            " [ -8.  -9.]\n",
            " [ -7.  -8.]\n",
            " [ -5.  -6.]\n",
            " [ -7.  -8.]], shape=(15, 2), dtype=float32)\n",
            "Batch Y (target): tf.Tensor(\n",
            "[[ 0.]\n",
            " [ 3.]\n",
            " [ 9.]\n",
            " [12.]\n",
            " [12.]\n",
            " [ 3.]\n",
            " [ 0.]\n",
            " [ 6.]\n",
            " [ 0.]\n",
            " [ 9.]\n",
            " [ 9.]\n",
            " [ 3.]\n",
            " [ 6.]\n",
            " [12.]\n",
            " [ 6.]], shape=(15, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def create_housing_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                           n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                           n_parse_threads=5, batch_size=32,\n",
        "                           n_inputs=8): # n_inputs sekarang digunakan untuk menentukan ukuran mean/std\n",
        "\n",
        "    # Dummy mean dan std. Buat ini secara dinamis berdasarkan n_inputs\n",
        "    # Misalnya, kita bisa membuat array dummy dengan ukuran yang benar\n",
        "    # atau jika Anda memiliki nilai mean/std sebenarnya, potong/sesuaikan ukurannya.\n",
        "    dummy_mean_values = np.linspace(10.0, 80.0, 8, dtype=np.float32) # Contoh untuk 8 fitur\n",
        "    dummy_std_values = np.linspace(1.0, 8.0, 8, dtype=np.float32) # Contoh untuk 8 fitur\n",
        "\n",
        "    # Gunakan slicing untuk mendapatkan jumlah fitur yang sesuai dengan n_inputs\n",
        "    X_mean = tf.constant(dummy_mean_values[:n_inputs])\n",
        "    X_std = tf.constant(dummy_std_values[:n_inputs])\n",
        "\n",
        "    # Fungsi pra-pemrosesan baris CSV lokal untuk fungsi ini\n",
        "    def preprocess_local(line):\n",
        "        defs = [0.0] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "        fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "\n",
        "        x = tf.stack(fields[:-1])\n",
        "        y = tf.stack(fields[-1:])\n",
        "\n",
        "        # Operasi sekarang akan dilakukan sepenuhnya di TensorFlow dengan dimensi yang sesuai\n",
        "        x_scaled = (x - X_mean) / X_std\n",
        "        return x_scaled, y\n",
        "\n",
        "    # 1. Daftar file\n",
        "    filepath_dataset = tf.data.Dataset.list_files(filepaths, seed=42)\n",
        "\n",
        "    # 2. Interleave (baca baris dari banyak file secara paralel)\n",
        "    dataset = filepath_dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # Skip header\n",
        "        cycle_length=n_readers, num_parallel_calls=n_read_threads\n",
        "    )\n",
        "\n",
        "    # 3. Pra-pemrosesan setiap baris (menguraikan dan menskalakan)\n",
        "    dataset = dataset.map(preprocess_local, num_parallel_calls=n_parse_threads)\n",
        "\n",
        "    # 4. Acak dan Ulangi\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "\n",
        "    # 5. Batch dan Prefetch\n",
        "    return dataset.batch(batch_size).prefetch(1)\n",
        "\n",
        "# --- Bagian untuk mendemonstrasikan perbaikan ---\n",
        "\n",
        "# Bersihkan dummy files dari percobaan sebelumnya jika ada\n",
        "if os.path.exists(\"dummy_data\"):\n",
        "    for file_name in os.listdir(\"dummy_data\"):\n",
        "        os.remove(os.path.join(\"dummy_data\", file_name))\n",
        "    os.rmdir(\"dummy_data\")\n",
        "\n",
        "# Dummy file paths untuk demonstrasi fungsi gabungan\n",
        "dummy_filepaths = [f\"dummy_data/file_{i}.csv\" for i in range(3)]\n",
        "os.makedirs(\"dummy_data\", exist_ok=True)\n",
        "for fp in dummy_filepaths:\n",
        "    with open(fp, \"w\") as f:\n",
        "        # Menulis 2 fitur dan 1 target, agar sesuai dengan n_inputs=2\n",
        "        f.write(\"feature1,feature2,target\\n\")\n",
        "        for j in range(5):\n",
        "            f.write(f\"{j+1},{j*2},{j*3}\\n\")\n",
        "\n",
        "# Membuat dataset menggunakan fungsi gabungan dengan n_inputs=2\n",
        "dummy_full_dataset = create_housing_dataset(dummy_filepaths, repeat=1, n_inputs=2)\n",
        "\n",
        "print(\"\\nOutput dari fungsi create_housing_dataset (mengambil 2 batch):\")\n",
        "for batch_x, batch_y in dummy_full_dataset.take(2):\n",
        "    print(\"Batch X (processed):\", batch_x)\n",
        "    print(\"Batch Y (target):\", batch_y)\n",
        "\n",
        "# Membersihkan dummy files lagi\n",
        "for fp in dummy_filepaths:\n",
        "    os.remove(fp)\n",
        "os.rmdir(\"dummy_data\")"
      ],
      "id": "f5k70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1l70d4d"
      },
      "source": [
        "### 2.7. Prefetching\n",
        "\n",
        "`prefetch(1)` adalah optimasi performa penting. Ia membuat dataset selalu \"satu *batch* di depan\". Artinya, saat algoritma pelatihan memproses satu *batch*, dataset sudah bekerja secara paralel untuk menyiapkan *batch* berikutnya (misalnya, membaca data dari disk dan memprosesnya). Ini secara dramatis dapat meningkatkan kinerja dengan memastikan GPU/TPU tidak menganggur menunggu data."
      ],
      "id": "d1l70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f5l70d4e"
      },
      "outputs": [],
      "source": [
        "# Konsep Prefetching\n",
        "# CPU membaca dan memproses data sementara GPU melatih model.\n",
        "# Ini menciptakan pipeline yang mulus.\n",
        "# Dataset.prefetch(buffer_size)\n",
        "\n",
        "# Jika dataset muat dalam memori, gunakan cache()\n",
        "# dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(1)\n",
        "# cache() harus dipanggil setelah transformasi yang intensif komputasi,\n",
        "# tetapi sebelum shuffling dan repeating."
      ],
      "id": "f5l70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1m70d4d"
      },
      "source": [
        "## 3. The TFRecord Format\n",
        "\n",
        "Format TFRecord adalah format yang disarankan TensorFlow untuk menyimpan sejumlah besar data dan membacanya secara efisien. Ini adalah format biner yang sederhana dan fleksibel."
      ],
      "id": "d1m70d4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1n70d4d"
      },
      "source": [
        "### 3.1. Membuat File TFRecord"
      ],
      "id": "d1n70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f5n70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d01e70-ec8f-4f29-bde6-0c788c1f873c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Isi file TFRecord:\n",
            "tf.Tensor(b'Ini adalah record pertama', shape=(), dtype=string)\n",
            "tf.Tensor(b'Dan ini adalah record kedua', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Membuat file TFRecord sederhana\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"Ini adalah record pertama\")\n",
        "    f.write(b\"Dan ini adalah record kedua\")\n",
        "\n",
        "# Membaca file TFRecord\n",
        "filepaths = [\"my_data.tfrecord\"]\n",
        "tfrecord_dataset = tf.data.TFRecordDataset(filepaths)\n",
        "\n",
        "print(\"\\nIsi file TFRecord:\")\n",
        "for item in tfrecord_dataset:\n",
        "    print(item)"
      ],
      "id": "f5n70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1n70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.io.TFRecordWriter`: Digunakan untuk menulis data biner ke file TFRecord. Anda bisa menulis *byte strings* apa pun.\n",
        "* `tf.data.TFRecordDataset`: Digunakan untuk membaca data dari satu atau lebih file TFRecord. Secara *default*, ini membaca file satu per satu, tetapi Anda dapat menginterleave-nya untuk paralelisme."
      ],
      "id": "c1n70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1o70d4d"
      },
      "source": [
        "### 3.2. Compressed TFRecord Files (File TFRecord Terkompresi)\n",
        "\n",
        "File TFRecord dapat dikompresi untuk menghemat ruang disk, terutama saat perlu dimuat melalui jaringan."
      ],
      "id": "d1o70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f5o70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda5f604-6f9a-4a15-e2f5-8e1ef2813a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Isi file TFRecord terkompresi:\n",
            "tf.Tensor(b'Record terkompresi pertama', shape=(), dtype=string)\n",
            "tf.Tensor(b'Record terkompresi kedua', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Membuat file TFRecord terkompresi (GZIP)\n",
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "with tf.io.TFRecordWriter(\"my_compressed_data.tfrecord\", options) as f:\n",
        "    f.write(b\"Record terkompresi pertama\")\n",
        "    f.write(b\"Record terkompresi kedua\")\n",
        "\n",
        "# Membaca file TFRecord terkompresi\n",
        "compressed_tfrecord_dataset = tf.data.TFRecordDataset(\n",
        "    [\"my_compressed_data.tfrecord\"],\n",
        "    compression_type=\"GZIP\"\n",
        ")\n",
        "\n",
        "print(\"\\nIsi file TFRecord terkompresi:\")\n",
        "for item in compressed_tfrecord_dataset:\n",
        "    print(item)\n",
        "\n",
        "# Membersihkan file TFRecord\n",
        "os.remove(\"my_data.tfrecord\")\n",
        "os.remove(\"my_compressed_data.tfrecord\")"
      ],
      "id": "f5o70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1o70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* Anda dapat menentukan `compression_type` (misalnya, \"GZIP\" atau \"ZLIB\") saat menulis dan membaca file TFRecord."
      ],
      "id": "c1o70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1p70d4d"
      },
      "source": [
        "### 3.3. Protocol Buffers (Protobufs)\n",
        "\n",
        "Meskipun setiap record TFRecord dapat menggunakan format biner apa pun, biasanya mereka berisi *serialized protocol buffers* (protobufs). Protobufs adalah format biner yang portabel, dapat diperluas, dan efisien."
      ],
      "id": "d1p70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f5p70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed193d26-8f08-466f-9db2-080221c3aaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Protobuf berhasil dibuat dan disimpan.\n"
          ]
        }
      ],
      "source": [
        "# Contoh dasar penggunaan Protobuf (membutuhkan instalasi protobuf-compiler dan kompilasi .proto file)\n",
        "# Biasanya, TensorFlow menyediakan definisi protobuf yang sudah dikompilasi (seperti tf.train.Example).\n",
        "\n",
        "# Untuk demonstrasi, kita akan langsung menggunakan tf.train.Example\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "# Membuat tf.train.Example\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "# Menulis Example ke file TFRecord\n",
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString())\n",
        "\n",
        "print(\"\\nExample Protobuf berhasil dibuat dan disimpan.\")"
      ],
      "id": "f5p70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1p70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.train.Example`: Protobuf standar TensorFlow untuk merepresentasikan satu instance data. Ini berisi kamus fitur, di mana setiap fitur dapat berupa daftar *byte strings*, *floats*, atau *integers*.\n",
        "* `SerializeToString()`: Metode pada objek protobuf untuk mengkonversinya menjadi *byte string*."
      ],
      "id": "c1p70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1q70d4d"
      },
      "source": [
        "### 3.4. Loading and Parsing Examples (Memuat dan Menguraikan Example)"
      ],
      "id": "d1q70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f5q70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab992232-09db-4947-efc3-65d4e4902e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Menguraikan Example Protobuf:\n",
            "Parsed Example: {'emails': SparseTensor(indices=tf.Tensor(\n",
            "[[0]\n",
            " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
            "Emails (sparse): SparseTensor(indices=tf.Tensor(\n",
            "[[0]\n",
            " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64))\n",
            "Emails (dense): tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Mendefinisikan deskripsi fitur untuk parsing\n",
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string), # emails adalah fitur panjang variabel\n",
        "}\n",
        "\n",
        "# Membaca dan menguraikan file TFRecord\n",
        "parsed_dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"])\n",
        "\n",
        "print(\"\\nMenguraikan Example Protobuf:\")\n",
        "for serialized_example in parsed_dataset:\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "    print(\"Parsed Example:\", parsed_example)\n",
        "    print(\"Emails (sparse):\", parsed_example[\"emails\"])\n",
        "    # Mengkonversi sparse tensor ke dense tensor\n",
        "    print(\"Emails (dense):\", tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\"))\n",
        "\n",
        "# Membersihkan file TFRecord\n",
        "os.remove(\"my_contacts.tfrecord\")"
      ],
      "id": "f5q70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1q70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.io.FixedLenFeature`: Digunakan untuk fitur dengan panjang tetap (skalar atau tensor dengan bentuk tetap).\n",
        "* `tf.io.VarLenFeature`: Digunakan untuk fitur dengan panjang variabel, menguraikannya sebagai `tf.SparseTensor`.\n",
        "* `tf.sparse.to_dense()`: Mengkonversi `tf.SparseTensor` menjadi `tf.Tensor` biasa, dengan mengisi nilai nol pada elemen yang tidak ada."
      ],
      "id": "c1q70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1r70d4d"
      },
      "source": [
        "### 3.5. Handling Lists of Lists Using the SequenceExample Protobuf\n",
        "\n",
        "Untuk kasus yang lebih kompleks dengan daftar-daftar (misalnya, dokumen teks dengan daftar kalimat, dan setiap kalimat adalah daftar kata), `tf.train.SequenceExample` protobuf lebih cocok."
      ],
      "id": "d1r70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f5r70d4e"
      },
      "outputs": [],
      "source": [
        "# Definisi SequenceExample protobuf (disini disederhanakan, Anda tidak perlu mengkompilasinya)\n",
        "# sequence_example = tf.train.SequenceExample(...)\n",
        "# Untuk demonstrasi, kita akan melewati pembuatan SequenceExample dan langsung membahas parsingnya.\n",
        "\n",
        "# Deskripsi fitur konteks (metadata dokumen)\n",
        "context_feature_descriptions = {\n",
        "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"title\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "}\n",
        "\n",
        "# Deskripsi fitur sequence (daftar kalimat, daftar kata)\n",
        "sequence_feature_descriptions = {\n",
        "    \"sentences\": tf.io.VarLenFeature(tf.int64), # Setiap \"sentence\" adalah daftar word_id\n",
        "    \"words_in_sentences\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True), # Jika setiap elemen sequence adalah skalar\n",
        "}\n",
        "\n",
        "# Asumsikan Anda memiliki serialized_sequence_example\n",
        "# serialized_sequence_example = b\"...\" # Ini harus dari SequenceExample yang sebenarnya\n",
        "\n",
        "# # Ini hanya contoh sintaks parsing, tidak akan dijalankan tanpa serialized_sequence_example yang valid\n",
        "# # parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
        "# #     serialized_sequence_example, context_feature_descriptions,\n",
        "# #     sequence_feature_descriptions)\n",
        "# # parsed_sentences = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"sentences\"])"
      ],
      "id": "f5r70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1r70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.train.SequenceExample`: Dirancang untuk data urutan, dengan dua bagian utama: `context` (untuk fitur global/metadata) dan `feature_lists` (untuk fitur urutan yang bervariasi panjangnya).\n",
        "* `tf.io.parse_single_sequence_example()`: Menguraikan satu `SequenceExample`. Mengembalikan tuple yang berisi kamus fitur konteks dan kamus daftar fitur.\n",
        "* `tf.io.FixedLenSequenceFeature`: Digunakan dalam `sequence_feature_descriptions` untuk fitur urutan di mana setiap elemen urutan memiliki panjang tetap, tetapi panjang total urutannya dapat bervariasi.\n",
        "* `tf.RaggedTensor`: Penting untuk merepresentasikan data dengan dimensi \"compang-camping\" (ragged), di mana baris-baris memiliki panjang yang berbeda."
      ],
      "id": "c1r70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1s70d4d"
      },
      "source": [
        "## 4. Preprocessing the Input Features (Pra-pemrosesan Fitur Input)\n",
        "\n",
        "Pra-pemrosesan fitur dapat dilakukan *offline* (sebelum pelatihan) atau *on-the-fly* (saat pelatihan atau inferensi). Mengintegrasikan lapisan pra-pemrosesan langsung ke dalam model Keras adalah praktik yang baik untuk konsistensi."
      ],
      "id": "d1s70d4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1t70d4d"
      },
      "source": [
        "### 4.1. Custom Standardization Layer (Lapisan Standardisasi Kustom)"
      ],
      "id": "d1t70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f5t70d4e"
      },
      "outputs": [],
      "source": [
        "class Standardization(tf.keras.layers.Layer):\n",
        "    def adapt(self, data_sample):\n",
        "        # Hitung mean dan std dari sampel data\n",
        "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Lakukan standardisasi\n",
        "        return (inputs - self.means_) / (self.stds_ + tf.keras.backend.epsilon())\n",
        "\n",
        "# Contoh penggunaan\n",
        "# Asumsikan X_train_sample adalah sampel data training Anda\n",
        "X_train_sample = np.random.rand(100, 8).astype(np.float32)\n",
        "\n",
        "std_layer = Standardization()\n",
        "std_layer.adapt(X_train_sample)\n",
        "\n",
        "# Menggunakan lapisan dalam model Keras\n",
        "model = tf.keras.Sequential([\n",
        "    std_layer,\n",
        "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# # Contoh compile dan fit model (membutuhkan X_train, y_train yang nyata)\n",
        "# # model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "# # model.fit(X_train, y_train, epochs=1)"
      ],
      "id": "f5t70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1t70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* Lapisan kustom ini mewarisi dari `tf.keras.layers.Layer`.\n",
        "* `adapt(data_sample)`: Metode ini dirancang untuk menghitung statistik yang diperlukan dari sampel data (misalnya, *mean* dan *standard deviation* untuk standardisasi). Ini harus dipanggil sebelum lapisan digunakan dalam pelatihan.\n",
        "* `call(inputs)`: Metode ini mendefinisikan operasi *forward pass* lapisan."
      ],
      "id": "c1t70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1u70d4d"
      },
      "source": [
        "### 4.2. Encoding Categorical Features Using One-Hot Vectors (Encoding Fitur Kategorikal Menggunakan One-Hot Vectors)\n",
        "\n",
        "Fitur kategorikal (misalnya, `ocean_proximity`) perlu diubah menjadi representasi numerik. *One-hot encoding* adalah pilihan yang baik untuk kategori dalam jumlah kecil."
      ],
      "id": "d1u70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f5u70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3036f7b6-7374-4934-98d4-f2c0d070bb51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indeks kategori: tf.Tensor([3 5 1 1], shape=(4,), dtype=int64)\n",
            "One-hot encoded: tf.Tensor(\n",
            "[[0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]], shape=(4, 7), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "# Buat indeks untuk setiap kategori\n",
        "indices = tf.range(len(vocab), dtype=tf.int64)\n",
        "\n",
        "# Inisialisasi lookup table\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
        "num_oov_buckets = 2 # Jumlah bucket untuk Out-Of-Vocabulary (Oov)\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
        "\n",
        "# Contoh encoding\n",
        "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
        "cat_indices = table.lookup(categories)\n",
        "print(\"\\nIndeks kategori:\", cat_indices)\n",
        "\n",
        "# Melakukan one-hot encoding\n",
        "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
        "print(\"One-hot encoded:\", cat_one_hot)"
      ],
      "id": "f5u70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1u70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.lookup.StaticVocabularyTable`: Membuat tabel pencarian (lookup table) yang memetakan string ke integer.\n",
        "* `num_oov_buckets`: Menentukan berapa banyak \"bucket\" yang akan digunakan untuk kategori yang tidak ada dalam kosakata yang ditentukan. Kategori OOV akan di-hash dan dipetakan ke salah satu bucket ini.\n",
        "* `tf.one_hot(indices, depth)`: Melakukan *one-hot encoding* pada tensor indeks."
      ],
      "id": "c1u70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1v70d4d"
      },
      "source": [
        "### 4.3. Encoding Categorical Features Using Embeddings (Encoding Fitur Kategorikal Menggunakan Embeddings)\n",
        "\n",
        "Untuk fitur kategorikal dengan jumlah kategori yang besar, *embeddings* adalah pilihan yang lebih efisien daripada *one-hot encoding*. *Embeddings* adalah vektor padat yang dapat dilatih yang merepresentasikan setiap kategori."
      ],
      "id": "d1v70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f5v70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "1c3f0435-a7b5-4f88-e6a1-6591bc869bab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ categories_input    │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ categories_input… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ regular_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m14\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ regular_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m11\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ categories_input    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ categories_input… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ regular_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ regular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25\u001b[0m (100.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> (100.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25\u001b[0m (100.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> (100.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Pastikan Anda memiliki 'vocab' dan 'table' yang sudah didefinisikan dari sel sebelumnya.\n",
        "# Untuk demo ini, saya akan mendefinisikannya kembali secara minimal.\n",
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "indices = tf.range(len(vocab), dtype=tf.int64)\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
        "num_oov_buckets = 2\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
        "\n",
        "embedding_dim = 2 # Dimensi embedding\n",
        "\n",
        "# Menggabungkan fitur numerik dan embedding dalam model\n",
        "# Asumsikan regular_inputs adalah tensor fitur numerik\n",
        "regular_inputs = tf.keras.layers.Input(shape=[8], name=\"regular_input\")\n",
        "categories_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"categories_input\")\n",
        "\n",
        "# Mengubah kategori menjadi indeks menggunakan tabel pencarian\n",
        "# Perbaikan: Tambahkan `output_shape`\n",
        "cat_indices_layer = tf.keras.layers.Lambda(\n",
        "    lambda cats: table.lookup(cats),\n",
        "    output_shape=lambda input_shape: input_shape # Output shape sama dengan input shape\n",
        ")(categories_input)\n",
        "\n",
        "# Melakukan embedding\n",
        "cat_embed_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=len(vocab) + num_oov_buckets, output_dim=embedding_dim\n",
        ")(cat_indices_layer)\n",
        "\n",
        "# Menggabungkan fitur reguler dan embedding\n",
        "encoded_inputs = tf.keras.layers.concatenate([regular_inputs, cat_embed_layer])\n",
        "\n",
        "# Model Dense sederhana setelah embedding\n",
        "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
        "\n",
        "model_with_embedding = tf.keras.Model(inputs=[regular_inputs, categories_input],\n",
        "                                       outputs=[outputs])\n",
        "\n",
        "# Contoh compile dan fit\n",
        "# model_with_embedding.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "# dummy_X_num = np.random.rand(10, 8).astype(np.float32)\n",
        "# dummy_X_cat = np.array([\"INLAND\"]*5 + [\"NEAR BAY\"]*5)\n",
        "# dummy_Y = np.random.rand(10, 1).astype(np.float32)\n",
        "# model_with_embedding.fit({\"regular_input\": dummy_X_num, \"categories_input\": dummy_X_cat}, dummy_Y, epochs=1)\n",
        "\n",
        "model_with_embedding.summary()"
      ],
      "id": "f5v70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1v70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tf.keras.layers.Embedding`: Lapisan ini mengambil indeks integer sebagai input dan mengembalikan vektor *embedding* yang sesuai. Matriks *embedding* itu sendiri adalah bobot yang dapat dilatih oleh model.\n",
        "* *Embeddings* memungkinkan model mempelajari representasi yang bermakna untuk kategori, di mana kategori yang serupa akan memiliki *embedding* yang dekat dalam ruang *embedding*."
      ],
      "id": "c1v70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1w70d4d"
      },
      "source": [
        "### 4.4. Keras Preprocessing Layers (Lapisan Pra-pemrosesan Keras)\n",
        "\n",
        "TensorFlow sedang mengembangkan serangkaian lapisan pra-pemrosesan Keras standar (`tf.keras.layers.experimental.preprocessing` atau nanti langsung di `tf.keras.layers`). Ini menyederhanakan pra-pemrosesan data dalam model."
      ],
      "id": "d1w70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f5w70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33dbc68-affb-4511-887c-1d9791e5195c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lapisan Pra-pemrosesan Keras (konsep):\n",
            "Lapisan ini akan memudahkan pra-pemrosesan data mentah langsung dalam model.\n"
          ]
        }
      ],
      "source": [
        "# Contoh Lapisan Pra-pemrosesan Keras (fitur eksperimental, mungkin berbeda di versi TF yang lebih baru)\n",
        "# Asumsikan Anda memiliki data mentah\n",
        "# raw_numerical_data = np.random.rand(100, 5).astype(np.float32)\n",
        "# raw_categorical_data = np.array([\"A\", \"B\", \"C\", \"A\", \"B\"]*20)\n",
        "\n",
        "# Lapisan Normalization (standardisasi)\n",
        "# normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "# normalizer.adapt(raw_numerical_data) # Hitung mean dan variance\n",
        "# normalized_data = normalizer(raw_numerical_data)\n",
        "\n",
        "# Lapisan TextVectorization (untuk teks, atau kategorikal string)\n",
        "# text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "#     max_tokens=1000, output_mode=\"int\") # max_tokens = ukuran kosakata\n",
        "# text_vectorizer.adapt(raw_categorical_data) # Bangun kosakata\n",
        "# encoded_text = text_vectorizer(raw_categorical_data)\n",
        "\n",
        "# Lapisan Discretization\n",
        "# discretizer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "#     bin_boundaries=[0.1, 0.5, 0.9]) # Batas untuk membagi data kontinu ke dalam bin\n",
        "# discretized_data = discretizer(raw_numerical_data[:, 0])\n",
        "\n",
        "print(\"\\nLapisan Pra-pemrosesan Keras (konsep):\")\n",
        "print(\"Lapisan ini akan memudahkan pra-pemrosesan data mentah langsung dalam model.\")"
      ],
      "id": "f5w70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1w70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* Lapisan-lapisan ini (`Normalization`, `TextVectorization`, `Discretization`, dll.) dirancang untuk diintegrasikan langsung ke dalam model Keras.\n",
        "* Metode `adapt()` pada lapisan-lapisan ini memungkinkan mereka untuk menganalisis sampel data (biasanya *training set*) dan menghitung statistik yang diperlukan (misalnya, *mean*/*std* untuk normalisasi, *vocabulary* untuk teks).\n",
        "* Setelah di-`adapt`, lapisan dapat digunakan dalam model dan akan menerapkan transformasi yang sama secara konsisten selama pelatihan dan inferensi."
      ],
      "id": "c1w70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1x70d4d"
      },
      "source": [
        "## 5. TF Transform (`tf.Transform`)\n",
        "\n",
        "`tf.Transform` adalah bagian dari TensorFlow Extended (TFX) dan memungkinkan Anda menulis fungsi pra-pemrosesan tunggal yang dapat dijalankan dalam mode *batch* di seluruh *training set* sebelum pelatihan, dan kemudian diekspor ke `tf.function` untuk digunakan dalam model yang di-*deploy*. Ini mengatasi masalah *training/serving skew*."
      ],
      "id": "d1x70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f5x70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499b9199-529f-403e-dc7a-628ee5cdfab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_transform in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (1.26.4)\n",
            "Requirement already satisfied: pyarrow<11,>=10 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (10.0.1)\n",
            "Requirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (1.4.2)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (2.16.2)\n",
            "Requirement already satisfied: tensorflow-metadata<1.17.0,>=1.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (1.16.1)\n",
            "Requirement already satisfied: tf-keras>=2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (2.16.0)\n",
            "Requirement already satisfied: tfx-bsl<1.17.0,>=1.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (1.16.1)\n",
            "Requirement already satisfied: apache-beam<3,>=2.53 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.65.0)\n",
            "Requirement already satisfied: protobuf<6,>=4.25.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_transform) (4.25.8)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.10.18)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.3.1.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.11.1)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.65.5)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.24.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.4.2)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.7.1)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (24.2)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2025.2)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (5.3.0)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2024.11.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.32.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.14.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (6.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.7)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (5.5.2)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.25.1)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.5.31)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.30.0)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.12.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.34.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.32.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.4.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.31.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.55.0)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.30.0)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.17.2)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.16.2)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.10.2)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.10.18)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.97.0)\n",
            "Requirement already satisfied: keyrings.google-artifactregistry-auth in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from pydot<2,>=1.2->tensorflow_transform) (3.2.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (1.17.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tensorflow_transform) (0.37.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata<1.17.0,>=1.16.1->tensorflow_transform) (1.70.0)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.11/dist-packages (from tfx-bsl<1.17.0,>=1.16.1->tensorflow_transform) (1.12.11)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.11/dist-packages (from tfx-bsl<1.17.0,>=1.16.1->tensorflow_transform) (1.5.3)\n",
            "Requirement already satisfied: tensorflow-serving-api<3,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tfx-bsl<1.17.0,>=1.16.1->tensorflow_transform) (2.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tensorflow_transform) (0.45.1)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.17.0,>=1.16.1->tensorflow_transform) (3.0.1)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.11/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.20.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.11.7)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.14.2)\n",
            "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.7.1)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.62.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.34.1)\n",
            "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (7.7.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.5.3)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.15.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.6.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.25.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow_transform) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow_transform) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow_transform) (0.16.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.7.0)\n",
            "Requirement already satisfied: PyJWT~=2.9.0 in /usr/local/lib/python3.11/dist-packages (from redis<6,>=5.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow_transform) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow_transform) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow_transform) (3.1.3)\n",
            "Requirement already satisfied: keyring in /usr/local/lib/python3.11/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (25.6.0)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.11/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.6.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.9.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (15.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.55b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow_transform) (3.0.2)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.3.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (4.1.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (6.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow_transform) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow_transform) (2.19.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow_transform) (0.1.2)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.11/dist-packages (from SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (10.7.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.11/dist-packages (from jaraco.context->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow_transform) (2.22)\n",
            "\n",
            "Konsep tf.Transform:\n",
            "Memungkinkan pra-pemrosesan konsisten antara pelatihan dan serving.\n"
          ]
        }
      ],
      "source": [
        "# Konsep tf.Transform (membutuhkan instalasi tensorflow_transform dan Apache Beam)\n",
        "!pip install tensorflow_transform\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "# Contoh fungsi pra-pemrosesan (hanya untuk ilustrasi)\n",
        "def preprocess_with_tft(inputs):\n",
        "    # inputs adalah sebuah batch dari fitur input\n",
        "    median_age = inputs[\"housing_median_age\"]\n",
        "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
        "\n",
        "    # Skala fitur numerik\n",
        "    standardized_age = tft.scale_to_z_score(median_age)\n",
        "\n",
        "    # Bangun kosakata untuk fitur kategorikal\n",
        "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
        "\n",
        "    return {\n",
        "        \"standardized_median_age\": standardized_age,\n",
        "        \"ocean_proximity_id\": ocean_proximity_id\n",
        "    }\n",
        "\n",
        "print(\"\\nKonsep tf.Transform:\")\n",
        "print(\"Memungkinkan pra-pemrosesan konsisten antara pelatihan dan serving.\")"
      ],
      "id": "f5x70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1x70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* **Offline Pra-pemrosesan:** `tf.Transform` menjalankan fungsi pra-pemrosesan di seluruh *training set* (*offline*) menggunakan kerangka pemrosesan data terdistribusi (seperti Apache Beam). Selama proses ini, ia menghitung statistik (misalnya, *mean*, *std*, *vocabulary*) yang diperlukan untuk transformasi.\n",
        "* **Export ke `tf.function`:** Setelah statistik dihitung, `tf.Transform` menghasilkan `tf.function` yang merangkum logika pra-pemrosesan dan nilai-nilai statistik yang dihitung.\n",
        "* **Konsistensi:** Fungsi `tf.function` ini kemudian dapat menjadi bagian dari model yang diekspor, memastikan bahwa pra-pemrosesan selama *serving* (inferensi) persis sama dengan yang dilakukan selama pelatihan."
      ],
      "id": "c1x70d4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1y70d4d"
      },
      "source": [
        "## 6. The TensorFlow Datasets (TFDS) Project\n",
        "\n",
        "TFDS menyediakan cara mudah untuk mengunduh dan memuat banyak dataset umum secara langsung ke dalam format `tf.data.Dataset`."
      ],
      "id": "d1y70d4d"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "f5y70d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5dfb5aa-afc1-4562-e89a-aa9f81bdd947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset MNIST dari TFDS:\n",
            "Info Dataset: tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    full_name='mnist/3.0.1',\n",
            "    description=\"\"\"\n",
            "    The MNIST database of handwritten digits.\n",
            "    \"\"\",\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    data_dir='/root/tensorflow_datasets/mnist/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=11.06 MiB,\n",
            "    dataset_size=21.00 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    nondeterministic_order=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            ")\n",
            "Ukuran Training Set: 60000\n",
            "\n",
            "Contoh batch dari MNIST training set:\n",
            "Shape Gambar: (32, 28, 28, 1)\n",
            "Shape Label: (32,)\n",
            "Contoh Label: [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7735 - loss: 17.1587\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d9f26ee23d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Mengunduh dan memuat dataset MNIST\n",
        "# as_supervised=True akan mengembalikan (features, label) tuples, cocok untuk Keras\n",
        "dataset, info = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True, with_info=True)\n",
        "\n",
        "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "print(\"\\nDataset MNIST dari TFDS:\")\n",
        "print(\"Info Dataset:\", info)\n",
        "print(\"Ukuran Training Set:\", info.splits[\"train\"].num_examples)\n",
        "\n",
        "# Contoh iterasi pada dataset\n",
        "print(\"\\nContoh batch dari MNIST training set:\")\n",
        "for images, labels in mnist_train.take(1):\n",
        "    print(\"Shape Gambar:\", images.shape)\n",
        "    print(\"Shape Label:\", labels.shape)\n",
        "    print(\"Contoh Label:\", labels.numpy())\n",
        "\n",
        "# Contoh penggunaan langsung dengan Keras\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(mnist_train, epochs=1)"
      ],
      "id": "f5y70d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1y70d4f"
      },
      "source": [
        "**Penjelasan Teoritis:**\n",
        "* `tfds.load(name, as_supervised, with_info)`: Fungsi utama untuk memuat dataset dari katalog TFDS.\n",
        "    * `name`: Nama dataset (misalnya, \"mnist\", \"imdb_reviews\").\n",
        "    * `as_supervised=True`: Mengembalikan elemen-elemen sebagai tuple `(features, label)`, ideal untuk model Keras yang dilatih secara *supervised*.\n",
        "    * `with_info=True`: Mengembalikan metadata tentang dataset.\n",
        "* TFDS secara otomatis menangani pengunduhan, ekstraksi, dan pembagian dataset menjadi `tf.data.Dataset` objek, yang kemudian dapat digunakan langsung dalam pelatihan model Keras."
      ],
      "id": "c1y70d4f"
    }
  ]
}