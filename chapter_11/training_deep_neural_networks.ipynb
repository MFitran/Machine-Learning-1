{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M7V6d5mmwVC"
      },
      "source": [
        "# Bab 11 - Melatih Deep Neural Networks\n",
        "\n",
        "**Tujuan Pembelajaran:**\n",
        "Memperdalam pemahaman dan keterampilan praktis dalam mengimplementasikan konsep inti Machine Learning melalui reproduksi kode dan penjelasan teoritis yang terstruktur, menggunakan buku *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems* (O’Reilly) sebagai referensi utama."
      ],
      "id": "9M7V6d5mmwVC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAWnz-wQmwVI"
      },
      "source": [
        "## Pendahuluan\n",
        "\n",
        "Bab ini membahas tantangan-tantangan dalam melatih Jaringan Neural Dalam (DNN) dan teknik-teknik untuk mengatasinya. Kita akan menjelajahi masalah *vanishing/exploding gradients*, strategi inisialisasi yang baik, fungsi aktivasi non-saturasi, Batch Normalization, dan Gradient Clipping. Selanjutnya, kita akan mempelajari teknik *transfer learning* dan *unsupervised pretraining* untuk skenario data terbatas. Terakhir, kita akan melihat berbagai *optimizer* dan teknik regularisasi untuk meningkatkan kinerja dan stabilitas model."
      ],
      "id": "nAWnz-wQmwVI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHmCIdmemwVJ"
      },
      "source": [
        "---"
      ],
      "id": "IHmCIdmemwVJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iufeJSAEmwVK"
      },
      "source": [
        "## Bagian 1: Masalah Vanishing/Exploding Gradients"
      ],
      "id": "iufeJSAEmwVK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "indVBBNbmwVK"
      },
      "source": [
        "### 1.1. Latar Belakang Masalah\n",
        "\n",
        "Saat melatih jaringan neural yang dalam, gradien seringkali menjadi sangat kecil (\"vanishing gradients\") atau sangat besar (\"exploding gradients\") seiring dengan propagasi mundur melalui lapisan-lapisan.\n",
        "\n",
        "* **Vanishing Gradients**: Menyebabkan pembaruan bobot pada lapisan bawah menjadi sangat kecil, sehingga pelatihan tidak konvergen.\n",
        "* **Exploding Gradients**: Menyebabkan pembaruan bobot menjadi sangat besar, sehingga algoritma menjadi tidak stabil dan menyimpang.\n",
        "\n",
        "Kedua masalah ini membuat lapisan-lapisan yang lebih rendah sulit untuk dilatih secara efektif, menyebabkan pelatihan yang lambat atau tidak stabil."
      ],
      "id": "indVBBNbmwVK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0xTNBNFmwVL"
      },
      "source": [
        "### 1.2. Inisialisasi Glorot dan He\n",
        "\n",
        "Masalah gradien yang tidak stabil sebagian besar disebabkan oleh kombinasi fungsi aktivasi saturasi (seperti sigmoid atau tanh) dan skema inisialisasi bobot yang populer pada masa itu (distribusi normal dengan mean 0 dan standar deviasi 1).  Glorot dan Bengio mengusulkan bahwa agar sinyal mengalir dengan baik ke depan (saat membuat prediksi) dan ke belakang (saat *backpropagating gradients*), varians output setiap lapisan harus sama dengan varians inputnya, dan gradien harus memiliki varians yang sama sebelum dan sesudah mengalir melalui lapisan.\n",
        "\n",
        "**Inisialisasi Glorot (Xavier Initialization)**:\n",
        "Untuk fungsi aktivasi sigmoid atau tanh, bobot harus diinisialisasi secara acak dari distribusi normal dengan mean 0 dan varians $\\sigma^2 = \\frac{1}{\\text{fan}_{avg}}$ atau dari distribusi uniform antara $-r$ dan $+r$ dengan $r = \\sqrt{\\frac{3}{\\text{fan}_{avg}}}$, di mana $\\text{fan}_{avg} = (\\text{fan}_{in} + \\text{fan}_{out}) / 2$.\n",
        "\n",
        "**Inisialisasi He**:\n",
        "Untuk fungsi aktivasi ReLU dan variannya, bobot harus diinisialisasi dari distribusi normal dengan mean 0 dan varians $\\sigma^2 = \\frac{2}{\\text{fan}_{in}}$ atau dari distribusi uniform dengan $r = \\sqrt{\\frac{6}{\\text{fan}_{in}}}$.\n",
        "\n",
        "**Kode Implementasi (Inisialisasi)**"
      ],
      "id": "E0xTNBNFmwVL"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8--_xfymwVM",
        "outputId": "2f84936b-d97d-492b-eca1-94323927a369"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_2, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Contoh penggunaan inisialisasi He pada lapisan Dense\n",
        "# Untuk ReLU dan variannya\n",
        "he_normal_init = keras.initializers.HeNormal()\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=he_normal_init)\n",
        "\n",
        "# Untuk SELU, gunakan inisialisasi LeCun\n",
        "lecun_normal_init = keras.initializers.LecunNormal()\n",
        "keras.layers.Dense(10, activation=\"selu\", kernel_initializer=lecun_normal_init)\n",
        "\n",
        "# Jika ingin kustomisasi lebih lanjut (contoh VarianceScaling)\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "id": "w8--_xfymwVM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5RJ-pKhmwVO"
      },
      "source": [
        "### 1.3. Fungsi Aktivasi Non-Saturasi\n",
        "\n",
        "Fungsi aktivasi sigmoid dan tanh mengalami masalah saturasi, di mana gradien menjadi sangat dekat dengan nol untuk input yang besar (negatif atau positif), yang menyebabkan *vanishing gradients*.  Fungsi aktivasi non-saturasi membantu mengurangi masalah ini.\n",
        "\n",
        "* **ReLU (Rectified Linear Unit)**: $ReLU(z) = max(0, z)$. Cepat dihitung dan tidak saturasi untuk nilai positif. Namun, dapat mengalami masalah \"dying ReLUs\" di mana neuron berhenti mengeluarkan apa pun selain 0.\n",
        "* **Leaky ReLU**: $LeakyReLU_{\\alpha}(z) = max(\\alpha z, z)$. Memiliki sedikit kemiringan untuk $z < 0$ ($\\alpha$ adalah hiperparameter, biasanya 0.01), yang mencegah neuron \"mati\".\n",
        "* **PReLU (Parametric Leaky ReLU)**: Mirip dengan Leaky ReLU, tetapi $\\alpha$ dipelajari selama pelatihan.\n",
        "* **ELU (Exponential Linear Unit)**: $ELU_{\\alpha}(z) = \\alpha(exp(z) - 1)$ jika $z < 0$, dan $z$ jika $z \\ge 0$. Mengungguli varian ReLU dalam eksperimen, mengurangi waktu pelatihan dan meningkatkan kinerja. Mengambil nilai negatif, yang membantu meredakan masalah *vanishing gradients*.\n",
        "* **SELU (Scaled ELU)**: Varian ELU yang diskalakan. Jika jaringan hanya terdiri dari lapisan Dense dan menggunakan fungsi aktivasi SELU, serta bobot diinisialisasi dengan inisialisasi LeCun normal, jaringan akan *self-normalize*, yang secara efektif menyelesaikan masalah *vanishing/exploding gradients*."
      ],
      "id": "N5RJ-pKhmwVO"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Uul9gQmwVO",
        "outputId": "f7f20bde-925b-4d48-b4bc-4a2b5c2f4f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Contoh model dengan fungsi aktivasi ELU dan inisialisasi HeNormal\n",
        "model_elu = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Contoh model dengan fungsi aktivasi Leaky ReLU\n",
        "model_leaky_relu = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2), # Tambahkan Leaky ReLU sebagai lapisan terpisah\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Contoh model dengan fungsi aktivasi SELU (membutuhkan input yang distandardisasi)\n",
        "# Pastikan X_train_scaled, X_valid_scaled, X_test_scaled sudah distandardisasi (mean 0, std 1)\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train.astype(np.float32).reshape(-1, 28*28)).reshape(-1, 28, 28)\n",
        "\n",
        "model_selu = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "id": "I7Uul9gQmwVO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEKSh-4CmwVP"
      },
      "source": [
        "### 1.4. Batch Normalization\n",
        "\n",
        "Batch Normalization (BN) adalah teknik yang mengatasi masalah gradien tidak stabil dengan menambahkan operasi normalisasi tepat sebelum atau sesudah fungsi aktivasi setiap lapisan tersembunyi.  Operasi ini menormalkan dan menormalisasi input setiap input (mean 0, standar deviasi 1) kemudian menskalakannya dan menggeser hasilnya menggunakan dua vektor parameter baru (satu untuk penskalaan $\\gamma$, satu untuk pergeseran $\\beta$).\n",
        "\n",
        "* **Manfaat**:\n",
        "    * Mengurangi masalah *vanishing/exploding gradients*.\n",
        "    * Jaringan kurang sensitif terhadap inisialisasi bobot.\n",
        "    * Memungkinkan penggunaan *learning rates* yang lebih besar, mempercepat pelatihan.\n",
        "    * Bertindak sebagai regularizer, mengurangi kebutuhan akan teknik regularisasi lainnya.\n",
        "* **Cara kerja**: Selama pelatihan, BN memperkirakan mean dan standar deviasi input berdasarkan mini-batch saat ini.  Selama inferensi, ia menggunakan rata-rata bergerak dari mean dan standar deviasi input yang dihitung selama pelatihan.\n",
        "\n",
        "**Kode Implementasi (Batch Normalization)**"
      ],
      "id": "xEKSh-4CmwVP"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "OOaKU5-3mwVQ",
        "outputId": "155e37dd-d073-4354-a1a2-f127fb024236"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m270,946\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">270,946</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,578\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,578</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Contoh model dengan Batch Normalization setelah lapisan Dense dan sebelum aktivasi\n",
        "model_bn = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(), # Normalisasi input pertama\n",
        "    keras.layers.Dense(300, use_bias=False), # Hapus bias karena BN akan menambahkannya\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Contoh model dengan Batch Normalization setelah fungsi aktivasi (opsi lain)\n",
        "model_bn_after_activation = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"elu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_bn.summary()"
      ],
      "id": "OOaKU5-3mwVQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgb-PCBpmwVR"
      },
      "source": [
        "### 1.5. Gradient Clipping\n",
        "\n",
        "Gradient Clipping adalah teknik populer untuk mengatasi masalah *exploding gradients* dengan membatasi gradien agar tidak melebihi ambang batas tertentu selama *backpropagation*.  Ini sering digunakan pada jaringan neural berulang (RNN) di mana *Batch Normalization* sulit diterapkan.\n",
        "\n",
        "**Kode Implementasi (Gradient Clipping)**"
      ],
      "id": "xgb-PCBpmwVR"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J7LGmy27mwVR"
      },
      "outputs": [],
      "source": [
        "# Contoh penggunaan Gradient Clipping pada optimizer\n",
        "optimizer_clipped = keras.optimizers.SGD(clipvalue=1.0) # Memotong setiap komponen gradien antara -1.0 dan 1.0\n",
        "model_clipped = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_clipped.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_clipped)\n",
        "\n",
        "# Opsi lain: clipnorm (memotong seluruh vektor gradien jika norma L2-nya melebihi ambang batas)\n",
        "optimizer_clipped_norm = keras.optimizers.SGD(clipnorm=1.0)\n",
        "model_clipped_norm = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_clipped_norm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_clipped_norm)"
      ],
      "id": "J7LGmy27mwVR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeRHRdiNmwVS"
      },
      "source": [
        "---"
      ],
      "id": "BeRHRdiNmwVS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_uBJnzXmwVS"
      },
      "source": [
        "## Bagian 2: Memanfaatkan Model Terlatih (Pretrained Models)"
      ],
      "id": "1_uBJnzXmwVS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz0kuUFxmwVS"
      },
      "source": [
        "### 2.1. Reusing Pretrained Layers (Transfer Learning)\n",
        "\n",
        "*Transfer learning* adalah teknik yang sangat efektif di mana kita menggunakan kembali lapisan bawah dari jaringan neural yang sudah terlatih pada tugas serupa.  Ini secara signifikan mempercepat pelatihan dan membutuhkan lebih sedikit data pelatihan.\n",
        "* **Strategi**: Ganti lapisan output model asli, bekukan lapisan-lapisan yang digunakan kembali (membuat bobotnya tidak dapat dilatih) untuk beberapa epoch awal, dan kemudian secara opsional *unfreeze* lapisan atas dan sesuaikan *learning rate* untuk *fine-tuning*.\n",
        "* **Manfaat**: Mengurangi *overfitting* karena model tidak perlu mempelajari fitur tingkat rendah dari awal.\n",
        "\n",
        "**Kode Implementasi (Transfer Learning)**"
      ],
      "id": "vz0kuUFxmwVS"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W6tPk0ZNmwVT"
      },
      "outputs": [],
      "source": [
        "# Contoh (ini adalah contoh konseptual, model_A perlu dilatih sebelumnya)\n",
        "# Misalkan model_A adalah model yang sudah terlatih untuk tugas serupa\n",
        "# model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "# Ambil semua lapisan kecuali lapisan output\n",
        "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "# Tambahkan lapisan output baru untuk tugas B\n",
        "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Bekukan lapisan-lapisan yang digunakan kembali\n",
        "# for layer in model_B_on_A.layers[:-1]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# Kompilasi model setelah membekukan lapisan\n",
        "# model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Latih model untuk beberapa epoch\n",
        "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "# # Unfreeze lapisan-lapisan yang digunakan kembali untuk fine-tuning\n",
        "# for layer in model_B_on_A.layers[:-1]:\n",
        "#     layer.trainable = True\n",
        "\n",
        "# # Sesuaikan learning rate agar lebih kecil\n",
        "# optimizer_fine_tune = keras.optimizers.SGD(lr=1e-4)\n",
        "# model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer_fine_tune, metrics=[\"accuracy\"])\n",
        "\n",
        "# # Lanjutkan pelatihan\n",
        "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
      ],
      "id": "W6tPk0ZNmwVT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd0695BYmwVT"
      },
      "source": [
        "### 2.2. Unsupervised Pretraining\n",
        "\n",
        "Jika Anda memiliki banyak data pelatihan tidak berlabel tetapi sedikit data berlabel, Anda dapat menggunakan *unsupervised pretraining*. Ini melibatkan pelatihan model tak berarah (seperti autoencoder atau GAN) pada semua data (berlabel dan tidak berlabel), kemudian menggunakan kembali lapisan bawah model tersebut sebagai dasar untuk tugas Anda yang sebenarnya (menggunakan data berlabel)."
      ],
      "id": "wd0695BYmwVT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si87Glt9mwVU"
      },
      "source": [
        "### 2.3. Pretraining on an Auxiliary Task\n",
        "\n",
        "Jika Anda tidak memiliki banyak data berlabel, Anda bisa melatih jaringan neural pertama pada *tugas bantu* (auxiliary task) di mana Anda dapat dengan mudah memperoleh atau menghasilkan data berlabel.  Kemudian, Anda dapat menggunakan kembali lapisan-lapisan bawah jaringan tersebut untuk tugas utama Anda."
      ],
      "id": "si87Glt9mwVU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDbg6vAgmwVU"
      },
      "source": [
        "---"
      ],
      "id": "gDbg6vAgmwVU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blm5-E4qmwVU"
      },
      "source": [
        "## Bagian 3: Optimizer yang Lebih Cepat\n",
        "\n",
        "Pengoptimal yang lebih canggih daripada *Gradient Descent* biasa dapat secara signifikan mempercepat pelatihan model besar."
      ],
      "id": "blm5-E4qmwVU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKdObrwmmwVU"
      },
      "source": [
        "### 3.1. Momentum Optimization\n",
        "\n",
        "Momentum Optimization mempercepat pelatihan dengan menambahkan sebagian dari pembaruan gradien sebelumnya ke pembaruan saat ini.  Ini membantu algoritma mengatasi lembah datar dan \"meluncur\" melewati minimum lokal lebih cepat.\n",
        "* Rumus: $m \\leftarrow \\beta m - \\eta \\nabla_\\theta J(\\theta)$, $\\theta \\leftarrow \\theta + m$\n",
        "* $\\beta$ (momentum): Biasanya 0.9."
      ],
      "id": "JKdObrwmmwVU"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WnEnPtO9mwVU"
      },
      "outputs": [],
      "source": [
        "optimizer_momentum = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_momentum, metrics=[\"accuracy\"])"
      ],
      "id": "WnEnPtO9mwVU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvHoB3s0mwVV"
      },
      "source": [
        "### 3.2. Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "NAG adalah varian dari Momentum Optimization yang hampir selalu lebih cepat.  Ia mengukur gradien fungsi biaya sedikit lebih maju dalam arah momentum, menghasilkan pembaruan yang lebih akurat dan mengurangi osilasi."
      ],
      "id": "uvHoB3s0mwVV"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5pY2VJL3mwVV"
      },
      "outputs": [],
      "source": [
        "optimizer_nag = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_nag, metrics=[\"accuracy\"])"
      ],
      "id": "5pY2VJL3mwVV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOit6NDMmwVV"
      },
      "source": [
        "### 3.3. AdaGrad\n",
        "\n",
        "AdaGrad menyesuaikan *learning rate* untuk setiap parameter, menguranginya lebih cepat untuk dimensi yang curam.  Ini membantu mengarahkan pembaruan secara lebih langsung menuju optimum global, tetapi dapat berhenti terlalu cepat pada pelatihan jaringan neural."
      ],
      "id": "MOit6NDMmwVV"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PwD41UBCmwVV"
      },
      "outputs": [],
      "source": [
        "# Tidak direkomendasikan untuk DNN yang dalam karena cenderung berhenti terlalu cepat.\n",
        "optimizer_adagrad = keras.optimizers.Adagrad(learning_rate=0.001)\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_adagrad, metrics=[\"accuracy\"])"
      ],
      "id": "PwD41UBCmwVV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEN67qzxmwVW"
      },
      "source": [
        "### 3.4. RMSProp\n",
        "\n",
        "RMSProp mengatasi masalah *AdaGrad* dengan mengakumulasi hanya gradien dari iterasi terbaru menggunakan rata-rata bergerak eksponensial.  Ini mencegah *learning rate* menurun terlalu cepat."
      ],
      "id": "UEN67qzxmwVW"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xn-mWhhTmwVW"
      },
      "outputs": [],
      "source": [
        "optimizer_rmsprop = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9) # rho adalah parameter peluruhan\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_rmsprop, metrics=[\"accuracy\"])"
      ],
      "id": "Xn-mWhhTmwVW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVXa0t1KmwVW"
      },
      "source": [
        "### 3.5. Adam dan Nadam Optimization\n",
        "\n",
        "* **Adam (Adaptive Moment Estimation)**: Menggabungkan ide-ide Momentum Optimization dan RMSProp. Ia melacak rata-rata bergerak eksponensial dari gradien di masa lalu (seperti momentum) dan rata-rata bergerak eksponensial dari kuadrat gradien di masa lalu (seperti RMSProp).\n",
        "* **Nadam (Nesterov Adam)**: Adam ditambah trik Nesterov Accelerated Gradient.  Seringkali konvergen sedikit lebih cepat daripada Adam."
      ],
      "id": "oVXa0t1KmwVW"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MPA-d6ThmwVX"
      },
      "outputs": [],
      "source": [
        "optimizer_adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_adam, metrics=[\"accuracy\"])\n",
        "\n",
        "optimizer_nadam = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_nadam, metrics=[\"accuracy\"])"
      ],
      "id": "MPA-d6ThmwVX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0IC64agmwVX"
      },
      "source": [
        "### 3.6. Learning Rate Scheduling\n",
        "\n",
        "Menemukan *learning rate* yang baik sangat penting.  Memulai dengan *learning rate* yang besar dan kemudian menguranginya seiring berjalannya pelatihan dapat mempercepat konvergensi dan mencapai solusi yang lebih baik daripada dengan *learning rate* konstan yang optimal.\n",
        "\n",
        "* **Power Scheduling**: $\\eta(t) = \\eta_0 / (1 + t/s)^c$. *Learning rate* menurun lebih cepat di awal dan kemudian melambat.\n",
        "* **Exponential Scheduling**: $\\eta(t) = \\eta_0 0.1^{t/s}$. *Learning rate* menurun secara eksponensial.\n",
        "* **Piecewise Constant Scheduling**: Menggunakan *learning rate* konstan untuk sejumlah epoch, lalu menurunkannya.\n",
        "* **Performance Scheduling**: Mengurangi *learning rate* saat metrik validasi berhenti membaik.\n",
        "* **1cycle Scheduling**: Meningkatkan *learning rate* secara linear di awal pelatihan, kemudian menguranginya secara linear, dan akhirnya menjatuhkannya lagi di akhir. Ini juga melibatkan perubahan momentum."
      ],
      "id": "A0IC64agmwVX"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RgW0Nc_9mwVX"
      },
      "outputs": [],
      "source": [
        "# Contoh Power Scheduling (decay parameter pada optimizer SGD)\n",
        "# optimizer_power = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
        "\n",
        "# Contoh Exponential Scheduling dengan LearningRateScheduler callback\n",
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "# model.fit(X_train_scaled, y_train, epochs=30, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler])\n",
        "\n",
        "# Contoh ReduceLROnPlateau (Performance Scheduling)\n",
        "lr_plateau_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "# model.fit(X_train_scaled, y_train, epochs=30, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_plateau_scheduler])\n",
        "\n",
        "# Contoh ExponentialDecay dari keras.optimizers.schedules (learning rate per step)\n",
        "# s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
        "# learning_rate_schedule = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
        "# optimizer_schedule = keras.optimizers.SGD(learning_rate=learning_rate_schedule)\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_schedule, metrics=[\"accuracy\"])"
      ],
      "id": "RgW0Nc_9mwVX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENT3ezfgmwVX"
      },
      "source": [
        "---"
      ],
      "id": "ENT3ezfgmwVX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2h1pWMhmwVY"
      },
      "source": [
        "## Bagian 4: Menghindari Overfitting Melalui Regularisasi\n",
        "\n",
        "Jaringan neural yang dalam dengan jutaan parameter sangat rentan terhadap *overfitting*.  Selain *early stopping* (yang sudah dibahas di Bab 10) dan *Batch Normalization* (yang juga bertindak sebagai regularizer), ada teknik regularisasi lainnya."
      ],
      "id": "Z2h1pWMhmwVY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMl6nziimwVY"
      },
      "source": [
        "### 4.1. L1 dan L2 Regularization\n",
        "\n",
        "Sama seperti pada model linear, kita dapat menambahkan *penalty term* L1 atau L2 ke fungsi biaya untuk mengontrol kompleksitas model.\n",
        "* **L2 Regularization** (Ridge): Menambahkan $\\alpha \\sum_{i} \\theta_i^2$ ke fungsi biaya. Mendorong bobot agar tetap kecil.\n",
        "* **L1 Regularization** (Lasso): Menambahkan $\\alpha \\sum_{i} |\\theta_i|$ ke fungsi biaya. Mendorong bobot yang kurang penting menjadi nol, menghasilkan model yang *sparse* (seleksi fitur otomatis)."
      ],
      "id": "pMl6nziimwVY"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBtrgqEwmwVY",
        "outputId": "71cbc9c0-4f02-402d-b006-e28bf04f67d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "# L2 regularization\n",
        "RegularizedDense_L2 = partial(keras.layers.Dense,\n",
        "                              activation=\"elu\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model_l2 = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense_L2(300),\n",
        "    RegularizedDense_L2(100),\n",
        "    RegularizedDense_L2(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
        "])\n",
        "\n",
        "# L1 regularization (atau L1_L2 untuk kombinasi)\n",
        "# RegularizedDense_L1 = partial(keras.layers.Dense,\n",
        "#                               activation=\"elu\",\n",
        "#                               kernel_initializer=\"he_normal\",\n",
        "#                               kernel_regularizer=keras.regularizers.l1(0.01))"
      ],
      "id": "yBtrgqEwmwVY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrNp6KOkmwVY"
      },
      "source": [
        "### 4.2. Dropout\n",
        "\n",
        "Dropout adalah salah satu teknik regularisasi paling populer dan sukses.  Pada setiap langkah pelatihan, setiap neuron (kecuali neuron output) memiliki probabilitas $p$ untuk \"dihilangkan\" sementara, artinya ia diabaikan sepenuhnya selama langkah pelatihan tersebut.\n",
        "* **Dropout rate** ($p$): Biasanya antara 10% dan 50%.\n",
        "* **Manfaat**: Mencegah neuron beradaptasi secara berlebihan satu sama lain, membuat jaringan lebih kuat dan lebih baik dalam menggeneralisasi.  Dapat dilihat sebagai rata-rata dari banyak jaringan neural yang lebih kecil.\n",
        "* **MC Dropout**: Setelah pelatihan, membuat banyak prediksi dengan Dropout diaktifkan (mengatur `training=True`) dan merata-ratakan prediksi tersebut.  Ini memberikan estimasi ketidakpastian model dan dapat meningkatkan kinerja."
      ],
      "id": "RrNp6KOkmwVY"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7DmBuY4xmwVZ"
      },
      "outputs": [],
      "source": [
        "model_dropout = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2), # Dropout pada input layer\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2), # Dropout setelah hidden layer pertama\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2), # Dropout setelah hidden layer kedua\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Implementasi MC Dropout (untuk inferensi)\n",
        "# Setelah model dilatih:\n",
        "# class MCDropout(keras.layers.Dropout):\n",
        "#     def call(self, inputs):\n",
        "#         return super().call(inputs, training=True)\n",
        "\n",
        "# # Buat model baru dengan lapisan MCDropout atau langsung panggil model dengan training=True\n",
        "# # y_probas = np.stack([model_dropout(X_test_scaled, training=True) for sample in range(100)])\n",
        "# # y_proba = y_probas.mean(axis=0)"
      ],
      "id": "7DmBuY4xmwVZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahqwX08nmwVh"
      },
      "source": [
        "### 4.3. Max-Norm Regularization\n",
        "\n",
        "Max-Norm Regularization membatasi bobot $w$ dari koneksi yang masuk ke setiap neuron sehingga $||w||_2 \\le r$, di mana $r$ adalah hiperparameter *max-norm*.  Ini membantu meredakan masalah gradien yang tidak stabil dan mengurangi *overfitting*."
      ],
      "id": "ahqwX08nmwVh"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cs9xX5HNmwVi"
      },
      "outputs": [],
      "source": [
        "model_max_norm = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                        kernel_constraint=keras.constraints.max_norm(1.)), # Batasi norma bobot menjadi 1\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                        kernel_constraint=keras.constraints.max_norm(1.)),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "id": "Cs9xX5HNmwVi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiiq5S1jmwVi"
      },
      "source": [
        "---"
      ],
      "id": "Eiiq5S1jmwVi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alJLFEfLmwVi"
      },
      "source": [
        "## Bagian 5: Ringkasan dan Pedoman Praktis"
      ],
      "id": "alJLFEfLmwVi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixxoGh-xmwVj"
      },
      "source": [
        "**Pedoman Umum untuk Konfigurasi DNN:**\n",
        "\n",
        "| Hiperparameter       | Nilai Default (Umum)                |\n",
        "| :------------------- | :---------------------------------- |\n",
        "| Kernel Initializer   | He Initialization                   |\n",
        "| Activation Function  | ELU                                 |\n",
        "| Normalization        | None (jika dangkal); Batch Norm (jika dalam) |\n",
        "| Regularization       | Early Stopping (+ L2 reg. jika diperlukan) |\n",
        "| Optimizer            | Momentum Optimization (atau RMSProp/Nadam) |\n",
        "| Learning Rate Schedule | 1cycle                              |\n",
        "\n",
        "**Pedoman untuk Jaringan yang *Self-Normalizing* (menggunakan SELU):**\n",
        "\n",
        "| Hiperparameter       | Nilai Default (SELU)                |\n",
        "| :------------------- | :---------------------------------- |\n",
        "| Kernel Initializer   | LeCun Initialization                |\n",
        "| Activation Function  | SELU                                |\n",
        "| Normalization        | None (self-normalization)           |\n",
        "| Regularization       | Alpha Dropout (jika diperlukan)    |\n",
        "| Optimizer            | Momentum Optimization (atau RMSProp/Nadam) |\n",
        "| Learning Rate Schedule | 1cycle                              |\n",
        "\n",
        "**Catatan Penting:**\n",
        "* Selalu normalisasi fitur input.\n",
        "* Pertimbangkan *transfer learning* jika memungkinkan.\n",
        "* Pertimbangkan *unsupervised pretraining* jika data berlabel terbatas.\n",
        "* Jika model membutuhkan model yang *sparse*, gunakan L1 regularization.\n",
        "* Untuk model *low-latency* (prediksi sangat cepat), gunakan lapisan yang lebih sedikit, gabungkan lapisan *Batch Normalization*, dan gunakan fungsi aktivasi yang lebih cepat.\n",
        "* Untuk aplikasi yang sensitif terhadap risiko, gunakan MC Dropout untuk estimasi probabilitas yang lebih diandalkan dan estimasi ketidakpastian."
      ],
      "id": "ixxoGh-xmwVj"
    }
  ]
}